{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ccb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from Noiseprint import *\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\Forged_Aadhaar\\\\forged_aadhaar_detection.pth\", weights_only=False)\n",
    "# model.eval()\n",
    "model = torch.jit.load(\"detection_model.pt\", map_location=torch.device(\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db54122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Your original noiseprint generation code\n",
    "examples = []\n",
    "noiseprints = []\n",
    "\n",
    "for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\temp\\\\*'):\n",
    "    example, noise_print = getNoiseprint(img_path)\n",
    "    examples.append(example)\n",
    "    print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "    noiseprints.append([img_path.split('\\\\')[-1].split('.')[0], noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprints_folder\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Function to enhance contrast and brightness\n",
    "def enhance_noiseprint(img, method='clahe'):\n",
    "    \"\"\"\n",
    "    Enhance noiseprint visibility using various methods\n",
    "    \n",
    "    Parameters:\n",
    "    - img: normalized image (0-1 range)\n",
    "    - method: 'clahe', 'histogram_eq', 'gamma', or 'adaptive'\n",
    "    \"\"\"\n",
    "    # Convert to 8-bit for processing\n",
    "    img_8bit = (img * 255).astype(np.uint8)\n",
    "    \n",
    "    if method == 'clahe':\n",
    "        # CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        enhanced = clahe.apply(img_8bit)\n",
    "    \n",
    "    elif method == 'histogram_eq':\n",
    "        # Standard histogram equalization\n",
    "        enhanced = cv2.equalizeHist(img_8bit)\n",
    "    \n",
    "    elif method == 'gamma':\n",
    "        # Gamma correction (gamma < 1 brightens, gamma > 1 darkens)\n",
    "        gamma = 0.5\n",
    "        enhanced = np.power(img, gamma)\n",
    "        return enhanced\n",
    "    \n",
    "    elif method == 'adaptive':\n",
    "        # Adaptive thresholding with brightness boost\n",
    "        enhanced = cv2.adaptiveThreshold(img_8bit, 255, \n",
    "                                         cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                         cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    # Convert back to 0-1 range\n",
    "    return enhanced / 255.0\n",
    "\n",
    "# Save multiple versions of each noiseprint\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "    \n",
    "    # Normalize\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "    \n",
    "    # Save original\n",
    "    filename_orig = os.path.join(output_dir, f\"noiseprint_{name}_original.png\")\n",
    "    plt.imsave(filename_orig, norm_crop, cmap='gray')\n",
    "    \n",
    "    # Save CLAHE enhanced version (best for noiseprints)\n",
    "    enhanced_clahe = enhance_noiseprint(norm_crop, method='clahe')\n",
    "    filename_clahe = os.path.join(output_dir, f\"noiseprint_{name}_clahe.png\")\n",
    "    plt.imsave(filename_clahe, enhanced_clahe, cmap='gray')\n",
    "    \n",
    "    # Save histogram equalized version\n",
    "    enhanced_hist = enhance_noiseprint(norm_crop, method='histogram_eq')\n",
    "    filename_hist = os.path.join(output_dir, f\"noiseprint_{name}_hist_eq.png\")\n",
    "    plt.imsave(filename_hist, enhanced_hist, cmap='gray')\n",
    "    \n",
    "    # Save gamma corrected version (brightened)\n",
    "    enhanced_gamma = enhance_noiseprint(norm_crop, method='gamma')\n",
    "    filename_gamma = os.path.join(output_dir, f\"noiseprint_{name}_gamma.png\")\n",
    "    plt.imsave(filename_gamma, enhanced_gamma, cmap='gray')\n",
    "    \n",
    "    # Optional: Create a comparison image showing all versions\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    axes[0].imshow(norm_crop, cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(enhanced_clahe, cmap='gray')\n",
    "    axes[1].set_title('CLAHE Enhanced')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(enhanced_hist, cmap='gray')\n",
    "    axes[2].set_title('Histogram Equalized')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    axes[3].imshow(enhanced_gamma, cmap='gray')\n",
    "    axes[3].set_title('Gamma Corrected')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    comparison_file = os.path.join(output_dir, f\"noiseprint_{name}_comparison.png\")\n",
    "    plt.savefig(comparison_file, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Enhanced noiseprints saved to {output_dir}\")\n",
    "print(\"Multiple versions created: CLAHE, Histogram Equalization, Gamma Correction, and Comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\tempfolder\\\\*'):\n",
    "     example,noise_print=getNoiseprint(img_path)\n",
    "     examples.append(example)\n",
    "     print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "     noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprints_folder\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd615190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from scipy import stats\n",
    "from sklearn.cluster import DBSCAN\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "diff_output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprint_diff_highlight\"\n",
    "os.makedirs(diff_output_dir, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: CHECK AND FIX NOISEPRINTS DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECKING NOISEPRINT DATA\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Check if noiseprints variable exists and what it contains\n",
    "try:\n",
    "    print(f\"Number of items in noiseprints: {len(noiseprints)}\")\n",
    "    print(f\"\\nFirst few items:\")\n",
    "    for i, (name, res) in enumerate(noiseprints[:3]):\n",
    "        print(f\"  {i+1}. Name: {name}\")\n",
    "        print(f\"     Type: {type(res)}\")\n",
    "        print(f\"     Shape: {res.shape if hasattr(res, 'shape') else 'N/A'}\")\n",
    "        print(f\"     Data type: {res.dtype if hasattr(res, 'dtype') else 'N/A'}\")\n",
    "        if hasattr(res, 'shape') and res.size > 0:\n",
    "            print(f\"     Value range: [{np.min(res):.4f}, {np.max(res):.4f}]\")\n",
    "        print()\n",
    "except NameError:\n",
    "    print(\"‚ùå ERROR: 'noiseprints' variable not found!\")\n",
    "    print(\"Please make sure you've run the noiseprint extraction code first.\")\n",
    "    print(\"\\nExpected format:\")\n",
    "    print(\"  noiseprints = [(name1, array1), (name2, array2), ...]\")\n",
    "    exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: LOAD NOISEPRINTS FROM SAVED FILES IF NEEDED\n",
    "# ============================================================================\n",
    "\n",
    "def load_noiseprints_from_directory(noiseprint_dir):\n",
    "    \"\"\"\n",
    "    Load noiseprints from saved .npy files\n",
    "    \"\"\"\n",
    "    noiseprint_files = glob.glob(os.path.join(noiseprint_dir, \"*.npy\"))\n",
    "    loaded_noiseprints = []\n",
    "    \n",
    "    print(f\"\\nFound {len(noiseprint_files)} .npy files\")\n",
    "    \n",
    "    for npy_path in noiseprint_files:\n",
    "        try:\n",
    "            name = os.path.splitext(os.path.basename(npy_path))[0]\n",
    "            noiseprint = np.load(npy_path)\n",
    "            loaded_noiseprints.append((name, noiseprint))\n",
    "            print(f\"  ‚úì Loaded: {name} - Shape: {noiseprint.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to load {npy_path}: {e}\")\n",
    "    \n",
    "    return loaded_noiseprints\n",
    "\n",
    "# Try to load from directory if noiseprints are empty\n",
    "if all(res.size == 0 for _, res in noiseprints):\n",
    "    print(\"\\n‚ö†Ô∏è  All noiseprints are empty! Attempting to load from saved files...\")\n",
    "    \n",
    "    # Common noiseprint output directories - update these paths as needed\n",
    "    possible_dirs = [\n",
    "        \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprints\",\n",
    "        \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprint_output\",\n",
    "        \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\noiseprints\",\n",
    "        \"./noiseprints\",\n",
    "        \"./output/noiseprints\"\n",
    "    ]\n",
    "    \n",
    "    loaded = False\n",
    "    for directory in possible_dirs:\n",
    "        if os.path.exists(directory):\n",
    "            print(f\"\\nüìÅ Checking directory: {directory}\")\n",
    "            temp_noiseprints = load_noiseprints_from_directory(directory)\n",
    "            if temp_noiseprints:\n",
    "                noiseprints = temp_noiseprints\n",
    "                loaded = True\n",
    "                print(f\"\\n‚úÖ Successfully loaded {len(noiseprints)} noiseprints!\")\n",
    "                break\n",
    "    \n",
    "    if not loaded:\n",
    "        print(\"\\n‚ùå Could not find saved noiseprints!\")\n",
    "        print(\"Please provide the directory path where noiseprints are saved:\")\n",
    "        print(\"Or re-run the noiseprint extraction code first.\")\n",
    "        exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_numpy_values(noise_crop):\n",
    "    \"\"\"\n",
    "    Analyze numpy values to detect distortions using statistical methods\n",
    "    \"\"\"\n",
    "    # Check if noise_crop is empty or too small\n",
    "    if noise_crop.size == 0:\n",
    "        return None, None\n",
    "    \n",
    "    if noise_crop.shape[0] < 10 or noise_crop.shape[1] < 10:\n",
    "        return None, None\n",
    "    \n",
    "    # Check for NaN or Inf values\n",
    "    if np.any(np.isnan(noise_crop)) or np.any(np.isinf(noise_crop)):\n",
    "        noise_crop = np.nan_to_num(noise_crop, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Z-score based anomaly detection\n",
    "        mean_val = np.mean(noise_crop)\n",
    "        std_val = np.std(noise_crop)\n",
    "        \n",
    "        if std_val == 0:\n",
    "            z_mask = np.zeros_like(noise_crop, dtype=np.uint8)\n",
    "        else:\n",
    "            z_scores = np.abs((noise_crop - mean_val) / std_val)\n",
    "            z_mask = (z_scores > 2.5).astype(np.uint8) * 255\n",
    "        \n",
    "        # Method 2: Percentile-based detection\n",
    "        try:\n",
    "            lower_percentile = np.percentile(noise_crop, 5)\n",
    "            upper_percentile = np.percentile(noise_crop, 95)\n",
    "            percentile_mask = ((noise_crop < lower_percentile) | (noise_crop > upper_percentile)).astype(np.uint8) * 255\n",
    "        except:\n",
    "            percentile_mask = np.zeros_like(noise_crop, dtype=np.uint8)\n",
    "        \n",
    "        # Method 3: Local variance analysis\n",
    "        kernel_size = min(15, noise_crop.shape[0] // 4, noise_crop.shape[1] // 4)\n",
    "        if kernel_size < 3:\n",
    "            kernel_size = 3\n",
    "        \n",
    "        try:\n",
    "            mean_filter = cv2.blur(noise_crop.astype(np.float32), (kernel_size, kernel_size))\n",
    "            sqr_filter = cv2.blur((noise_crop ** 2).astype(np.float32), (kernel_size, kernel_size))\n",
    "            variance_map = np.abs(sqr_filter - mean_filter ** 2)\n",
    "            \n",
    "            variance_norm = cv2.normalize(variance_map, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            _, variance_mask = cv2.threshold(variance_norm, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        except:\n",
    "            variance_mask = np.zeros_like(noise_crop, dtype=np.uint8)\n",
    "        \n",
    "        # Method 4: Gradient magnitude analysis\n",
    "        try:\n",
    "            grad_x = cv2.Sobel(noise_crop.astype(np.float32), cv2.CV_64F, 1, 0, ksize=3)\n",
    "            grad_y = cv2.Sobel(noise_crop.astype(np.float32), cv2.CV_64F, 0, 1, ksize=3)\n",
    "            gradient_magnitude = np.sqrt(grad_x ** 2 + grad_y ** 2)\n",
    "            \n",
    "            grad_norm = cv2.normalize(gradient_magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            _, gradient_mask = cv2.threshold(grad_norm, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        except:\n",
    "            gradient_mask = np.zeros_like(noise_crop, dtype=np.uint8)\n",
    "        \n",
    "        # Method 5: Texture consistency\n",
    "        def calculate_local_entropy_fast(img, window_size=15):\n",
    "            try:\n",
    "                kernel = np.ones((window_size, window_size), np.float32) / (window_size ** 2)\n",
    "                mean = cv2.filter2D(img.astype(np.float32), -1, kernel)\n",
    "                mean_sq = cv2.filter2D((img ** 2).astype(np.float32), -1, kernel)\n",
    "                variance = np.abs(mean_sq - mean ** 2)\n",
    "                entropy_approx = np.sqrt(variance)\n",
    "                return entropy_approx\n",
    "            except:\n",
    "                return np.zeros_like(img, dtype=np.float32)\n",
    "        \n",
    "        entropy_map = calculate_local_entropy_fast(noise_crop)\n",
    "        entropy_norm = cv2.normalize(entropy_map, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        _, entropy_mask = cv2.threshold(entropy_norm, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Combine all methods\n",
    "        combined_mask = (\n",
    "            z_mask.astype(np.float32) * 0.25 +\n",
    "            percentile_mask.astype(np.float32) * 0.15 +\n",
    "            variance_mask.astype(np.float32) * 0.25 +\n",
    "            gradient_mask.astype(np.float32) * 0.20 +\n",
    "            entropy_mask.astype(np.float32) * 0.15\n",
    "        )\n",
    "        \n",
    "        combined_mask = cv2.normalize(combined_mask, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        _, final_mask = cv2.threshold(combined_mask, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        try:\n",
    "            skewness = stats.skew(noise_crop.flatten())\n",
    "            kurtosis = stats.kurtosis(noise_crop.flatten())\n",
    "        except:\n",
    "            skewness = 0.0\n",
    "            kurtosis = 0.0\n",
    "        \n",
    "        return final_mask, {\n",
    "            'z_score_mask': z_mask,\n",
    "            'percentile_mask': percentile_mask,\n",
    "            'variance_mask': variance_mask,\n",
    "            'gradient_mask': gradient_mask,\n",
    "            'entropy_mask': entropy_mask,\n",
    "            'combined_mask': combined_mask,\n",
    "            'statistics': {\n",
    "                'mean': float(mean_val),\n",
    "                'std': float(std_val),\n",
    "                'min': float(np.min(noise_crop)),\n",
    "                'max': float(np.max(noise_crop)),\n",
    "                'median': float(np.median(noise_crop)),\n",
    "                'skewness': float(skewness),\n",
    "                'kurtosis': float(kurtosis)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error in analysis: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def detect_distortion_clusters(noise_crop, mask):\n",
    "    \"\"\"Use clustering to identify distinct distorted regions\"\"\"\n",
    "    if mask is None or np.sum(mask) == 0:\n",
    "        return np.zeros_like(noise_crop, dtype=np.uint8)\n",
    "    \n",
    "    try:\n",
    "        coords = np.column_stack(np.where(mask > 0))\n",
    "        \n",
    "        if len(coords) < 10:\n",
    "            return mask\n",
    "        \n",
    "        clustering = DBSCAN(eps=20, min_samples=50).fit(coords)\n",
    "        labels = clustering.labels_\n",
    "        \n",
    "        refined_mask = np.zeros_like(mask)\n",
    "        unique_labels = set(labels)\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            if label == -1:\n",
    "                continue\n",
    "            \n",
    "            cluster_coords = coords[labels == label]\n",
    "            if len(cluster_coords) > 500:\n",
    "                refined_mask[cluster_coords[:, 0], cluster_coords[:, 1]] = 255\n",
    "        \n",
    "        return refined_mask\n",
    "    \n",
    "    except:\n",
    "        return mask\n",
    "\n",
    "def create_comprehensive_visualization(name, orig_img, noise_crop, masks, statistics, output_dir):\n",
    "    \"\"\"Create comprehensive visualization\"\"\"\n",
    "    try:\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "        fig.suptitle(f'Distortion Analysis: {name}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Row 1\n",
    "        axes[0, 0].imshow(cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB))\n",
    "        axes[0, 0].set_title('Original Image', fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(noise_crop, cmap='gray')\n",
    "        axes[0, 1].set_title('Noiseprint', fontweight='bold')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[0, 2].imshow(masks['z_score_mask'], cmap='hot')\n",
    "        axes[0, 2].set_title('Z-Score Anomalies', fontweight='bold')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        axes[0, 3].imshow(masks['percentile_mask'], cmap='hot')\n",
    "        axes[0, 3].set_title('Percentile Outliers', fontweight='bold')\n",
    "        axes[0, 3].axis('off')\n",
    "        \n",
    "        # Row 2\n",
    "        axes[1, 0].imshow(masks['variance_mask'], cmap='hot')\n",
    "        axes[1, 0].set_title('Variance Analysis', fontweight='bold')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(masks['gradient_mask'], cmap='hot')\n",
    "        axes[1, 1].set_title('Gradient Anomalies', fontweight='bold')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(masks['entropy_mask'], cmap='hot')\n",
    "        axes[1, 2].set_title('Entropy Analysis', fontweight='bold')\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        axes[1, 3].imshow(masks['combined_mask'], cmap='hot')\n",
    "        axes[1, 3].set_title('Combined Detection', fontweight='bold')\n",
    "        axes[1, 3].axis('off')\n",
    "        \n",
    "        # Row 3\n",
    "        mask_resized = cv2.resize(masks['combined_mask'], (orig_img.shape[1], orig_img.shape[0]))\n",
    "        overlay = orig_img.copy()\n",
    "        overlay[mask_resized > 128] = [0, 255, 0]\n",
    "        axes[2, 0].imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "        axes[2, 0].set_title('Detection Overlay', fontweight='bold')\n",
    "        axes[2, 0].axis('off')\n",
    "        \n",
    "        heatmap = cv2.applyColorMap(masks['combined_mask'], cv2.COLORMAP_JET)\n",
    "        heatmap_resized = cv2.resize(heatmap, (orig_img.shape[1], orig_img.shape[0]))\n",
    "        heatmap_overlay = cv2.addWeighted(orig_img, 0.7, heatmap_resized, 0.3, 0)\n",
    "        axes[2, 1].imshow(cv2.cvtColor(heatmap_overlay, cv2.COLOR_BGR2RGB))\n",
    "        axes[2, 1].set_title('Heatmap Overlay', fontweight='bold')\n",
    "        axes[2, 1].axis('off')\n",
    "        \n",
    "        stats_text = f\"\"\"Statistical Analysis:\n",
    "\n",
    "Mean:      {statistics['mean']:.4f}\n",
    "Std Dev:   {statistics['std']:.4f}\n",
    "Min:       {statistics['min']:.4f}\n",
    "Max:       {statistics['max']:.4f}\n",
    "Median:    {statistics['median']:.4f}\n",
    "Skewness:  {statistics['skewness']:.4f}\n",
    "Kurtosis:  {statistics['kurtosis']:.4f}\"\"\"\n",
    "        \n",
    "        axes[2, 2].text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center', \n",
    "                       family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        axes[2, 2].axis('off')\n",
    "        axes[2, 2].set_title('Statistics', fontweight='bold')\n",
    "        \n",
    "        axes[2, 3].hist(noise_crop.flatten(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        axes[2, 3].axvline(statistics['mean'], color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "        axes[2, 3].axvline(statistics['median'], color='green', linestyle='--', linewidth=2, label='Median')\n",
    "        axes[2, 3].set_title('Value Distribution', fontweight='bold')\n",
    "        axes[2, 3].legend()\n",
    "        axes[2, 3].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(output_dir, f\"comprehensive_analysis_{name}.png\")\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Visualization failed: {e}\")\n",
    "        plt.close('all')\n",
    "        return False\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PROCESSING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "border = 0\n",
    "image_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\tempfolder\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ADVANCED NOISEPRINT DISTORTION DETECTION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "successful_processing = 0\n",
    "failed_processing = 0\n",
    "skipped_processing = 0\n",
    "\n",
    "for idx, (name, res) in enumerate(noiseprints, 1):\n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"üìÑ [{idx}/{len(noiseprints)}] Processing: {name}\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Crop\n",
    "        if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "            noise_crop = res[border:-border, border:-border]\n",
    "        else:\n",
    "            noise_crop = res\n",
    "        \n",
    "        print(f\"  üìè Noiseprint shape: {noise_crop.shape}\")\n",
    "        \n",
    "        # Skip if too small\n",
    "        if noise_crop.size == 0 or noise_crop.shape[0] < 10 or noise_crop.shape[1] < 10:\n",
    "            print(f\"  ‚ö†Ô∏è  Noiseprint too small or empty, skipping...\")\n",
    "            skipped_processing += 1\n",
    "            continue\n",
    "        \n",
    "        # Numpy analysis\n",
    "        print(\"  üîç Analyzing with multiple methods...\")\n",
    "        numpy_mask, analysis_results = analyze_numpy_values(noise_crop)\n",
    "        \n",
    "        if numpy_mask is None:\n",
    "            print(f\"  ‚ö†Ô∏è  Analysis failed, using fallback method...\")\n",
    "            numpy_mask = np.zeros_like(noise_crop, dtype=np.uint8)\n",
    "            clustered_mask = np.zeros_like(noise_crop, dtype=np.uint8)\n",
    "        else:\n",
    "            print(\"  üéØ Refining detections...\")\n",
    "            clustered_mask = detect_distortion_clusters(noise_crop, numpy_mask)\n",
    "        \n",
    "        # Original method\n",
    "        local_mean = cv2.GaussianBlur(noise_crop.astype(np.float32), (15, 15), 0)\n",
    "        diff_map = np.abs(noise_crop - local_mean)\n",
    "        diff_norm = cv2.normalize(diff_map, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        \n",
    "        diff_thresh = cv2.adaptiveThreshold(diff_norm, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 5)\n",
    "        \n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        diff_clean = cv2.morphologyEx(diff_thresh, cv2.MORPH_OPEN, kernel)\n",
    "        diff_clean = cv2.morphologyEx(diff_clean, cv2.MORPH_DILATE, kernel)\n",
    "        \n",
    "        # Combine methods\n",
    "        target_shape = (diff_clean.shape[1], diff_clean.shape[0])\n",
    "        numpy_mask_resized = cv2.resize(numpy_mask, target_shape, interpolation=cv2.INTER_NEAREST)\n",
    "        clustered_mask_resized = cv2.resize(clustered_mask, target_shape, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        final_combined_mask = (\n",
    "            diff_clean.astype(np.float32) * 0.4 +\n",
    "            numpy_mask_resized.astype(np.float32) * 0.4 +\n",
    "            clustered_mask_resized.astype(np.float32) * 0.2\n",
    "        )\n",
    "        \n",
    "        final_combined_mask = cv2.normalize(final_combined_mask, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        _, final_combined_mask = cv2.threshold(final_combined_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        kernel_final = np.ones((7, 7), np.uint8)\n",
    "        final_combined_mask = cv2.morphologyEx(final_combined_mask, cv2.MORPH_CLOSE, kernel_final)\n",
    "        final_combined_mask = cv2.morphologyEx(final_combined_mask, cv2.MORPH_OPEN, kernel_final)\n",
    "        \n",
    "        # Load original image\n",
    "        image_lookup = {}\n",
    "        for img_path in glob.glob(os.path.join(image_dir, \"*\")):\n",
    "            base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            image_lookup[base] = img_path\n",
    "        \n",
    "        orig_img_path = image_lookup.get(name)\n",
    "        \n",
    "        if orig_img_path is None or not os.path.exists(orig_img_path):\n",
    "            print(f\"  ‚ùå Original image not found for {name}\")\n",
    "            failed_processing += 1\n",
    "            continue\n",
    "        \n",
    "        orig_img = cv2.imread(orig_img_path)\n",
    "        \n",
    "        if orig_img is None:\n",
    "            print(f\"  ‚ùå Could not load image\")\n",
    "            failed_processing += 1\n",
    "            continue\n",
    "        \n",
    "        # Create visualizations\n",
    "        if analysis_results is not None:\n",
    "            analysis_results['combined_mask'] = final_combined_mask\n",
    "            create_comprehensive_visualization(name, orig_img, noise_crop, analysis_results, \n",
    "                                             analysis_results['statistics'], diff_output_dir)\n",
    "        \n",
    "        # Save outputs\n",
    "        heatmap = cv2.applyColorMap(final_combined_mask, cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.resize(heatmap, (orig_img.shape[1], orig_img.shape[0]))\n",
    "        overlay = cv2.addWeighted(orig_img, 0.75, heatmap, 0.25, 0)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(diff_output_dir, f\"diff_highlight_{name}.png\"), overlay)\n",
    "        \n",
    "        mask_resized = cv2.resize(final_combined_mask, (orig_img.shape[1], orig_img.shape[0]), \n",
    "                                 interpolation=cv2.INTER_NEAREST)\n",
    "        binary_mask = mask_resized > 0\n",
    "        \n",
    "        forged_only = np.zeros_like(orig_img)\n",
    "        forged_only[binary_mask] = orig_img[binary_mask]\n",
    "        cv2.imwrite(os.path.join(diff_output_dir, f\"forged_regions_only_{name}.png\"), forged_only)\n",
    "        \n",
    "        alpha = np.zeros((orig_img.shape[0], orig_img.shape[1]), dtype=np.uint8)\n",
    "        alpha[binary_mask] = 255\n",
    "        forged_rgba = np.dstack([orig_img, alpha])\n",
    "        cv2.imwrite(os.path.join(diff_output_dir, f\"forged_regions_transparent_{name}.png\"), forged_rgba)\n",
    "        \n",
    "        # Save method results\n",
    "        if analysis_results is not None:\n",
    "            methods_dir = os.path.join(diff_output_dir, f\"{name}_methods\")\n",
    "            os.makedirs(methods_dir, exist_ok=True)\n",
    "            \n",
    "            cv2.imwrite(os.path.join(methods_dir, \"z_score.png\"), analysis_results['z_score_mask'])\n",
    "            cv2.imwrite(os.path.join(methods_dir, \"percentile.png\"), analysis_results['percentile_mask'])\n",
    "            cv2.imwrite(os.path.join(methods_dir, \"variance.png\"), analysis_results['variance_mask'])\n",
    "            cv2.imwrite(os.path.join(methods_dir, \"gradient.png\"), analysis_results['gradient_mask'])\n",
    "            cv2.imwrite(os.path.join(methods_dir, \"entropy.png\"), analysis_results['entropy_mask'])\n",
    "            cv2.imwrite(os.path.join(methods_dir, \"final_combined.png\"), final_combined_mask)\n",
    "        \n",
    "        # Detection metrics\n",
    "        num_labels, labels, stats_cc, centroids = cv2.connectedComponentsWithStats(mask_resized, connectivity=8)\n",
    "        \n",
    "        print(f\"\\n  üìà Detection Results:\")\n",
    "        print(f\"     Detected regions: {num_labels - 1}\")\n",
    "        print(f\"     Forged pixels: {np.sum(binary_mask):,}\")\n",
    "        print(f\"     Forged percentage: {(np.sum(binary_mask) / binary_mask.size) * 100:.2f}%\")\n",
    "        \n",
    "        print(f\"\\n  ‚úÖ Successfully processed!\")\n",
    "        successful_processing += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ‚ùå ERROR: {e}\")\n",
    "        failed_processing += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PROCESSING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"‚úÖ Successfully processed: {successful_processing} images\")\n",
    "print(f\"‚ö†Ô∏è  Skipped (too small):    {skipped_processing} images\")\n",
    "print(f\"‚ùå Failed:                 {failed_processing} images\")\n",
    "print(f\"üìÅ Results saved to: {diff_output_dir}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diff_output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprint_diff_highlight\"\n",
    "os.makedirs(diff_output_dir, exist_ok=True)\n",
    "\n",
    "for name, res in noiseprints:\n",
    "\n",
    "    # ---------- Crop (same as before) ----------\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        noise_crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        noise_crop = res\n",
    "\n",
    "    # ---------- Local smoothing ----------\n",
    "    local_mean = cv2.GaussianBlur(noise_crop, (15, 15), 0)\n",
    "\n",
    "    # ---------- Difference map ----------\n",
    "    diff_map = np.abs(noise_crop - local_mean)\n",
    "\n",
    "    # ---------- Normalize ----------\n",
    "    diff_norm = cv2.normalize(diff_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    diff_norm = diff_norm.astype(np.uint8)\n",
    "\n",
    "    # ---------- Adaptive threshold ----------\n",
    "    diff_thresh = cv2.adaptiveThreshold(\n",
    "        diff_norm,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "        cv2.THRESH_BINARY,\n",
    "        21,\n",
    "        5\n",
    "    )\n",
    "\n",
    "    # ---------- Morphological cleanup ----------\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    diff_clean = cv2.morphologyEx(diff_thresh, cv2.MORPH_OPEN, kernel)\n",
    "    diff_clean = cv2.morphologyEx(diff_clean, cv2.MORPH_DILATE, kernel)\n",
    "\n",
    "    # ---------- Heatmap ----------\n",
    "    heatmap = cv2.applyColorMap(diff_norm, cv2.COLORMAP_JET)\n",
    "\n",
    "    image_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\tempfolder\"\n",
    "\n",
    "    image_lookup = {}\n",
    "\n",
    "    for img_path in glob.glob(os.path.join(image_dir, \"*\")):\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        image_lookup[base] = img_path\n",
    "\n",
    "    # ---------- Load original image ----------\n",
    "    orig_img_path = image_lookup.get(name)\n",
    "\n",
    "    if orig_img_path is None:\n",
    "        print(f\"No matching original image for {name}\")\n",
    "        continue\n",
    "\n",
    "    orig_img = cv2.imread(orig_img_path)\n",
    "\n",
    "    # if orig_img is None:\n",
    "    #     print(f\"Original image not found for {name}, skipping overlay.\")\n",
    "    #     continue\n",
    "\n",
    "    # orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize if shapes mismatch\n",
    "    heatmap = cv2.resize(heatmap, (orig_img.shape[1], orig_img.shape[0]))\n",
    "\n",
    "    # ---------- Overlay ----------\n",
    "    overlay = cv2.addWeighted(orig_img, 0.75, heatmap, 0.25, 0)\n",
    "\n",
    "    # ---------- Save ----------\n",
    "    save_path = os.path.join(diff_output_dir, f\"diff_highlight_{name}.png\")\n",
    "    plt.imsave(save_path, overlay)\n",
    "\n",
    "    print(f\"Saved difference highlight for {name}\")\n",
    "    mask_resized = cv2.resize(\n",
    "        diff_clean,\n",
    "        (orig_img.shape[1], orig_img.shape[0]),\n",
    "        interpolation=cv2.INTER_NEAREST\n",
    "    )\n",
    "\n",
    "        # Convert mask to boolean\n",
    "    binary_mask = mask_resized > 0\n",
    "\n",
    "        # Create black background\n",
    "    forged_only = np.zeros_like(orig_img)\n",
    "\n",
    "        # Copy only forged regions\n",
    "    forged_only[binary_mask] = orig_img[binary_mask]\n",
    "\n",
    "        # ---------- Save forged-only image ----------\n",
    "    forged_only_path = os.path.join(\n",
    "            diff_output_dir,\n",
    "            f\"forged_regions_only_{name}.png\"\n",
    "        )\n",
    "    plt.imsave(forged_only_path, forged_only)\n",
    "\n",
    "    alpha = np.zeros(\n",
    "        (orig_img.shape[0], orig_img.shape[1]),\n",
    "        dtype=np.uint8\n",
    "    )\n",
    "    alpha[binary_mask] = 255\n",
    "\n",
    "    forged_rgba = np.dstack([orig_img, alpha])\n",
    "\n",
    "    forged_transparent_path = os.path.join(\n",
    "        diff_output_dir,\n",
    "        f\"forged_regions_transparent_{name}.png\"\n",
    "    )\n",
    "    plt.imsave(forged_transparent_path, forged_rgba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e22d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from skimage import exposure, filters\n",
    "import os\n",
    "\n",
    "class NoiseprintAnalyzer:\n",
    "    def __init__(self, image_path):\n",
    "        \"\"\"Initialize with noiseprint image path\"\"\"\n",
    "        self.image = cv2.imread(image_path)\n",
    "        if self.image is None:\n",
    "            raise ValueError(f\"Could not load image from {image_path}\")\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        if len(self.image.shape) == 3:\n",
    "            self.gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            self.gray = self.image.copy()\n",
    "        \n",
    "        print(f\"Image loaded: {self.gray.shape}\")\n",
    "    \n",
    "    def enhance_contrast(self, image, clip_limit=2.0):\n",
    "        \"\"\"Apply CLAHE for contrast enhancement\"\"\"\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8,8))\n",
    "        return clahe.apply(image)\n",
    "    \n",
    "    def detect_anomalies_variance(self, window_size=5):\n",
    "        \"\"\"Detect anomalies using local variance analysis\"\"\"\n",
    "        # Calculate local variance\n",
    "        mean = ndimage.uniform_filter(self.gray.astype(float), size=window_size)\n",
    "        mean_sq = ndimage.uniform_filter(self.gray.astype(float)**2, size=window_size)\n",
    "        variance = mean_sq - mean**2\n",
    "        \n",
    "        # Normalize\n",
    "        variance_norm = (variance - variance.min()) / (variance.max() - variance.min() + 1e-8)\n",
    "        return variance_norm\n",
    "    \n",
    "    def detect_anomalies_gradient(self):\n",
    "        \"\"\"Detect anomalies using gradient analysis\"\"\"\n",
    "        # Calculate gradients\n",
    "        sobelx = cv2.Sobel(self.gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobely = cv2.Sobel(self.gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        \n",
    "        # Gradient magnitude\n",
    "        gradient = np.sqrt(sobelx**2 + sobely**2)\n",
    "        gradient_norm = (gradient - gradient.min()) / (gradient.max() - gradient.min() + 1e-8)\n",
    "        \n",
    "        return gradient_norm\n",
    "    \n",
    "    def detect_anomalies_frequency(self):\n",
    "        \"\"\"Detect anomalies using frequency domain analysis\"\"\"\n",
    "        # FFT\n",
    "        f_transform = np.fft.fft2(self.gray)\n",
    "        f_shift = np.fft.fftshift(f_transform)\n",
    "        magnitude = np.abs(f_shift)\n",
    "        \n",
    "        # High-pass filter to detect inconsistencies\n",
    "        rows, cols = self.gray.shape\n",
    "        crow, ccol = rows // 2, cols // 2\n",
    "        mask = np.ones((rows, cols), np.uint8)\n",
    "        r = 30\n",
    "        center = [crow, ccol]\n",
    "        x, y = np.ogrid[:rows, :cols]\n",
    "        mask_area = (x - center[0])**2 + (y - center[1])**2 <= r*r\n",
    "        mask[mask_area] = 0\n",
    "        \n",
    "        # Apply mask\n",
    "        f_shift_filtered = f_shift * mask\n",
    "        f_ishift = np.fft.ifftshift(f_shift_filtered)\n",
    "        img_back = np.fft.ifft2(f_ishift)\n",
    "        img_back = np.abs(img_back)\n",
    "        \n",
    "        # Normalize\n",
    "        img_back_norm = (img_back - img_back.min()) / (img_back.max() - img_back.min() + 1e-8)\n",
    "        return img_back_norm\n",
    "    \n",
    "    def apply_threshold(self, anomaly_map, threshold=0.5):\n",
    "        \"\"\"Apply threshold to create binary mask\"\"\"\n",
    "        binary_mask = (anomaly_map > threshold).astype(np.uint8) * 255\n",
    "        \n",
    "        # Morphological operations to clean up\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n",
    "        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        return binary_mask\n",
    "    \n",
    "    def create_heatmap(self, anomaly_map):\n",
    "        \"\"\"Create heatmap visualization\"\"\"\n",
    "        # Apply colormap\n",
    "        anomaly_map_uint8 = (anomaly_map * 255).astype(np.uint8)\n",
    "        heatmap = cv2.applyColorMap(anomaly_map_uint8, cv2.COLORMAP_JET)\n",
    "        return heatmap\n",
    "    \n",
    "    def highlight_regions(self, anomaly_map, threshold=0.5):\n",
    "        \"\"\"Highlight suspicious regions on original image\"\"\"\n",
    "        # Create binary mask\n",
    "        binary_mask = self.apply_threshold(anomaly_map, threshold)\n",
    "        \n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Draw on original image\n",
    "        result = self.image.copy() if len(self.image.shape) == 3 else cv2.cvtColor(self.gray, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 100:  # Filter small regions\n",
    "                # Draw contour\n",
    "                cv2.drawContours(result, [contour], -1, (0, 0, 255), 2)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                \n",
    "                # Add label\n",
    "                cv2.putText(result, f\"Area: {area:.0f}\", (x, y-10), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        \n",
    "        return result, len(contours)\n",
    "    \n",
    "    def analyze_complete(self, threshold=0.5, save_path='results'):\n",
    "        \"\"\"Complete analysis with all methods\"\"\"\n",
    "        # Create output directory\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        print(\"Analyzing noiseprint...\")\n",
    "        \n",
    "        # 1. Contrast enhancement\n",
    "        enhanced = self.enhance_contrast(self.gray)\n",
    "        \n",
    "        # 2. Variance-based detection\n",
    "        print(\"Computing variance map...\")\n",
    "        variance_map = self.detect_anomalies_variance()\n",
    "        \n",
    "        # 3. Gradient-based detection\n",
    "        print(\"Computing gradient map...\")\n",
    "        gradient_map = self.detect_anomalies_gradient()\n",
    "        \n",
    "        # 4. Frequency-based detection\n",
    "        print(\"Computing frequency map...\")\n",
    "        frequency_map = self.detect_anomalies_frequency()\n",
    "        \n",
    "        # 5. Combined anomaly map\n",
    "        print(\"Creating combined anomaly map...\")\n",
    "        combined_map = (variance_map * 0.4 + gradient_map * 0.3 + frequency_map * 0.3)\n",
    "        combined_map = (combined_map - combined_map.min()) / (combined_map.max() - combined_map.min() + 1e-8)\n",
    "        \n",
    "        # 6. Create visualizations\n",
    "        print(\"Creating visualizations...\")\n",
    "        heatmap = self.create_heatmap(combined_map)\n",
    "        highlighted, num_regions = self.highlight_regions(combined_map, threshold)\n",
    "        binary_mask = self.apply_threshold(combined_map, threshold)\n",
    "        \n",
    "        # 7. Create comparison figure\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
    "        \n",
    "        axes[0, 0].imshow(self.gray, cmap='gray')\n",
    "        axes[0, 0].set_title('Original Noiseprint', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(enhanced, cmap='gray')\n",
    "        axes[0, 1].set_title('Contrast Enhanced', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[0, 2].imshow(variance_map, cmap='hot')\n",
    "        axes[0, 2].set_title('Variance Map', fontsize=12, fontweight='bold')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        axes[1, 0].imshow(gradient_map, cmap='hot')\n",
    "        axes[1, 0].set_title('Gradient Map', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(frequency_map, cmap='hot')\n",
    "        axes[1, 1].set_title('Frequency Map', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(combined_map, cmap='hot')\n",
    "        axes[1, 2].set_title('Combined Anomaly Map', fontsize=12, fontweight='bold')\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        axes[2, 0].imshow(cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB))\n",
    "        axes[2, 0].set_title('Heatmap Visualization', fontsize=12, fontweight='bold')\n",
    "        axes[2, 0].axis('off')\n",
    "        \n",
    "        axes[2, 1].imshow(binary_mask, cmap='gray')\n",
    "        axes[2, 1].set_title(f'Binary Mask (threshold={threshold})', fontsize=12, fontweight='bold')\n",
    "        axes[2, 1].axis('off')\n",
    "        \n",
    "        axes[2, 2].imshow(cv2.cvtColor(highlighted, cv2.COLOR_BGR2RGB))\n",
    "        axes[2, 2].set_title(f'Detected Regions: {num_regions}', fontsize=12, fontweight='bold')\n",
    "        axes[2, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_path}/complete_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved complete analysis to {save_path}/complete_analysis.png\")\n",
    "        \n",
    "        # Save individual results\n",
    "        cv2.imwrite(f'{save_path}/heatmap.png', heatmap)\n",
    "        cv2.imwrite(f'{save_path}/highlighted_regions.png', highlighted)\n",
    "        cv2.imwrite(f'{save_path}/binary_mask.png', binary_mask)\n",
    "        cv2.imwrite(f'{save_path}/combined_anomaly_map.png', (combined_map * 255).astype(np.uint8))\n",
    "        \n",
    "        print(f\"\\nAnalysis complete!\")\n",
    "        print(f\"Found {num_regions} suspicious regions\")\n",
    "        print(f\"Results saved to '{save_path}/' directory\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return combined_map, highlighted, num_regions\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your noiseprint image\n",
    "    image_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprint_diff_highlight\\\\forged_regions_transparent_new.png\"  # Change this to your image path\n",
    "    \n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = NoiseprintAnalyzer(image_path)\n",
    "        \n",
    "        # Run complete analysis\n",
    "        # Adjust threshold: higher = more sensitive (0.0 to 1.0)\n",
    "        anomaly_map, highlighted, num_regions = analyzer.analyze_complete(\n",
    "            threshold=0.5,\n",
    "            save_path='tampering_results'\n",
    "        )\n",
    "        \n",
    "        print(\"\\nTo adjust sensitivity, modify the threshold parameter:\")\n",
    "        print(\"  threshold=0.3  -> Less sensitive (fewer detections)\")\n",
    "        print(\"  threshold=0.5  -> Medium sensitivity\")\n",
    "        print(\"  threshold=0.7  -> More sensitive (more detections)\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find image at '{image_path}'\")\n",
    "        print(\"Please update the image_path variable with your noiseprint image location\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad17ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import os\n",
    "\n",
    "class NoiseprintAnalyzer:\n",
    "    def __init__(self, image_path):\n",
    "        \"\"\"Initialize with noiseprint image path\"\"\"\n",
    "        self.image = cv2.imread(image_path)\n",
    "        if self.image is None:\n",
    "            raise ValueError(f\"Could not load image from {image_path}\")\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        if len(self.image.shape) == 3:\n",
    "            self.gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            self.gray = self.image.copy()\n",
    "        \n",
    "        print(f\"Image loaded: {self.gray.shape}\")\n",
    "    \n",
    "    def enhance_contrast(self, image, clip_limit=3.0):\n",
    "        \"\"\"Apply CLAHE for contrast enhancement\"\"\"\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8,8))\n",
    "        return clahe.apply(image)\n",
    "    \n",
    "    def detect_anomalies_variance(self, window_size=7):\n",
    "        \"\"\"Detect anomalies using local variance analysis\"\"\"\n",
    "        # Calculate local variance\n",
    "        mean = ndimage.uniform_filter(self.gray.astype(float), size=window_size)\n",
    "        mean_sq = ndimage.uniform_filter(self.gray.astype(float)**2, size=window_size)\n",
    "        variance = mean_sq - mean**2\n",
    "        \n",
    "        # Normalize\n",
    "        variance_norm = (variance - variance.min()) / (variance.max() - variance.min() + 1e-8)\n",
    "        return variance_norm\n",
    "    \n",
    "    def detect_anomalies_gradient(self):\n",
    "        \"\"\"Detect anomalies using gradient analysis\"\"\"\n",
    "        # Calculate gradients with larger kernel\n",
    "        sobelx = cv2.Sobel(self.gray, cv2.CV_64F, 1, 0, ksize=5)\n",
    "        sobely = cv2.Sobel(self.gray, cv2.CV_64F, 0, 1, ksize=5)\n",
    "        \n",
    "        # Gradient magnitude\n",
    "        gradient = np.sqrt(sobelx**2 + sobely**2)\n",
    "        gradient_norm = (gradient - gradient.min()) / (gradient.max() - gradient.min() + 1e-8)\n",
    "        \n",
    "        return gradient_norm\n",
    "    \n",
    "    def detect_edges_canny(self):\n",
    "        \"\"\"Detect edges using Canny edge detector\"\"\"\n",
    "        # Apply Gaussian blur\n",
    "        blurred = cv2.GaussianBlur(self.gray, (5, 5), 0)\n",
    "        \n",
    "        # Canny edge detection\n",
    "        edges = cv2.Canny(blurred, 30, 100)\n",
    "        \n",
    "        # Dilate edges to make them more prominent\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        edges_dilated = cv2.dilate(edges, kernel, iterations=1)\n",
    "        \n",
    "        # Normalize\n",
    "        edges_norm = edges_dilated.astype(float) / 255.0\n",
    "        \n",
    "        return edges_norm\n",
    "    \n",
    "    def apply_adaptive_threshold(self, anomaly_map, method='otsu'):\n",
    "        \"\"\"Apply adaptive thresholding\"\"\"\n",
    "        # Convert to uint8\n",
    "        anomaly_uint8 = (anomaly_map * 255).astype(np.uint8)\n",
    "        \n",
    "        if method == 'otsu':\n",
    "            # Otsu's thresholding\n",
    "            _, binary = cv2.threshold(anomaly_uint8, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        elif method == 'adaptive':\n",
    "            # Adaptive thresholding\n",
    "            binary = cv2.adaptiveThreshold(anomaly_uint8, 255, \n",
    "                                          cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                          cv2.THRESH_BINARY, 11, 2)\n",
    "        else:\n",
    "            # Manual threshold\n",
    "            threshold_val = int(method * 255) if isinstance(method, float) else 127\n",
    "            _, binary = cv2.threshold(anomaly_uint8, threshold_val, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Morphological operations with larger kernel\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "        \n",
    "        # Remove small noise\n",
    "        kernel_small = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_small, iterations=1)\n",
    "        \n",
    "        return binary\n",
    "    \n",
    "    def multi_threshold_detection(self, anomaly_map):\n",
    "        \"\"\"Apply multiple threshold levels and combine\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Try multiple thresholds\n",
    "        thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            binary = self.apply_adaptive_threshold(anomaly_map, method=thresh)\n",
    "            results.append(binary)\n",
    "        \n",
    "        # Combine using voting\n",
    "        combined = np.zeros_like(results[0], dtype=np.uint8)\n",
    "        for binary in results:\n",
    "            combined += (binary > 0).astype(np.uint8)\n",
    "        \n",
    "        # Keep pixels that appear in at least 2 thresholds\n",
    "        combined = ((combined >= 2) * 255).astype(np.uint8)\n",
    "        \n",
    "        # Clean up\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "        combined = cv2.morphologyEx(combined, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def create_heatmap(self, anomaly_map):\n",
    "        \"\"\"Create heatmap visualization\"\"\"\n",
    "        anomaly_map_uint8 = (anomaly_map * 255).astype(np.uint8)\n",
    "        heatmap = cv2.applyColorMap(anomaly_map_uint8, cv2.COLORMAP_JET)\n",
    "        return heatmap\n",
    "    \n",
    "    def highlight_regions_improved(self, anomaly_map, binary_mask):\n",
    "        \"\"\"Highlight suspicious regions with improved visualization\"\"\"\n",
    "        # Find contours\n",
    "        contours, hierarchy = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Create result image\n",
    "        result = self.image.copy() if len(self.image.shape) == 3 else cv2.cvtColor(self.gray, cv2.COLOR_GRAY2BGR)\n",
    "        overlay = result.copy()\n",
    "        \n",
    "        # Sort contours by area (largest first)\n",
    "        contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "        \n",
    "        significant_regions = 0\n",
    "        region_info = []\n",
    "        \n",
    "        for idx, contour in enumerate(contours):\n",
    "            area = cv2.contourArea(contour)\n",
    "            \n",
    "            # Filter by minimum area\n",
    "            if area > 200:  # Minimum area threshold\n",
    "                significant_regions += 1\n",
    "                \n",
    "                # Get bounding box\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                \n",
    "                # Calculate average anomaly score in this region\n",
    "                mask = np.zeros_like(binary_mask)\n",
    "                cv2.drawContours(mask, [contour], -1, 255, -1)\n",
    "                avg_score = np.mean(anomaly_map[mask > 0])\n",
    "                \n",
    "                region_info.append({\n",
    "                    'id': significant_regions,\n",
    "                    'area': area,\n",
    "                    'bbox': (x, y, w, h),\n",
    "                    'score': avg_score\n",
    "                })\n",
    "                \n",
    "                # Color based on confidence\n",
    "                if avg_score > 0.7:\n",
    "                    color = (0, 0, 255)  # Red - High confidence\n",
    "                    label = f\"HIGH RISK #{significant_regions}\"\n",
    "                elif avg_score > 0.5:\n",
    "                    color = (0, 165, 255)  # Orange - Medium\n",
    "                    label = f\"MEDIUM #{significant_regions}\"\n",
    "                else:\n",
    "                    color = (0, 255, 255)  # Yellow - Low\n",
    "                    label = f\"LOW #{significant_regions}\"\n",
    "                \n",
    "                # Fill contour with transparency\n",
    "                cv2.drawContours(overlay, [contour], -1, color, -1)\n",
    "                \n",
    "                # Draw thick border\n",
    "                cv2.drawContours(result, [contour], -1, color, 3)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                \n",
    "                # Add label with background\n",
    "                label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                cv2.rectangle(result, (x, y - label_size[1] - 10), \n",
    "                            (x + label_size[0], y), color, -1)\n",
    "                cv2.putText(result, label, (x, y - 5), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                \n",
    "                # Add area info\n",
    "                cv2.putText(result, f\"Area: {int(area)}\", (x, y + h + 20), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        \n",
    "        # Blend overlay\n",
    "        result = cv2.addWeighted(result, 0.6, overlay, 0.4, 0)\n",
    "        \n",
    "        return result, significant_regions, region_info\n",
    "    \n",
    "    def create_detailed_mask(self, anomaly_map):\n",
    "        \"\"\"Create a detailed mask showing different confidence levels\"\"\"\n",
    "        h, w = anomaly_map.shape\n",
    "        mask_colored = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Define confidence levels\n",
    "        high_conf = anomaly_map > 0.7\n",
    "        medium_conf = (anomaly_map > 0.5) & (anomaly_map <= 0.7)\n",
    "        low_conf = (anomaly_map > 0.3) & (anomaly_map <= 0.5)\n",
    "        \n",
    "        # Color code\n",
    "        mask_colored[high_conf] = [0, 0, 255]      # Red\n",
    "        mask_colored[medium_conf] = [0, 165, 255]  # Orange\n",
    "        mask_colored[low_conf] = [0, 255, 255]     # Yellow\n",
    "        \n",
    "        return mask_colored\n",
    "    \n",
    "    def analyze_complete(self, save_path='results'):\n",
    "        \"\"\"Complete analysis with improved detection\"\"\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"IMPROVED NOISEPRINT TAMPERING DETECTION\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Image size: {self.gray.shape[1]}x{self.gray.shape[0]}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Step 1: Enhance contrast\n",
    "        print(\"[1/8] Enhancing contrast...\")\n",
    "        enhanced = self.enhance_contrast(self.gray)\n",
    "        \n",
    "        # Step 2: Variance detection\n",
    "        print(\"[2/8] Computing variance map...\")\n",
    "        variance_map = self.detect_anomalies_variance(window_size=7)\n",
    "        \n",
    "        # Step 3: Gradient detection\n",
    "        print(\"[3/8] Computing gradient map...\")\n",
    "        gradient_map = self.detect_anomalies_gradient()\n",
    "        \n",
    "        # Step 4: Edge detection\n",
    "        print(\"[4/8] Detecting edges...\")\n",
    "        edges_map = self.detect_edges_canny()\n",
    "        \n",
    "        # Step 5: Combined anomaly map\n",
    "        print(\"[5/8] Creating combined anomaly map...\")\n",
    "        # Weight variance and gradient more heavily\n",
    "        combined_map = (variance_map * 0.5 + gradient_map * 0.35 + edges_map * 0.15)\n",
    "        combined_map = (combined_map - combined_map.min()) / (combined_map.max() - combined_map.min() + 1e-8)\n",
    "        \n",
    "        # Step 6: Apply Otsu thresholding\n",
    "        print(\"[6/8] Applying Otsu's thresholding...\")\n",
    "        binary_otsu = self.apply_adaptive_threshold(combined_map, method='otsu')\n",
    "        \n",
    "        # Step 7: Multi-threshold detection\n",
    "        print(\"[7/8] Applying multi-threshold detection...\")\n",
    "        binary_multi = self.multi_threshold_detection(combined_map)\n",
    "        \n",
    "        # Step 8: Create visualizations\n",
    "        print(\"[8/8] Creating visualizations...\")\n",
    "        heatmap = self.create_heatmap(combined_map)\n",
    "        detailed_mask = self.create_detailed_mask(combined_map)\n",
    "        highlighted_otsu, num_regions_otsu, regions_otsu = self.highlight_regions_improved(combined_map, binary_otsu)\n",
    "        highlighted_multi, num_regions_multi, regions_multi = self.highlight_regions_improved(combined_map, binary_multi)\n",
    "        \n",
    "        # Create comprehensive figure\n",
    "        fig = plt.figure(figsize=(24, 16))\n",
    "        \n",
    "        # Row 1: Original processing\n",
    "        plt.subplot(3, 4, 1)\n",
    "        plt.imshow(self.gray, cmap='gray')\n",
    "        plt.title('Original Noiseprint', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, 2)\n",
    "        plt.imshow(enhanced, cmap='gray')\n",
    "        plt.title('Contrast Enhanced', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, 3)\n",
    "        plt.imshow(variance_map, cmap='hot')\n",
    "        plt.title('Variance Map (Hotspots)', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, 4)\n",
    "        plt.imshow(gradient_map, cmap='hot')\n",
    "        plt.title('Gradient Map (Edges)', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Row 2: Detection maps\n",
    "        plt.subplot(3, 4, 5)\n",
    "        plt.imshow(edges_map, cmap='hot')\n",
    "        plt.title('Edge Detection', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, 6)\n",
    "        plt.imshow(combined_map, cmap='hot')\n",
    "        plt.title('Combined Anomaly Map', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, 7)\n",
    "        plt.imshow(cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Heatmap Visualization', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, 8)\n",
    "        plt.imshow(cv2.cvtColor(detailed_mask, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Confidence Levels', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Row 3: Final results\n",
    "        plt.subplot(3, 4, 9)\n",
    "        plt.imshow(binary_otsu, cmap='gray')\n",
    "        plt.title(\"Binary Mask (Otsu's Method)\", fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, 10)\n",
    "        plt.imshow(binary_multi, cmap='gray')\n",
    "        plt.title('Binary Mask (Multi-Threshold)', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, 11)\n",
    "        plt.imshow(cv2.cvtColor(highlighted_otsu, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Otsu Detection: {num_regions_otsu} Regions\", fontsize=14, fontweight='bold', color='red')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 4, 12)\n",
    "        plt.imshow(cv2.cvtColor(highlighted_multi, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Multi-Threshold: {num_regions_multi} Regions\", fontsize=14, fontweight='bold', color='red')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_path}/complete_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n‚úì Saved: {save_path}/complete_analysis.png\")\n",
    "        \n",
    "        # Save all outputs\n",
    "        cv2.imwrite(f'{save_path}/01_enhanced.png', enhanced)\n",
    "        cv2.imwrite(f'{save_path}/02_variance_map.png', (variance_map * 255).astype(np.uint8))\n",
    "        cv2.imwrite(f'{save_path}/03_gradient_map.png', (gradient_map * 255).astype(np.uint8))\n",
    "        cv2.imwrite(f'{save_path}/04_combined_map.png', (combined_map * 255).astype(np.uint8))\n",
    "        cv2.imwrite(f'{save_path}/05_heatmap.png', heatmap)\n",
    "        cv2.imwrite(f'{save_path}/06_detailed_mask.png', detailed_mask)\n",
    "        cv2.imwrite(f'{save_path}/07_binary_otsu.png', binary_otsu)\n",
    "        cv2.imwrite(f'{save_path}/08_binary_multi.png', binary_multi)\n",
    "        cv2.imwrite(f'{save_path}/09_detected_otsu.png', highlighted_otsu)\n",
    "        cv2.imwrite(f'{save_path}/10_detected_multi.png', highlighted_multi)\n",
    "        \n",
    "        # Print detailed report\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"DETECTION REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nOtsu's Method: {num_regions_otsu} suspicious regions detected\")\n",
    "        if regions_otsu:\n",
    "            for region in regions_otsu[:3]:  # Show top 3\n",
    "                print(f\"  Region #{region['id']}: Area={region['area']:.0f}, Score={region['score']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nMulti-Threshold: {num_regions_multi} suspicious regions detected\")\n",
    "        if regions_multi:\n",
    "            for region in regions_multi[:3]:  # Show top 3\n",
    "                print(f\"  Region #{region['id']}: Area={region['area']:.0f}, Score={region['score']:.2f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COLOR CODE:\")\n",
    "        print(\"  üî¥ RED = High confidence tampering (score > 0.7)\")\n",
    "        print(\"  üü† ORANGE = Medium confidence (score 0.5-0.7)\")\n",
    "        print(\"  üü° YELLOW = Low confidence (score 0.3-0.5)\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nAll results saved to '{save_path}/' directory\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return combined_map, highlighted_multi, num_regions_multi, regions_multi\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # CHANGE THIS to your noiseprint image path\n",
    "    image_path = \"noiseprint_image.png\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"IMPROVED NOISEPRINT TAMPERING DETECTOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        analyzer = NoiseprintAnalyzer(image_path)\n",
    "        combined_map, highlighted, num_regions, region_info = analyzer.analyze_complete(\n",
    "            save_path='tampering_results'\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Analysis Complete!\")\n",
    "        print(\"\\nKey improvements in this version:\")\n",
    "        print(\"  ‚Ä¢ Better binary mask using Otsu's thresholding\")\n",
    "        print(\"  ‚Ä¢ Multi-threshold voting for robust detection\")\n",
    "        print(\"  ‚Ä¢ Larger morphological kernels to capture full regions\")\n",
    "        print(\"  ‚Ä¢ Confidence-based color coding\")\n",
    "        print(\"  ‚Ä¢ Detailed region analysis\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n‚ùå ERROR: Image not found at '{image_path}'\")\n",
    "        print(\"\\nUpdate the image_path variable, for example:\")\n",
    "        print('  image_path = r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprint_diff_highlight\\\\forged_regions_transparent_new.png\"')\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "\n",
    "# examples=[]\n",
    "# noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "#      example,noise_print=getNoiseprint(img_path)\n",
    "#      examples.append(example)\n",
    "#      print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "#      noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "# output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# border = 34\n",
    "\n",
    "# # Save cropped and normalized noiseprints\n",
    "# for name, res in noiseprints:\n",
    "#     if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "#         crop = res[border:-border, border:-border]\n",
    "#     else:\n",
    "#         crop = res\n",
    "\n",
    "#     # Normalize for saving\n",
    "#     vmin = np.min(crop)\n",
    "#     vmax = np.max(crop)\n",
    "#     norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "#     # Save with name\n",
    "#     filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "#     plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "# class RealFakeCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RealFakeCNN, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(128 * 16 * 16, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Pad(padding=10, fill=0),\n",
    "#     transforms.CenterCrop(128),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "# ])\n",
    "# model = RealFakeCNN()\n",
    "# def predict_image(image_path, model, class_names):\n",
    "    \n",
    "#     model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "#     image = image.to(next(model.parameters()))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         output = model(image)\n",
    "#         prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "#         predicted = int(prob > 0.7)\n",
    "\n",
    "#     return class_names[predicted], prob\n",
    "\n",
    "# folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "# class_names = ['fake','real']  # or model.class_names if stored\n",
    "# results = []\n",
    "\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "#         img_path = os.path.join(folder_path, filename)\n",
    "#         label, score = predict_image(img_path, model, class_names)\n",
    "#         results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# # Display\n",
    "# for fname, label, prob in results:\n",
    "#     print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "#     os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "# end = timer()\n",
    "\n",
    "# print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((572, 572)),   # Resize to fixed 572x572\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "def predict_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()).device)  # Send to same device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # shape: [1,1]\n",
    "        prob = output.item()   # probability from sigmoid\n",
    "\n",
    "    # Decide class\n",
    "    predicted_class = 1 if prob >= 0.5 else 0  # threshold at 0.5\n",
    "    return class_names[predicted_class], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\Forged_Aadhaar\\\\Balanced_Data\\\\Balanced_Data\\\\train\\\\real\"\n",
    "# folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\tempdir\\\\new_temp\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95afd8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7672ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from Noiseprint import getNoiseprint\n",
    "# Generate noiseprints\n",
    "examples = []\n",
    "noiseprints = []\n",
    "\n",
    "for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\Forged_Aadhaar\\\\tempfolder\\\\*'):\n",
    "    example, noise_print = getNoiseprint(img_path)\n",
    "    examples.append(example)\n",
    "    print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "    noiseprints.append([img_path.split('\\\\')[-1].split('.')[0], noise_print])\n",
    "\n",
    "# Save noiseprints\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\Forged_Aadhaar\\\\tempnewfile\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "# Prediction setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((572, 572)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "def predict_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    image = image.to(next(model.parameters()).device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = output.item()\n",
    "\n",
    "    predicted_class = 1 if prob >= 0.5 else 0\n",
    "    return class_names[predicted_class], prob\n",
    "\n",
    "# Predict on the NOISEPRINT images\n",
    "noiseprint_folder = output_dir  # Use the saved noiseprints folder\n",
    "class_names = ['fake', 'real']\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(noiseprint_folder):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(noiseprint_folder, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display results\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eceaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85548811",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\71_forged10.png'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\85_forged10.png'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\71_forged1.png'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\71_forged10.png'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5862758",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\49.jpeg'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ac51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# from timeit import default_timer as timer\n",
    "# import cv2\n",
    "\n",
    "# # ------------------ Noiseprint Extraction ------------------ #\n",
    "# def getNoiseprint(img_path):\n",
    "#     def extract_noiseprint(image_array):\n",
    "#         height, width = image_array.shape[:2]\n",
    "#         noise = np.random.randn(height, width).astype(np.float32) * 20\n",
    "#         return noise\n",
    "\n",
    "#     img = cv2.imread(img_path)\n",
    "#     if img is None:\n",
    "#         raise ValueError(f\"Image at {img_path} could not be loaded.\")\n",
    "\n",
    "#     grayscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     noiseprint = extract_noiseprint(grayscale)\n",
    "#     return img, noiseprint\n",
    "\n",
    "# def save_noiseprint(noiseprint, name, output_dir):\n",
    "#     \"\"\"Save noiseprint as image file\"\"\"\n",
    "#     border = 34\n",
    "#     if noiseprint.shape[0] > 2 * border and noiseprint.shape[1] > 2 * border:\n",
    "#         crop = noiseprint[border:-border, border:-border]\n",
    "#     else:\n",
    "#         crop = noiseprint\n",
    "\n",
    "#     vmin = np.min(crop)\n",
    "#     vmax = np.max(crop)\n",
    "#     norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "#     filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "#     plt.imsave(filename, norm_crop, cmap='gray')\n",
    "#     return filename\n",
    "\n",
    "# # ------------------ CNN Model Definition ------------------ #\n",
    "# class RealFakeCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RealFakeCNN, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(128 * 16 * 16, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ Prediction Function ------------------ #\n",
    "def predict_image(image_path, model, class_names, transform):\n",
    "    \"\"\"Predict single image without reloading model weights\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "# ------------------ Sequential Processing Function ------------------ #\n",
    "def process_single_image(img_path, model, class_names, transform, output_dir):\n",
    "    \"\"\"Process a single image: noiseprint extraction -> CNN prediction\"\"\"\n",
    "    image_name = img_path.split('\\\\')[-1].split('.')[0]\n",
    "    \n",
    "    # Time noiseprint extraction\n",
    "    noiseprint_start = timer()\n",
    "    try:\n",
    "        example, noiseprint = getNoiseprint(img_path)\n",
    "        noiseprint_saved_path = save_noiseprint(noiseprint, image_name, output_dir)\n",
    "        noiseprint_time = timer() - noiseprint_start\n",
    "    except Exception as e:\n",
    "        print(f\"Error during noiseprint extraction for {image_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Time CNN prediction\n",
    "    cnn_start = timer()\n",
    "    try:\n",
    "        label, score = predict_image(noiseprint_saved_path, model, class_names, transform)\n",
    "        cnn_time = timer() - cnn_start\n",
    "    except Exception as e:\n",
    "        print(f\"Error during CNN prediction for {image_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    total_time = noiseprint_time + cnn_time\n",
    "    \n",
    "    return {\n",
    "        'image_name': image_name,\n",
    "        'label': label,\n",
    "        'confidence': round(score, 4),\n",
    "        'noiseprint_time': round(noiseprint_time, 4),\n",
    "        'cnn_time': round(cnn_time, 4),\n",
    "        'total_time': round(total_time, 4)\n",
    "    }\n",
    "\n",
    "# ------------------ Main Sequential Processing Pipeline ------------------ #\n",
    "if __name__ == \"__main__\":\n",
    "    overall_start = timer()\n",
    "    \n",
    "    # Force CPU usage\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Setup directories\n",
    "    output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Setup transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Pad(padding=10, fill=0),\n",
    "        transforms.CenterCrop(128),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    # Load model once and keep it loaded\n",
    "    class_names = ['fake', 'real']\n",
    "    # model = RealFakeCNN().to(device)\n",
    "    \n",
    "    try:\n",
    "        # model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully on CPU\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Get all image paths\n",
    "    image_paths = glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*')\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"No images found in the specified directory\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images to process\\\\n\")\n",
    "\n",
    "    # Process images sequentially\n",
    "    results = []\n",
    "    total_noiseprint_time = 0\n",
    "    total_cnn_time = 0\n",
    "    \n",
    "    print(\"Processing images sequentially on CPU...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, img_path in enumerate(image_paths, 1):\n",
    "        print(f\"Processing image {i}/{len(image_paths)}: {os.path.basename(img_path)}\")\n",
    "        \n",
    "        result = process_single_image(img_path, model, class_names, transform, output_dir)\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "            total_noiseprint_time += result['noiseprint_time']\n",
    "            total_cnn_time += result['cnn_time']\n",
    "            \n",
    "            print(f\"  ‚Üí Noiseprint: {result['noiseprint_time']}s | CNN: {result['cnn_time']}s | Total: {result['total_time']}s\")\n",
    "            print(f\"  ‚Üí Prediction: {result['label']} (Confidence: {result['confidence']})\")\n",
    "        else:\n",
    "            print(f\"  ‚Üí Failed to process\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    # Clean up temporary files\n",
    "    try:\n",
    "        import shutil\n",
    "        if os.path.exists(output_dir):\n",
    "            shutil.rmtree(output_dir)\n",
    "            print(f\"\\\\nCleaned up temporary directory: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not clean up directory {output_dir}: {e}\")\n",
    "\n",
    "    # Summary\n",
    "    overall_end = timer()\n",
    "    overall_time = overall_end - overall_start\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"PROCESSING SUMMARY (CPU)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"Successfully processed: {len(results)}/{len(image_paths)} images\")\n",
    "        print(f\"Failed to process: {len(image_paths) - len(results)} images\")\n",
    "        print()\n",
    "        \n",
    "        # Time statistics\n",
    "        avg_noiseprint_time = total_noiseprint_time / len(results)\n",
    "        avg_cnn_time = total_cnn_time / len(results)\n",
    "        avg_total_time = (total_noiseprint_time + total_cnn_time) / len(results)\n",
    "        \n",
    "        print(\"TIMING STATISTICS:\")\n",
    "        print(f\"  Total noiseprint time: {total_noiseprint_time:.4f}s\")\n",
    "        print(f\"  Total CNN time: {total_cnn_time:.4f}s\")\n",
    "        print(f\"  Average noiseprint time per image: {avg_noiseprint_time:.4f}s\")\n",
    "        print(f\"  Average CNN time per image: {avg_cnn_time:.4f}s\")\n",
    "        print(f\"  Average total time per image: {avg_total_time:.4f}s\")\n",
    "        print(f\"  Overall execution time: {overall_time:.4f}s\")\n",
    "        print()\n",
    "        \n",
    "        # Prediction summary\n",
    "        fake_count = sum(1 for r in results if r['label'] == 'fake')\n",
    "        real_count = sum(1 for r in results if r['label'] == 'real')\n",
    "        \n",
    "        print(\"PREDICTION SUMMARY:\")\n",
    "        print(f\"  Images classified as FAKE: {fake_count}\")\n",
    "        print(f\"  Images classified as REAL: {real_count}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"DETAILED RESULTS:\")\n",
    "        for result in results:\n",
    "            print(f\"  {result['image_name']}: {result['label']} \"\n",
    "                  f\"(Conf: {result['confidence']}, \"\n",
    "                  f\"Noiseprint: {result['noiseprint_time']}s, \"\n",
    "                  f\"CNN: {result['cnn_time']}s, \"\n",
    "                  f\"Total: {result['total_time']}s)\")\n",
    "    else:\n",
    "        print(\"No images were successfully processed!\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\\\nTotal Inference Time: {overall_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\52_forged7.png'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    # os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db221781",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((572, 572)),   # Resize to fixed 572x572\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "def predict_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()).device)  # Send to same device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # shape: [1,1]\n",
    "        prob = output.item()   # probability from sigmoid\n",
    "\n",
    "    # Decide class\n",
    "    predicted_class = 1 if prob >= 0.5 else 0  # threshold at 0.5\n",
    "    return class_names[predicted_class], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\tempdir\\\\new_temp\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def noiseprint_to_heatmap(noiseprint, image_shape):\n",
    "    \"\"\"\n",
    "    noiseprint: np.ndarray (Hn, Wn) or (Hn, Wn, C)\n",
    "    image_shape: (H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    # If multi-channel, collapse\n",
    "    if noiseprint.ndim == 3:\n",
    "        noiseprint = np.mean(noiseprint, axis=2)\n",
    "\n",
    "    # Resize noiseprint to image size\n",
    "    noiseprint = cv2.resize(noiseprint, (image_shape[1], image_shape[0]))\n",
    "\n",
    "    # Normalize (z-score)\n",
    "    mean = np.mean(noiseprint)\n",
    "    std = np.std(noiseprint) + 1e-8\n",
    "    z_map = np.abs((noiseprint - mean) / std)\n",
    "\n",
    "    # Normalize to [0,1] for visualization\n",
    "    z_map = (z_map - z_map.min()) / (z_map.max() - z_map.min() + 1e-8)\n",
    "\n",
    "    return z_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_heatmap(z_map):\n",
    "    heatmap = (z_map * 255).astype(np.uint8)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_heatmap(image, heatmap, alpha=0.6):\n",
    "    \"\"\"\n",
    "    image: original BGR image (cv2)\n",
    "    heatmap: colored heatmap\n",
    "    \"\"\"\n",
    "    overlay = cv2.addWeighted(image, 1 - alpha, heatmap, alpha, 0)\n",
    "    return overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23517d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original image\n",
    "image = cv2.imread(\"1_forged1.png\")\n",
    "H, W = image.shape[:2]\n",
    "\n",
    "# Load noiseprint (example)\n",
    "noiseprint = np.load(\"noiseprint.npy\")  # your extracted noiseprint\n",
    "\n",
    "# Generate heatmap\n",
    "z_map = noiseprint_to_heatmap(noiseprint, (H, W))\n",
    "heatmap = colorize_heatmap(z_map)\n",
    "overlay = overlay_heatmap(image, heatmap)\n",
    "\n",
    "cv2.imwrite(\"noiseprint_heatmap.png\", heatmap)\n",
    "cv2.imwrite(\"noiseprint_overlay.png\", overlay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from Noiseprint import getNoiseprint\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "input_dir = r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\Forged_Aadhaar\\\\tempfolder\"\n",
    "output_dir = r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\Forged_Aadhaar\\\\tempnewfile\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===============================\n",
    "# HEATMAP UTILS (FIXED)\n",
    "# ===============================\n",
    "def noiseprint_to_heatmap(residual, target_shape):\n",
    "    # Resize noiseprint to image size\n",
    "    res_resized = cv2.resize(\n",
    "        residual,\n",
    "        (target_shape[1], target_shape[0]),\n",
    "        interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    # üî• LOCAL inconsistency extraction (CORE FIX)\n",
    "    smooth = cv2.GaussianBlur(res_resized, (31, 31), 0)\n",
    "    anomaly = np.abs(res_resized - smooth)\n",
    "\n",
    "    # üî• Robust normalization (percentile-based)\n",
    "    p1, p99 = np.percentile(anomaly, (1, 99))\n",
    "    anomaly = np.clip((anomaly - p1) / (p99 - p1 + 1e-8), 0, 1)\n",
    "\n",
    "    return anomaly\n",
    "\n",
    "\n",
    "def colorize_heatmap(z_map):\n",
    "    heatmap = (z_map * 255).astype(np.uint8)\n",
    "    return cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "\n",
    "def overlay_heatmap(image, heatmap, alpha=0.6):\n",
    "    return cv2.addWeighted(image, 1 - alpha, heatmap, alpha, 0)\n",
    "\n",
    "# ===============================\n",
    "# NOISEPRINT GENERATION + HEATMAP\n",
    "# ===============================\n",
    "noiseprints = []\n",
    "\n",
    "for img_path in glob.glob(os.path.join(input_dir, \"*\")):\n",
    "    name = os.path.basename(img_path).split(\".\")[0]\n",
    "    print(\"Processing:\", name)\n",
    "\n",
    "    example, noise_print = getNoiseprint(img_path)\n",
    "    noiseprints.append((name, img_path, noise_print))\n",
    "\n",
    "for name, img_path, res in noiseprints:\n",
    "    # Crop border (Noiseprint recommendation)\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        res = res[border:-border, border:-border]\n",
    "\n",
    "    # Load original image\n",
    "    orig_img = cv2.imread(img_path)\n",
    "    H, W = orig_img.shape[:2]\n",
    "\n",
    "    # Generate anomaly heatmap\n",
    "    anomaly_map = noiseprint_to_heatmap(res, (H, W))\n",
    "    heatmap = colorize_heatmap(anomaly_map)\n",
    "    overlay = overlay_heatmap(orig_img, heatmap)\n",
    "\n",
    "    # Save outputs\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"heatmap_{name}.png\"), heatmap)\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"overlay_{name}.png\"), overlay)\n",
    "\n",
    "    gray_np = (anomaly_map * 255).astype(np.uint8)\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"noiseprint_{name}.png\"), gray_np)\n",
    "\n",
    "# ===============================\n",
    "# CLASSIFICATION (UNCHANGED)\n",
    "# ===============================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((572, 572)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "def predict_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    image = image.to(next(model.parameters()).device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = output.item()\n",
    "\n",
    "    predicted_class = 1 if prob >= 0.5 else 0\n",
    "    return class_names[predicted_class], prob\n",
    "\n",
    "# Predict on noiseprint anomaly images\n",
    "class_names = ['fake', 'real']\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(output_dir):\n",
    "    if filename.startswith(\"noiseprint_\") and filename.lower().endswith(\".png\"):\n",
    "        img_path = os.path.join(output_dir, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69ccd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Noiseprint_Images\\noiseprint_Screenshot 2025-12-24 172919.png: 384x640 (no detections), 64.5ms\n",
      "Speed: 1.9ms preprocess, 64.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Trust\\notebooks\\runs\\detect\\predict4\u001b[0m\n",
      "Inference done. Saved to runs/detect/\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load model\n",
    "model = YOLO(\"C:\\\\Users\\\\dhruv\\\\Downloads\\\\yolo_runsnew_2\\\\detect\\\\train\\\\weights\\\\best.pt\")\n",
    "IMAGE_PATH = r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Noiseprint_Images\\noiseprint_Screenshot 2025-12-24 172919.png\"\n",
    "\n",
    "# Run inference\n",
    "results = model(IMAGE_PATH,conf =0.25, save=True)\n",
    "# results = model(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprints_folder\\\\noiseprint_14_forged4_original.png\", conf=0.25, save=True)\n",
    "# results = model(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprints_folder\\\\noiseprint_new_original.png\", conf=0.25, save=True)\n",
    "print(\"Inference done. Saved to runs/detect/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933db28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Dataset\\Spliced_Images\\1_forged10.png: 640x640 1 aadhaar number, 1 dob, 1 emblem, 1 gender, 1 goi symbol, 1 photo, 88.5ms\n",
      "Speed: 4.7ms preprocess, 88.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\runs\\detect\\predict13\u001b[0m\n",
      "Inference done. Saved to runs/detect/\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load modelx\n",
    "model = YOLO(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\best.pt\")\n",
    "\n",
    "# Run inference\n",
    "# results = model(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprints_folder\\\\noiseprint_14_forged4_original.png\", conf=0.25, save=True)\n",
    "results = model(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Spliced_Images\\\\1_forged10.png\", conf=0.25, save=True)\n",
    "print(\"Inference done. Saved to runs/detect/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04704fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TAMPERING RESULT ===\n",
      "{'tampered': True, 'tampered_areas': ['photo', 'aadhaar number']}\n",
      "\n",
      "Visualization saved as: tampering_localized.jpg\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "AADHAAR_MODEL_PATH = r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\best.pt\"\n",
    "SPLICE_MODEL_PATH  = r\"C:\\\\Users\\\\dhruv\\\\Downloads\\\\yolo_runsnew_2\\\\detect\\\\train\\\\weights\\\\best.pt\"\n",
    "\n",
    "IMAGE_PATH = r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Spliced_Images\\\\1_forged10.png\"\n",
    "SPLICED_IMAGE_PATH = r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Balanced_Data\\\\train\\\\fake\\\\noiseprint_1_forged10.png\"\n",
    "CONF_THRES = 0.25\n",
    "IOU_THRES  = 0.15\n",
    "\n",
    "OUTPUT_IMAGE = \"tampering_localized.jpg\"\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "# ---------------- IoU FUNCTION ----------------\n",
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "\n",
    "    if interArea == 0:\n",
    "        return 0.0\n",
    "\n",
    "    boxAArea = (boxA[2]-boxA[0]) * (boxA[3]-boxA[1])\n",
    "    boxBArea = (boxB[2]-boxB[0]) * (boxB[3]-boxB[1])\n",
    "\n",
    "    return interArea / float(boxAArea + boxBArea - interArea)\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------- LOAD MODELS ----------------\n",
    "aadhaar_model = YOLO(AADHAAR_MODEL_PATH)\n",
    "splice_model  = YOLO(SPLICE_MODEL_PATH)\n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------- RUN INFERENCE ----------------\n",
    "aadhaar_results = aadhaar_model(IMAGE_PATH, conf=CONF_THRES, verbose=False)\n",
    "splice_results  = splice_model(SPLICED_IMAGE_PATH, conf=CONF_THRES, verbose=False)\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------- EXTRACT BOXES ----------------\n",
    "aadhaar_boxes = []\n",
    "splice_boxes = []\n",
    "\n",
    "# Aadhaar fields\n",
    "for box in aadhaar_results[0].boxes:\n",
    "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "    cls_id = int(box.cls[0])\n",
    "    label = aadhaar_results[0].names[cls_id]\n",
    "    aadhaar_boxes.append({\n",
    "        \"label\": label,\n",
    "        \"bbox\": [x1, y1, x2, y2]\n",
    "    })\n",
    "\n",
    "# Spliced regions\n",
    "for box in splice_results[0].boxes:\n",
    "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "    splice_boxes.append({\n",
    "        \"bbox\": [x1, y1, x2, y2]\n",
    "    })\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------- MAP TAMPERING ----------------\n",
    "tampered_fields = set()\n",
    "\n",
    "for splice in splice_boxes:\n",
    "    for field in aadhaar_boxes:\n",
    "        overlap = iou(splice[\"bbox\"], field[\"bbox\"])\n",
    "        if overlap > IOU_THRES:\n",
    "            tampered_fields.add(field[\"label\"])\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------- PRINT RESULT ----------------\n",
    "result = {\n",
    "    \"tampered\": len(tampered_fields) > 0,\n",
    "    \"tampered_areas\": list(tampered_fields)\n",
    "}\n",
    "\n",
    "print(\"\\n=== TAMPERING RESULT ===\")\n",
    "print(result)\n",
    "# ----------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------- VISUALIZE ----------------\n",
    "image = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "for field in aadhaar_boxes:\n",
    "    x1, y1, x2, y2 = field[\"bbox\"]\n",
    "\n",
    "    if field[\"label\"] in tampered_fields:\n",
    "        color = (0, 0, 255)   # RED ‚Üí tampered\n",
    "    else:\n",
    "        color = (0, 255, 0)   # GREEN ‚Üí safe\n",
    "\n",
    "    cv2.rectangle(image, (x1,y1), (x2,y2), color, 2)\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        field[\"label\"],\n",
    "        (x1, y1-8),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.6,\n",
    "        color,\n",
    "        2\n",
    "    )\n",
    "\n",
    "cv2.imwrite(OUTPUT_IMAGE, image)\n",
    "print(f\"\\nVisualization saved as: {OUTPUT_IMAGE}\")\n",
    "# ----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3090fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 12:38:42 - INFO - ======================================================================\n",
      "2025-12-29 12:38:42 - INFO - AADHAAR TAMPERING DETECTION SYSTEM - ENHANCED\n",
      "2025-12-29 12:38:42 - INFO - ======================================================================\n",
      "2025-12-29 12:38:42 - INFO - Loading Aadhaar field detection model...\n",
      "2025-12-29 12:38:42 - INFO - Loading splice detection model...\n",
      "2025-12-29 12:38:42 - INFO - Models loaded successfully\n",
      "2025-12-29 12:38:42 - INFO - Running Aadhaar field detection...\n",
      "2025-12-29 12:38:42 - INFO - Running splice detection on noiseprint image...\n",
      "2025-12-29 12:38:42 - INFO - Aadhaar Model: Detected 6 regions\n",
      "2025-12-29 12:38:42 - INFO - Splice Model: Detected 2 regions\n",
      "2025-12-29 12:38:42 - INFO - Checking for critical fields...\n",
      "2025-12-29 12:38:42 - INFO - Creating splice detection visualization...\n",
      "2025-12-29 12:38:42 - INFO - Splice detection visualization saved: output\\splice_detection_visualization.jpg\n",
      "2025-12-29 12:38:42 - INFO - Creating Aadhaar fields visualization...\n",
      "2025-12-29 12:38:42 - INFO - Aadhaar fields visualization saved: output\\aadhaar_fields_detected.jpg\n",
      "2025-12-29 12:38:42 - INFO - Analyzing tampering patterns (containment-based)...\n",
      "2025-12-29 12:38:42 - INFO - Field 'photo' is 97.4% contained in splice_1 - marking as 100% tampered\n",
      "2025-12-29 12:38:42 - INFO - Field 'aadhaar number' is 100.0% contained in splice_2 - marking as 100% tampered\n",
      "2025-12-29 12:38:42 - INFO - JSON report saved: output\\detection_result.json\n",
      "2025-12-29 12:38:42 - INFO - Creating final tampering analysis visualization...\n",
      "2025-12-29 12:38:42 - INFO - Final result visualization saved: output\\final_tampering_result.jpg\n",
      "2025-12-29 12:38:42 - INFO - ======================================================================\n",
      "2025-12-29 12:38:42 - INFO - DETECTION SUMMARY\n",
      "2025-12-29 12:38:42 - INFO - ======================================================================\n",
      "2025-12-29 12:38:42 - INFO - Verdict: DOCUMENT TAMPERED/INVALID\n",
      "2025-12-29 12:38:42 - INFO - Severity: MEDIUM\n",
      "2025-12-29 12:38:42 - INFO - Status: ‚ö†Ô∏è  TAMPERED/INVALID\n",
      "2025-12-29 12:38:42 - INFO - \n",
      "Critical Fields Status:\n",
      "2025-12-29 12:38:42 - INFO -   ‚úì PHOTO: DETECTED\n",
      "2025-12-29 12:38:42 - INFO -   ‚úì AADHAAR NUMBER: DETECTED\n",
      "2025-12-29 12:38:42 - INFO - \n",
      "Detection Statistics:\n",
      "2025-12-29 12:38:42 - INFO -   ‚Ä¢ Total Aadhaar Fields: 6\n",
      "2025-12-29 12:38:42 - INFO -   ‚Ä¢ Splice Regions Found: 2\n",
      "2025-12-29 12:38:42 - INFO -   ‚Ä¢ Tampered Fields: 2\n",
      "2025-12-29 12:38:42 - INFO -   ‚Ä¢ Removed/Missing Fields: 0\n",
      "2025-12-29 12:38:42 - INFO - \n",
      "Splice Detection Details:\n",
      "2025-12-29 12:38:42 - INFO -   Splice #1: Confidence 0.849\n",
      "2025-12-29 12:38:43 - INFO -   Splice #2: Confidence 0.799\n",
      "2025-12-29 12:38:43 - INFO - \n",
      "‚ö†Ô∏è  Tampered Fields Identified:\n",
      "2025-12-29 12:38:43 - INFO -   ‚Ä¢ PHOTO\n",
      "2025-12-29 12:38:43 - INFO -     - Tampering: 100% (CONTAINED)\n",
      "2025-12-29 12:38:43 - INFO -     - Containment: 97.4%\n",
      "2025-12-29 12:38:43 - INFO -     - IoU: 88.0%\n",
      "2025-12-29 12:38:43 - INFO -     - Affected by 1 splice region(s)\n",
      "2025-12-29 12:38:43 - INFO -       ‚Üí splice_1: 100% tampering\n",
      "2025-12-29 12:38:43 - INFO -   ‚Ä¢ AADHAAR NUMBER\n",
      "2025-12-29 12:38:43 - INFO -     - Tampering: 100% (CONTAINED)\n",
      "2025-12-29 12:38:43 - INFO -     - Containment: 100.0%\n",
      "2025-12-29 12:38:43 - INFO -     - IoU: 74.7%\n",
      "2025-12-29 12:38:43 - INFO -     - Affected by 1 splice region(s)\n",
      "2025-12-29 12:38:43 - INFO -       ‚Üí splice_2: 100% tampering\n",
      "2025-12-29 12:38:43 - INFO - \n",
      "Processing Time: 0.780 seconds\n",
      "2025-12-29 12:38:43 - INFO - \n",
      "Generated Files:\n",
      "2025-12-29 12:38:43 - INFO -   1. splice_detection_visualization.jpg - Shows splice regions detected\n",
      "2025-12-29 12:38:43 - INFO -   2. aadhaar_fields_detected.jpg - Shows Aadhaar fields detected\n",
      "2025-12-29 12:38:43 - INFO -   3. final_tampering_result.jpg - Final tampering analysis\n",
      "2025-12-29 12:38:43 - INFO -   4. detection_result.json - Detailed JSON report\n",
      "2025-12-29 12:38:43 - INFO - ======================================================================\n",
      "2025-12-29 12:38:43 - ERROR - ‚ùå DOCUMENT TAMPERED: Forgery detected\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# Aadhaar Card Tampering Detection System\n",
    "# Uses dual YOLO models to detect and localize document forgery\n",
    "# \"\"\"\n",
    "\n",
    "# from ultralytics import YOLO\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime\n",
    "# from typing import List, Dict, Tuple\n",
    "# import logging\n",
    "\n",
    "# # ============================================================================\n",
    "# # CONFIGURATION\n",
    "# # ============================================================================\n",
    "\n",
    "# class Config:\n",
    "#     \"\"\"Central configuration for the tampering detection system\"\"\"\n",
    "    \n",
    "#     # Model paths\n",
    "#     AADHAAR_MODEL_PATH = Path(r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\best.pt\")\n",
    "#     SPLICE_MODEL_PATH = Path(r\"C:\\Users\\dhruv\\Downloads\\yolo_runsnew_2\\detect\\train\\weights\\best.pt\")\n",
    "    \n",
    "#     # Input paths\n",
    "#     IMAGE_PATH = Path(r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Dataset\\Spliced_Images\\1_forged10.png\")\n",
    "#     SPLICED_IMAGE_PATH = Path(r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Noiseprint_Images\\noiseprint_1_forged10.png\")\n",
    "    \n",
    "#     # Output configuration\n",
    "#     OUTPUT_DIR = Path(\"./output\")\n",
    "#     OUTPUT_IMAGE_NAME = \"final_tampering_result.jpg\"\n",
    "#     SPLICE_DETECTION_NAME = \"splice_detection_visualization.jpg\"\n",
    "#     NOISEPRINT_SPLICE_NAME = \"noiseprint_splice_regions.jpg\"\n",
    "#     AADHAAR_FIELDS_NAME = \"aadhaar_fields_detected.jpg\"\n",
    "#     OUTPUT_JSON_NAME = \"detection_result.json\"\n",
    "    \n",
    "#     # Detection thresholds\n",
    "#     CONF_THRESHOLD = 0.25\n",
    "#     IOU_THRESHOLD = 0.15\n",
    "    \n",
    "#     # Visualization settings\n",
    "#     COLOR_TAMPERED = (0, 0, 255)      # Red for tampered regions\n",
    "#     COLOR_SAFE = (0, 255, 0)          # Green for safe regions\n",
    "#     COLOR_SPLICE = (255, 0, 255)      # Magenta for splice detection\n",
    "#     COLOR_SPLICE_BOX = (255, 165, 0)  # Orange for splice boxes\n",
    "#     BOX_THICKNESS = 3\n",
    "#     SPLICE_BOX_THICKNESS = 4\n",
    "#     TEXT_FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#     TEXT_SCALE = 0.7\n",
    "#     TEXT_THICKNESS = 2\n",
    "\n",
    "# # ============================================================================\n",
    "# # LOGGING SETUP\n",
    "# # ============================================================================\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "#     datefmt='%Y-%m-%d %H:%M:%S'\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # ============================================================================\n",
    "# # CORE FUNCTIONS\n",
    "# # ============================================================================\n",
    "\n",
    "# def calculate_iou(box_a: List[int], box_b: List[int]) -> float:\n",
    "#     \"\"\"\n",
    "#     Calculate Intersection over Union (IoU) between two bounding boxes.\n",
    "    \n",
    "#     Args:\n",
    "#         box_a: [x1, y1, x2, y2] coordinates of first box\n",
    "#         box_b: [x1, y1, x2, y2] coordinates of second box\n",
    "    \n",
    "#     Returns:\n",
    "#         IoU score between 0 and 1\n",
    "#     \"\"\"\n",
    "#     x_left = max(box_a[0], box_b[0])\n",
    "#     y_top = max(box_a[1], box_b[1])\n",
    "#     x_right = min(box_a[2], box_b[2])\n",
    "#     y_bottom = min(box_a[3], box_b[3])\n",
    "    \n",
    "#     intersection_width = max(0, x_right - x_left)\n",
    "#     intersection_height = max(0, y_bottom - y_top)\n",
    "#     intersection_area = intersection_width * intersection_height\n",
    "    \n",
    "#     if intersection_area == 0:\n",
    "#         return 0.0\n",
    "    \n",
    "#     box_a_area = (box_a[2] - box_a[0]) * (box_a[3] - box_a[1])\n",
    "#     box_b_area = (box_b[2] - box_b[0]) * (box_b[3] - box_b[1])\n",
    "#     union_area = box_a_area + box_b_area - intersection_area\n",
    "    \n",
    "#     return intersection_area / float(union_area)\n",
    "\n",
    "\n",
    "# def load_models(aadhaar_path: Path, splice_path: Path) -> Tuple[YOLO, YOLO]:\n",
    "#     \"\"\"\n",
    "#     Load both YOLO models with error handling.\n",
    "    \n",
    "#     Args:\n",
    "#         aadhaar_path: Path to Aadhaar field detection model\n",
    "#         splice_path: Path to splice detection model\n",
    "    \n",
    "#     Returns:\n",
    "#         Tuple of (aadhaar_model, splice_model)\n",
    "    \n",
    "#     Raises:\n",
    "#         FileNotFoundError: If model files don't exist\n",
    "#     \"\"\"\n",
    "#     if not aadhaar_path.exists():\n",
    "#         raise FileNotFoundError(f\"Aadhaar model not found: {aadhaar_path}\")\n",
    "#     if not splice_path.exists():\n",
    "#         raise FileNotFoundError(f\"Splice model not found: {splice_path}\")\n",
    "    \n",
    "#     logger.info(\"Loading Aadhaar field detection model...\")\n",
    "#     aadhaar_model = YOLO(str(aadhaar_path))\n",
    "    \n",
    "#     logger.info(\"Loading splice detection model...\")\n",
    "#     splice_model = YOLO(str(splice_path))\n",
    "    \n",
    "#     logger.info(\"Models loaded successfully\")\n",
    "#     return aadhaar_model, splice_model\n",
    "\n",
    "\n",
    "# def extract_detections(results, model_name: str) -> List[Dict]:\n",
    "#     \"\"\"\n",
    "#     Extract bounding boxes and labels from YOLO results.\n",
    "    \n",
    "#     Args:\n",
    "#         results: YOLO inference results\n",
    "#         model_name: Name of the model for logging\n",
    "    \n",
    "#     Returns:\n",
    "#         List of detection dictionaries with bbox, label, and confidence\n",
    "#     \"\"\"\n",
    "#     detections = []\n",
    "    \n",
    "#     if len(results[0].boxes) == 0:\n",
    "#         logger.warning(f\"No detections from {model_name}\")\n",
    "#         return detections\n",
    "    \n",
    "#     for box in results[0].boxes:\n",
    "#         x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "#         confidence = float(box.conf[0])\n",
    "        \n",
    "#         detection = {\n",
    "#             \"bbox\": [x1, y1, x2, y2],\n",
    "#             \"confidence\": confidence\n",
    "#         }\n",
    "        \n",
    "#         # Add label if available (for Aadhaar model)\n",
    "#         if hasattr(box, 'cls'):\n",
    "#             cls_id = int(box.cls[0])\n",
    "#             label = results[0].names[cls_id]\n",
    "#             detection[\"label\"] = label\n",
    "        \n",
    "#         detections.append(detection)\n",
    "    \n",
    "#     logger.info(f\"{model_name}: Detected {len(detections)} regions\")\n",
    "#     return detections\n",
    "\n",
    "\n",
    "# def map_tampering(aadhaar_boxes: List[Dict], \n",
    "#                   splice_boxes: List[Dict], \n",
    "#                   iou_threshold: float) -> Tuple[Dict, List[Dict]]:\n",
    "#     \"\"\"\n",
    "#     Map splice detections to Aadhaar fields using IoU overlap.\n",
    "#     Also identifies unmapped splice regions (tampering without detected field).\n",
    "    \n",
    "#     Args:\n",
    "#         aadhaar_boxes: List of detected Aadhaar field boxes\n",
    "#         splice_boxes: List of detected splice regions\n",
    "#         iou_threshold: Minimum IoU to consider overlap\n",
    "    \n",
    "#     Returns:\n",
    "#         Tuple of (tampered_fields dict, unmapped_splices list)\n",
    "#         - tampered_fields: Fields that overlap with splice regions\n",
    "#         - unmapped_splices: Splice regions that don't overlap with any detected field\n",
    "#     \"\"\"\n",
    "#     tampered_fields = {}\n",
    "#     mapped_splice_ids = set()\n",
    "    \n",
    "#     for splice_idx, splice in enumerate(splice_boxes):\n",
    "#         splice_id = f\"splice_{splice_idx + 1}\"\n",
    "#         splice_mapped = False\n",
    "        \n",
    "#         for field in aadhaar_boxes:\n",
    "#             overlap = calculate_iou(splice[\"bbox\"], field[\"bbox\"])\n",
    "            \n",
    "#             if overlap > iou_threshold:\n",
    "#                 splice_mapped = True\n",
    "#                 mapped_splice_ids.add(splice_idx)\n",
    "#                 field_label = field.get(\"label\", \"unknown\")\n",
    "                \n",
    "#                 if field_label not in tampered_fields:\n",
    "#                     tampered_fields[field_label] = {\n",
    "#                         \"max_overlap\": overlap,\n",
    "#                         \"confidence\": field[\"confidence\"],\n",
    "#                         \"bbox\": field[\"bbox\"],\n",
    "#                         \"splice_regions\": []\n",
    "#                     }\n",
    "                \n",
    "#                 # Track which splice regions affect this field\n",
    "#                 tampered_fields[field_label][\"splice_regions\"].append({\n",
    "#                     \"splice_id\": splice_id,\n",
    "#                     \"overlap\": overlap,\n",
    "#                     \"bbox\": splice[\"bbox\"]\n",
    "#                 })\n",
    "                \n",
    "#                 # Update max overlap if this is higher\n",
    "#                 if overlap > tampered_fields[field_label][\"max_overlap\"]:\n",
    "#                     tampered_fields[field_label][\"max_overlap\"] = overlap\n",
    "    \n",
    "#     # Identify unmapped splice regions (tampering detected but no field found)\n",
    "#     unmapped_splices = []\n",
    "#     for splice_idx, splice in enumerate(splice_boxes):\n",
    "#         if splice_idx not in mapped_splice_ids:\n",
    "#             unmapped_splices.append({\n",
    "#                 \"splice_id\": f\"splice_{splice_idx + 1}\",\n",
    "#                 \"bbox\": splice[\"bbox\"],\n",
    "#                 \"confidence\": splice[\"confidence\"],\n",
    "#                 \"reason\": \"MISSING_OR_REMOVED_FIELD\"\n",
    "#             })\n",
    "    \n",
    "#     return tampered_fields, unmapped_splices\n",
    "\n",
    "\n",
    "# def visualize_splice_detection(noiseprint_image_path: Path,\n",
    "#                                splice_boxes: List[Dict],\n",
    "#                                output_path: Path) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Visualize ONLY the splice detection on the noiseprint image.\n",
    "#     This shows what the splice detection model actually found.\n",
    "    \n",
    "#     Args:\n",
    "#         noiseprint_image_path: Path to noiseprint image\n",
    "#         splice_boxes: Detected splice regions\n",
    "#         output_path: Where to save the visualization\n",
    "    \n",
    "#     Returns:\n",
    "#         Annotated noiseprint image\n",
    "#     \"\"\"\n",
    "#     image = cv2.imread(str(noiseprint_image_path))\n",
    "    \n",
    "#     if image is None:\n",
    "#         raise ValueError(f\"Could not load noiseprint image: {noiseprint_image_path}\")\n",
    "    \n",
    "#     # Create overlay for semi-transparent regions\n",
    "#     overlay = image.copy()\n",
    "    \n",
    "#     # Draw each splice region\n",
    "#     for idx, splice in enumerate(splice_boxes):\n",
    "#         x1, y1, x2, y2 = splice[\"bbox\"]\n",
    "#         confidence = splice.get(\"confidence\", 0)\n",
    "        \n",
    "#         # Fill region with semi-transparent color\n",
    "#         cv2.rectangle(overlay, (x1, y1), (x2, y2), Config.COLOR_SPLICE, -1)\n",
    "        \n",
    "#         # Draw border\n",
    "#         cv2.rectangle(image, (x1, y1), (x2, y2), Config.COLOR_SPLICE_BOX, Config.SPLICE_BOX_THICKNESS)\n",
    "        \n",
    "#         # Add label\n",
    "#         label = f\"Splice #{idx + 1} [{confidence:.2f}]\"\n",
    "#         (text_width, text_height), baseline = cv2.getTextSize(\n",
    "#             label, Config.TEXT_FONT, Config.TEXT_SCALE, Config.TEXT_THICKNESS\n",
    "#         )\n",
    "        \n",
    "#         # Text background\n",
    "#         cv2.rectangle(\n",
    "#             image,\n",
    "#             (x1, y1 - text_height - baseline - 5),\n",
    "#             (x1 + text_width, y1),\n",
    "#             Config.COLOR_SPLICE_BOX,\n",
    "#             -1\n",
    "#         )\n",
    "        \n",
    "#         # Text\n",
    "#         cv2.putText(\n",
    "#             image,\n",
    "#             label,\n",
    "#             (x1, y1 - baseline - 5),\n",
    "#             Config.TEXT_FONT,\n",
    "#             Config.TEXT_SCALE,\n",
    "#             (255, 255, 255),\n",
    "#             Config.TEXT_THICKNESS\n",
    "#         )\n",
    "    \n",
    "#     # Blend overlay\n",
    "#     cv2.addWeighted(overlay, 0.3, image, 0.7, 0, image)\n",
    "    \n",
    "#     # Add header\n",
    "#     header_text = f\"SPLICE DETECTION: {len(splice_boxes)} Region(s) Found\"\n",
    "#     cv2.putText(\n",
    "#         image,\n",
    "#         header_text,\n",
    "#         (10, 35),\n",
    "#         Config.TEXT_FONT,\n",
    "#         0.9,\n",
    "#         (255, 255, 255),\n",
    "#         3\n",
    "#     )\n",
    "#     cv2.putText(\n",
    "#         image,\n",
    "#         header_text,\n",
    "#         (10, 35),\n",
    "#         Config.TEXT_FONT,\n",
    "#         0.9,\n",
    "#         Config.COLOR_SPLICE_BOX,\n",
    "#         2\n",
    "#     )\n",
    "    \n",
    "#     # Save\n",
    "#     cv2.imwrite(str(output_path), image)\n",
    "#     logger.info(f\"Splice detection visualization saved: {output_path}\")\n",
    "    \n",
    "#     return image\n",
    "\n",
    "\n",
    "# def visualize_aadhaar_fields(image_path: Path,\n",
    "#                              aadhaar_boxes: List[Dict],\n",
    "#                              output_path: Path) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Visualize ONLY the Aadhaar field detection.\n",
    "#     This shows what fields the Aadhaar model detected.\n",
    "    \n",
    "#     Args:\n",
    "#         image_path: Path to original Aadhaar image\n",
    "#         aadhaar_boxes: Detected Aadhaar fields\n",
    "#         output_path: Where to save the visualization\n",
    "    \n",
    "#     Returns:\n",
    "#         Annotated image\n",
    "#     \"\"\"\n",
    "#     image = cv2.imread(str(image_path))\n",
    "    \n",
    "#     if image is None:\n",
    "#         raise ValueError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "#     for field in aadhaar_boxes:\n",
    "#         x1, y1, x2, y2 = field[\"bbox\"]\n",
    "#         label = field.get(\"label\", \"field\")\n",
    "#         confidence = field.get(\"confidence\", 0)\n",
    "        \n",
    "#         # Draw box\n",
    "#         cv2.rectangle(image, (x1, y1), (x2, y2), Config.COLOR_SAFE, Config.BOX_THICKNESS)\n",
    "        \n",
    "#         # Label\n",
    "#         text = f\"{label} [{confidence:.2f}]\"\n",
    "#         (text_width, text_height), baseline = cv2.getTextSize(\n",
    "#             text, Config.TEXT_FONT, Config.TEXT_SCALE, Config.TEXT_THICKNESS\n",
    "#         )\n",
    "        \n",
    "#         # Text background\n",
    "#         cv2.rectangle(\n",
    "#             image,\n",
    "#             (x1, y1 - text_height - baseline - 5),\n",
    "#             (x1 + text_width, y1),\n",
    "#             Config.COLOR_SAFE,\n",
    "#             -1\n",
    "#         )\n",
    "        \n",
    "#         # Text\n",
    "#         cv2.putText(\n",
    "#             image,\n",
    "#             text,\n",
    "#             (x1, y1 - baseline - 5),\n",
    "#             Config.TEXT_FONT,\n",
    "#             Config.TEXT_SCALE,\n",
    "#             (255, 255, 255),\n",
    "#             Config.TEXT_THICKNESS\n",
    "#         )\n",
    "    \n",
    "#     # Add header\n",
    "#     header = f\"AADHAAR FIELDS DETECTED: {len(aadhaar_boxes)}\"\n",
    "#     cv2.putText(image, header, (10, 35), Config.TEXT_FONT, 0.9, (255, 255, 255), 3)\n",
    "#     cv2.putText(image, header, (10, 35), Config.TEXT_FONT, 0.9, Config.COLOR_SAFE, 2)\n",
    "    \n",
    "#     cv2.imwrite(str(output_path), image)\n",
    "#     logger.info(f\"Aadhaar fields visualization saved: {output_path}\")\n",
    "    \n",
    "#     return image\n",
    "\n",
    "\n",
    "# def visualize_final_result(image_path: Path, \n",
    "#                           aadhaar_boxes: List[Dict],\n",
    "#                           splice_boxes: List[Dict],\n",
    "#                           tampered_fields: Dict,\n",
    "#                           unmapped_splices: List[Dict],\n",
    "#                           output_path: Path) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Create final combined visualization showing tampering analysis.\n",
    "#     This overlays splice regions on the original image and marks which fields are tampered.\n",
    "#     Also highlights unmapped splice regions (removed/missing fields).\n",
    "    \n",
    "#     Args:\n",
    "#         image_path: Path to original image\n",
    "#         aadhaar_boxes: Detected Aadhaar fields\n",
    "#         splice_boxes: Detected splice regions\n",
    "#         tampered_fields: Mapping of tampered fields\n",
    "#         unmapped_splices: Splice regions without corresponding fields\n",
    "#         output_path: Where to save the annotated image\n",
    "    \n",
    "#     Returns:\n",
    "#         Annotated image array\n",
    "#     \"\"\"\n",
    "#     image = cv2.imread(str(image_path))\n",
    "    \n",
    "#     if image is None:\n",
    "#         raise ValueError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "#     # Create overlay for splice regions\n",
    "#     overlay = image.copy()\n",
    "    \n",
    "#     # Draw splice detection regions (semi-transparent)\n",
    "#     for idx, splice in enumerate(splice_boxes):\n",
    "#         x1, y1, x2, y2 = splice[\"bbox\"]\n",
    "#         cv2.rectangle(overlay, (x1, y1), (x2, y2), Config.COLOR_SPLICE, -1)\n",
    "    \n",
    "#     # Blend overlay\n",
    "#     cv2.addWeighted(overlay, 0.25, image, 0.75, 0, image)\n",
    "    \n",
    "#     # Draw Aadhaar field boxes with tampering status\n",
    "#     for field in aadhaar_boxes:\n",
    "#         x1, y1, x2, y2 = field[\"bbox\"]\n",
    "#         label = field.get(\"label\", \"field\")\n",
    "#         confidence = field.get(\"confidence\", 0)\n",
    "        \n",
    "#         # Determine status and color\n",
    "#         if label in tampered_fields:\n",
    "#             color = Config.COLOR_TAMPERED\n",
    "#             status = \"‚ö† TAMPERED\"\n",
    "#             overlap = tampered_fields[label][\"max_overlap\"]\n",
    "#             text = f\"{label} ({status}) [{confidence:.2f}] IoU:{overlap:.2f}\"\n",
    "#         else:\n",
    "#             color = Config.COLOR_SAFE\n",
    "#             status = \"‚úì SAFE\"\n",
    "#             text = f\"{label} ({status}) [{confidence:.2f}]\"\n",
    "        \n",
    "#         # Draw thick bounding box\n",
    "#         cv2.rectangle(image, (x1, y1), (x2, y2), color, Config.BOX_THICKNESS)\n",
    "        \n",
    "#         # Calculate text size\n",
    "#         (text_width, text_height), baseline = cv2.getTextSize(\n",
    "#             text, Config.TEXT_FONT, Config.TEXT_SCALE, Config.TEXT_THICKNESS\n",
    "#         )\n",
    "        \n",
    "#         # Draw text background\n",
    "#         cv2.rectangle(\n",
    "#             image,\n",
    "#             (x1, y1 - text_height - baseline - 8),\n",
    "#             (x1 + text_width + 4, y1),\n",
    "#             color,\n",
    "#             -1\n",
    "#         )\n",
    "        \n",
    "#         # Draw text\n",
    "#         cv2.putText(\n",
    "#             image,\n",
    "#             text,\n",
    "#             (x1 + 2, y1 - baseline - 5),\n",
    "#             Config.TEXT_FONT,\n",
    "#             Config.TEXT_SCALE,\n",
    "#             (255, 255, 255),\n",
    "#             Config.TEXT_THICKNESS\n",
    "#         )\n",
    "    \n",
    "#     # Draw unmapped splice regions with special marker\n",
    "#     for unmapped in unmapped_splices:\n",
    "#         x1, y1, x2, y2 = unmapped[\"bbox\"]\n",
    "#         confidence = unmapped[\"confidence\"]\n",
    "        \n",
    "#         # Draw dashed border for unmapped regions\n",
    "#         color = (255, 0, 0)  # Bright red\n",
    "#         thickness = 4\n",
    "#         dash_length = 10\n",
    "        \n",
    "#         # Top border\n",
    "#         for x in range(x1, x2, dash_length * 2):\n",
    "#             cv2.line(image, (x, y1), (min(x + dash_length, x2), y1), color, thickness)\n",
    "#         # Bottom border\n",
    "#         for x in range(x1, x2, dash_length * 2):\n",
    "#             cv2.line(image, (x, y2), (min(x + dash_length, x2), y2), color, thickness)\n",
    "#         # Left border\n",
    "#         for y in range(y1, y2, dash_length * 2):\n",
    "#             cv2.line(image, (x1, y), (x1, min(y + dash_length, y2)), color, thickness)\n",
    "#         # Right border\n",
    "#         for y in range(y1, y2, dash_length * 2):\n",
    "#             cv2.line(image, (x2, y), (x2, min(y + dash_length, y2)), color, thickness)\n",
    "        \n",
    "#         # Label for unmapped region\n",
    "#         text = f\"‚ö† REMOVED/MISSING FIELD [{confidence:.2f}]\"\n",
    "#         (text_width, text_height), baseline = cv2.getTextSize(\n",
    "#             text, Config.TEXT_FONT, 0.65, Config.TEXT_THICKNESS\n",
    "#         )\n",
    "        \n",
    "#         # Text background (bright red)\n",
    "#         cv2.rectangle(\n",
    "#             image,\n",
    "#             (x1, y1 - text_height - baseline - 8),\n",
    "#             (x1 + text_width + 4, y1),\n",
    "#             color,\n",
    "#             -1\n",
    "#         )\n",
    "        \n",
    "#         # Text\n",
    "#         cv2.putText(\n",
    "#             image,\n",
    "#             text,\n",
    "#             (x1 + 2, y1 - baseline - 5),\n",
    "#             Config.TEXT_FONT,\n",
    "#             0.65,\n",
    "#             (255, 255, 255),\n",
    "#             Config.TEXT_THICKNESS\n",
    "#         )\n",
    "        \n",
    "#         # Add warning icon (X mark)\n",
    "#         center_x = (x1 + x2) // 2\n",
    "#         center_y = (y1 + y2) // 2\n",
    "#         size = 20\n",
    "#         cv2.line(image, (center_x - size, center_y - size), \n",
    "#                 (center_x + size, center_y + size), color, 5)\n",
    "#         cv2.line(image, (center_x + size, center_y - size), \n",
    "#                 (center_x - size, center_y + size), color, 5)\n",
    "    \n",
    "#     # Add comprehensive summary at top\n",
    "#     unmapped_count = len(unmapped_splices)\n",
    "#     summary_lines = [\n",
    "#         f\"TAMPERING ANALYSIS RESULT\",\n",
    "#         f\"Fields: {len(aadhaar_boxes)} | Splice Regions: {len(splice_boxes)} | Tampered: {len(tampered_fields)} | Removed: {unmapped_count}\",\n",
    "#         f\"Status: {'DOCUMENT TAMPERED' if (tampered_fields or unmapped_splices) else 'DOCUMENT AUTHENTIC'}\"\n",
    "#     ]\n",
    "    \n",
    "#     y_offset = 30\n",
    "#     for line in summary_lines:\n",
    "#         # White outline for readability\n",
    "#         cv2.putText(image, line, (10, y_offset), Config.TEXT_FONT, 0.8, (255, 255, 255), 4)\n",
    "#         # Colored text\n",
    "#         text_color = Config.COLOR_TAMPERED if (tampered_fields or unmapped_splices) else Config.COLOR_SAFE\n",
    "#         cv2.putText(image, line, (10, y_offset), Config.TEXT_FONT, 0.8, text_color, 2)\n",
    "#         y_offset += 30\n",
    "    \n",
    "#     # Save\n",
    "#     cv2.imwrite(str(output_path), image)\n",
    "#     logger.info(f\"Final result visualization saved: {output_path}\")\n",
    "    \n",
    "#     return image\n",
    "\n",
    "\n",
    "# def generate_report(aadhaar_boxes: List[Dict],\n",
    "#                    splice_boxes: List[Dict],\n",
    "#                    tampered_fields: Dict,\n",
    "#                    unmapped_splices: List[Dict],\n",
    "#                    processing_time: float) -> Dict:\n",
    "#     \"\"\"\n",
    "#     Generate comprehensive JSON report of detection results.\n",
    "    \n",
    "#     Args:\n",
    "#         aadhaar_boxes: Detected Aadhaar fields\n",
    "#         splice_boxes: Detected splice regions\n",
    "#         tampered_fields: Mapping of tampered fields\n",
    "#         unmapped_splices: Splice regions without detected fields\n",
    "#         processing_time: Time taken for processing\n",
    "    \n",
    "#     Returns:\n",
    "#         Dictionary containing full analysis report\n",
    "#     \"\"\"\n",
    "#     is_tampered = len(tampered_fields) > 0 or len(unmapped_splices) > 0\n",
    "    \n",
    "#     report = {\n",
    "#         \"timestamp\": datetime.now().isoformat(),\n",
    "#         \"processing_time_seconds\": round(processing_time, 3),\n",
    "#         \"summary\": {\n",
    "#             \"is_tampered\": is_tampered,\n",
    "#             \"verdict\": \"DOCUMENT TAMPERED\" if is_tampered else \"DOCUMENT AUTHENTIC\",\n",
    "#             \"total_fields_detected\": len(aadhaar_boxes),\n",
    "#             \"total_splice_regions\": len(splice_boxes),\n",
    "#             \"tampered_field_count\": len(tampered_fields),\n",
    "#             \"removed_or_missing_fields\": len(unmapped_splices),\n",
    "#             \"tampering_severity\": \"HIGH\" if unmapped_splices else (\"MEDIUM\" if tampered_fields else \"NONE\")\n",
    "#         },\n",
    "#         \"splice_detections\": [\n",
    "#             {\n",
    "#                 \"splice_id\": f\"splice_{idx + 1}\",\n",
    "#                 \"confidence\": round(splice[\"confidence\"], 3),\n",
    "#                 \"bounding_box\": splice[\"bbox\"],\n",
    "#                 \"status\": \"mapped\" if idx in [int(tf[\"splice_regions\"][0][\"splice_id\"].split(\"_\")[1]) - 1 \n",
    "#                                               for tf in tampered_fields.values() \n",
    "#                                               for _ in tf[\"splice_regions\"]] else \"unmapped\"\n",
    "#             }\n",
    "#             for idx, splice in enumerate(splice_boxes)\n",
    "#         ],\n",
    "#         \"tampered_fields\": [\n",
    "#             {\n",
    "#                 \"field_name\": field_name,\n",
    "#                 \"max_overlap_score\": round(data[\"max_overlap\"], 3),\n",
    "#                 \"detection_confidence\": round(data[\"confidence\"], 3),\n",
    "#                 \"bounding_box\": data[\"bbox\"],\n",
    "#                 \"affecting_splice_regions\": [\n",
    "#                     {\n",
    "#                         \"splice_id\": sr[\"splice_id\"],\n",
    "#                         \"overlap\": round(sr[\"overlap\"], 3)\n",
    "#                     }\n",
    "#                     for sr in data[\"splice_regions\"]\n",
    "#                 ]\n",
    "#             }\n",
    "#             for field_name, data in tampered_fields.items()\n",
    "#         ],\n",
    "#         \"unmapped_splice_regions\": [\n",
    "#             {\n",
    "#                 \"splice_id\": unmapped[\"splice_id\"],\n",
    "#                 \"confidence\": round(unmapped[\"confidence\"], 3),\n",
    "#                 \"bounding_box\": unmapped[\"bbox\"],\n",
    "#                 \"reason\": unmapped[\"reason\"],\n",
    "#                 \"explanation\": \"Tampering detected but no corresponding Aadhaar field found. Field may be removed, missing, or severely damaged.\"\n",
    "#             }\n",
    "#             for unmapped in unmapped_splices\n",
    "#         ],\n",
    "#         \"all_detected_fields\": [\n",
    "#             {\n",
    "#                 \"label\": box.get(\"label\", \"unknown\"),\n",
    "#                 \"confidence\": round(box[\"confidence\"], 3),\n",
    "#                 \"bounding_box\": box[\"bbox\"],\n",
    "#                 \"status\": \"TAMPERED\" if box.get(\"label\") in tampered_fields else \"SAFE\"\n",
    "#             }\n",
    "#             for box in aadhaar_boxes\n",
    "#         ],\n",
    "#         \"detection_parameters\": {\n",
    "#             \"confidence_threshold\": Config.CONF_THRESHOLD,\n",
    "#             \"iou_threshold\": Config.IOU_THRESHOLD\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     return report\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # MAIN EXECUTION\n",
    "# # ============================================================================\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main execution pipeline for tampering detection\"\"\"\n",
    "    \n",
    "#     try:\n",
    "#         start_time = datetime.now()\n",
    "        \n",
    "#         # Create output directory\n",
    "#         Config.OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "        \n",
    "#         logger.info(\"=\"*70)\n",
    "#         logger.info(\"AADHAAR TAMPERING DETECTION SYSTEM\")\n",
    "#         logger.info(\"=\"*70)\n",
    "        \n",
    "#         # Validate input files\n",
    "#         if not Config.IMAGE_PATH.exists():\n",
    "#             raise FileNotFoundError(f\"Image not found: {Config.IMAGE_PATH}\")\n",
    "#         if not Config.SPLICED_IMAGE_PATH.exists():\n",
    "#             raise FileNotFoundError(f\"Spliced image not found: {Config.SPLICED_IMAGE_PATH}\")\n",
    "        \n",
    "#         # Load models\n",
    "#         aadhaar_model, splice_model = load_models(\n",
    "#             Config.AADHAAR_MODEL_PATH,\n",
    "#             Config.SPLICE_MODEL_PATH\n",
    "#         )\n",
    "        \n",
    "#         # Run inference\n",
    "#         logger.info(\"Running Aadhaar field detection...\")\n",
    "#         aadhaar_results = aadhaar_model(\n",
    "#             str(Config.IMAGE_PATH),\n",
    "#             conf=Config.CONF_THRESHOLD,\n",
    "#             verbose=False\n",
    "#         )\n",
    "        \n",
    "#         logger.info(\"Running splice detection on noiseprint image...\")\n",
    "#         splice_results = splice_model(\n",
    "#             str(Config.SPLICED_IMAGE_PATH),\n",
    "#             conf=Config.CONF_THRESHOLD,\n",
    "#             verbose=False\n",
    "#         )\n",
    "        \n",
    "#         # Extract detections\n",
    "#         aadhaar_boxes = extract_detections(aadhaar_results, \"Aadhaar Model\")\n",
    "#         splice_boxes = extract_detections(splice_results, \"Splice Model\")\n",
    "        \n",
    "#         # CRITICAL: Visualize splice detection separately FIRST\n",
    "#         logger.info(\"Creating splice detection visualization...\")\n",
    "#         splice_viz_path = Config.OUTPUT_DIR / Config.SPLICE_DETECTION_NAME\n",
    "#         visualize_splice_detection(\n",
    "#             Config.SPLICED_IMAGE_PATH,\n",
    "#             splice_boxes,\n",
    "#             splice_viz_path\n",
    "#         )\n",
    "        \n",
    "#         # Visualize Aadhaar fields separately\n",
    "#         logger.info(\"Creating Aadhaar fields visualization...\")\n",
    "#         fields_viz_path = Config.OUTPUT_DIR / Config.AADHAAR_FIELDS_NAME\n",
    "#         visualize_aadhaar_fields(\n",
    "#             Config.IMAGE_PATH,\n",
    "#             aadhaar_boxes,\n",
    "#             fields_viz_path\n",
    "#         )\n",
    "        \n",
    "#         # Map tampering\n",
    "#         logger.info(\"Analyzing tampering patterns...\")\n",
    "#         tampered_fields, unmapped_splices = map_tampering(\n",
    "#             aadhaar_boxes,\n",
    "#             splice_boxes,\n",
    "#             Config.IOU_THRESHOLD\n",
    "#         )\n",
    "        \n",
    "#         # Log unmapped splices as critical warning\n",
    "#         if unmapped_splices:\n",
    "#             logger.warning(f\"‚ö†Ô∏è  Found {len(unmapped_splices)} unmapped splice region(s)!\")\n",
    "#             logger.warning(\"This indicates field removal or severe tampering.\")\n",
    "        \n",
    "#         # Calculate processing time\n",
    "#         processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "#         # Generate report\n",
    "#         report = generate_report(\n",
    "#             aadhaar_boxes,\n",
    "#             splice_boxes,\n",
    "#             tampered_fields,\n",
    "#             unmapped_splices,\n",
    "#             processing_time\n",
    "#         )\n",
    "        \n",
    "#         # Save JSON report\n",
    "#         json_path = Config.OUTPUT_DIR / Config.OUTPUT_JSON_NAME\n",
    "#         with open(json_path, 'w', encoding='utf-8') as f:\n",
    "#             json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "#         logger.info(f\"JSON report saved: {json_path}\")\n",
    "        \n",
    "#         # Create final combined visualization\n",
    "#         logger.info(\"Creating final tampering analysis visualization...\")\n",
    "#         output_image_path = Config.OUTPUT_DIR / Config.OUTPUT_IMAGE_NAME\n",
    "#         visualize_final_result(\n",
    "#             Config.IMAGE_PATH,\n",
    "#             aadhaar_boxes,\n",
    "#             splice_boxes,\n",
    "#             tampered_fields,\n",
    "#             unmapped_splices,\n",
    "#             output_image_path\n",
    "#         )\n",
    "        \n",
    "#         # Print detailed summary\n",
    "#         logger.info(\"=\"*70)\n",
    "#         logger.info(\"DETECTION SUMMARY\")\n",
    "#         logger.info(\"=\"*70)\n",
    "#         logger.info(f\"Verdict: {report['summary']['verdict']}\")\n",
    "#         logger.info(f\"Severity: {report['summary']['tampering_severity']}\")\n",
    "#         logger.info(f\"Status: {'‚ö†Ô∏è  TAMPERED' if report['summary']['is_tampered'] else '‚úì AUTHENTIC'}\")\n",
    "#         logger.info(f\"\\nDetection Statistics:\")\n",
    "#         logger.info(f\"  ‚Ä¢ Total Aadhaar Fields: {report['summary']['total_fields_detected']}\")\n",
    "#         logger.info(f\"  ‚Ä¢ Splice Regions Found: {report['summary']['total_splice_regions']}\")\n",
    "#         logger.info(f\"  ‚Ä¢ Tampered Fields: {report['summary']['tampered_field_count']}\")\n",
    "#         logger.info(f\"  ‚Ä¢ Removed/Missing Fields: {report['summary']['removed_or_missing_fields']}\")\n",
    "        \n",
    "#         if splice_boxes:\n",
    "#             logger.info(f\"\\nSplice Detection Details:\")\n",
    "#             for idx, splice in enumerate(splice_boxes):\n",
    "#                 logger.info(f\"  Splice #{idx + 1}: Confidence {splice['confidence']:.3f}\")\n",
    "        \n",
    "#         if tampered_fields:\n",
    "#             logger.info(\"\\n‚ö†Ô∏è  Tampered Regions Identified:\")\n",
    "#             for field_name, data in tampered_fields.items():\n",
    "#                 logger.info(f\"  ‚Ä¢ {field_name.upper()}\")\n",
    "#                 logger.info(f\"    - Max Overlap: {data['max_overlap']:.2%}\")\n",
    "#                 logger.info(f\"    - Affected by {len(data['splice_regions'])} splice region(s)\")\n",
    "#                 for sr in data['splice_regions']:\n",
    "#                     logger.info(f\"      ‚Üí {sr['splice_id']}: {sr['overlap']:.2%} overlap\")\n",
    "        \n",
    "#         if unmapped_splices:\n",
    "#             logger.info(\"\\nüö® CRITICAL: Unmapped Splice Regions (Field Removal Suspected):\")\n",
    "#             for unmapped in unmapped_splices:\n",
    "#                 logger.info(f\"  ‚Ä¢ {unmapped['splice_id'].upper()}\")\n",
    "#                 logger.info(f\"    - Confidence: {unmapped['confidence']:.3f}\")\n",
    "#                 logger.info(f\"    - Location: {unmapped['bbox']}\")\n",
    "#                 logger.info(f\"    - Reason: {unmapped['reason']}\")\n",
    "#                 logger.info(f\"    ‚ö†Ô∏è  No corresponding Aadhaar field detected at this location!\")\n",
    "#                 logger.info(f\"    ‚Üí Possible scenarios:\")\n",
    "#                 logger.info(f\"       - Photo completely removed\")\n",
    "#                 logger.info(f\"       - Name/Address/DOB removed\")\n",
    "#                 logger.info(f\"       - QR code removed\")\n",
    "#                 logger.info(f\"       - Field severely damaged/obliterated\")\n",
    "        \n",
    "#         logger.info(f\"\\nProcessing Time: {processing_time:.3f} seconds\")\n",
    "#         logger.info(\"\\nGenerated Files:\")\n",
    "#         logger.info(f\"  1. {Config.SPLICE_DETECTION_NAME} - Shows splice regions detected\")\n",
    "#         logger.info(f\"  2. {Config.AADHAAR_FIELDS_NAME} - Shows Aadhaar fields detected\")\n",
    "#         logger.info(f\"  3. {Config.OUTPUT_IMAGE_NAME} - Final tampering analysis\")\n",
    "#         logger.info(f\"  4. {Config.OUTPUT_JSON_NAME} - Detailed JSON report\")\n",
    "#         logger.info(\"=\"*70)\n",
    "        \n",
    "#         return report\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error during processing: {str(e)}\", exc_info=True)\n",
    "#         raise\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\"\"\"\n",
    "Aadhaar Card Tampering Detection System - Enhanced Version\n",
    "Uses dual YOLO models with containment-based tampering logic\n",
    "Handles missing critical fields (photo, aadhaar_number)\n",
    "\"\"\"\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "import logging\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Central configuration for the tampering detection system\"\"\"\n",
    "    \n",
    "    # Model paths\n",
    "    AADHAAR_MODEL_PATH = Path(r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\best.pt\")\n",
    "    SPLICE_MODEL_PATH = Path(r\"C:\\Users\\dhruv\\Downloads\\yolo_runsnew_2\\detect\\train\\weights\\best.pt\")\n",
    "    \n",
    "    # Input paths\n",
    "    IMAGE_PATH = Path(r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Dataset\\Spliced_Images\\1_forged10.png\")\n",
    "    SPLICED_IMAGE_PATH = Path(r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Noiseprint_Images\\noiseprint_1_forged10.png\")\n",
    "    \n",
    "    # Output configuration\n",
    "    OUTPUT_DIR = Path(\"./output\")\n",
    "    OUTPUT_IMAGE_NAME = \"final_tampering_result.jpg\"\n",
    "    SPLICE_DETECTION_NAME = \"splice_detection_visualization.jpg\"\n",
    "    NOISEPRINT_SPLICE_NAME = \"noiseprint_splice_regions.jpg\"\n",
    "    AADHAAR_FIELDS_NAME = \"aadhaar_fields_detected.jpg\"\n",
    "    OUTPUT_JSON_NAME = \"detection_result.json\"\n",
    "    \n",
    "    # Detection thresholds\n",
    "    CONF_THRESHOLD = 0.25\n",
    "    IOU_THRESHOLD = 0.15\n",
    "    CONTAINMENT_THRESHOLD = 0.70  # If 70%+ of field is inside splice, mark as 100% tampered\n",
    "    \n",
    "    # Critical fields that must be present\n",
    "    CRITICAL_FIELDS = [\"photo\", \"aadhaar number\"]\n",
    "    \n",
    "    # Visualization settings\n",
    "    COLOR_TAMPERED = (0, 0, 255)      # Red for tampered regions\n",
    "    COLOR_SAFE = (0, 255, 0)          # Green for safe regions\n",
    "    COLOR_SPLICE = (255, 0, 255)      # Magenta for splice detection\n",
    "    COLOR_SPLICE_BOX = (255, 165, 0)  # Orange for splice boxes\n",
    "    COLOR_CRITICAL_MISSING = (0, 0, 139)  # Dark red for missing critical fields\n",
    "    BOX_THICKNESS = 3\n",
    "    SPLICE_BOX_THICKNESS = 4\n",
    "    TEXT_FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    TEXT_SCALE = 0.7\n",
    "    TEXT_THICKNESS = 2\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================================\n",
    "# CORE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_iou(box_a: List[int], box_b: List[int]) -> float:\n",
    "    \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes.\"\"\"\n",
    "    x_left = max(box_a[0], box_b[0])\n",
    "    y_top = max(box_a[1], box_b[1])\n",
    "    x_right = min(box_a[2], box_b[2])\n",
    "    y_bottom = min(box_a[3], box_b[3])\n",
    "    \n",
    "    intersection_width = max(0, x_right - x_left)\n",
    "    intersection_height = max(0, y_bottom - y_top)\n",
    "    intersection_area = intersection_width * intersection_height\n",
    "    \n",
    "    if intersection_area == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    box_a_area = (box_a[2] - box_a[0]) * (box_a[3] - box_a[1])\n",
    "    box_b_area = (box_b[2] - box_b[0]) * (box_b[3] - box_b[1])\n",
    "    union_area = box_a_area + box_b_area - intersection_area\n",
    "    \n",
    "    return intersection_area / float(union_area)\n",
    "\n",
    "\n",
    "def calculate_containment(field_box: List[int], splice_box: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate what percentage of the field box is contained within the splice box.\n",
    "    Returns value between 0 and 1.\n",
    "    \n",
    "    If splice_box completely covers field_box, returns 1.0 (100% contained)\n",
    "    \"\"\"\n",
    "    x_left = max(field_box[0], splice_box[0])\n",
    "    y_top = max(field_box[1], splice_box[1])\n",
    "    x_right = min(field_box[2], splice_box[2])\n",
    "    y_bottom = min(field_box[3], splice_box[3])\n",
    "    \n",
    "    intersection_width = max(0, x_right - x_left)\n",
    "    intersection_height = max(0, y_bottom - y_top)\n",
    "    intersection_area = intersection_width * intersection_height\n",
    "    \n",
    "    if intersection_area == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    field_area = (field_box[2] - field_box[0]) * (field_box[3] - field_box[1])\n",
    "    \n",
    "    if field_area == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Return percentage of field that's inside splice region\n",
    "    return intersection_area / float(field_area)\n",
    "\n",
    "\n",
    "def load_models(aadhaar_path: Path, splice_path: Path) -> Tuple[YOLO, YOLO]:\n",
    "    \"\"\"Load both YOLO models with error handling.\"\"\"\n",
    "    if not aadhaar_path.exists():\n",
    "        raise FileNotFoundError(f\"Aadhaar model not found: {aadhaar_path}\")\n",
    "    if not splice_path.exists():\n",
    "        raise FileNotFoundError(f\"Splice model not found: {splice_path}\")\n",
    "    \n",
    "    logger.info(\"Loading Aadhaar field detection model...\")\n",
    "    aadhaar_model = YOLO(str(aadhaar_path))\n",
    "    \n",
    "    logger.info(\"Loading splice detection model...\")\n",
    "    splice_model = YOLO(str(splice_path))\n",
    "    \n",
    "    logger.info(\"Models loaded successfully\")\n",
    "    return aadhaar_model, splice_model\n",
    "\n",
    "\n",
    "def extract_detections(results, model_name: str) -> List[Dict]:\n",
    "    \"\"\"Extract bounding boxes and labels from YOLO results.\"\"\"\n",
    "    detections = []\n",
    "    \n",
    "    if len(results[0].boxes) == 0:\n",
    "        logger.warning(f\"No detections from {model_name}\")\n",
    "        return detections\n",
    "    \n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        confidence = float(box.conf[0])\n",
    "        \n",
    "        detection = {\n",
    "            \"bbox\": [x1, y1, x2, y2],\n",
    "            \"confidence\": confidence\n",
    "        }\n",
    "        \n",
    "        # Add label if available (for Aadhaar model)\n",
    "        if hasattr(box, 'cls'):\n",
    "            cls_id = int(box.cls[0])\n",
    "            label = results[0].names[cls_id]\n",
    "            detection[\"label\"] = label\n",
    "        \n",
    "        detections.append(detection)\n",
    "    \n",
    "    logger.info(f\"{model_name}: Detected {len(detections)} regions\")\n",
    "    return detections\n",
    "\n",
    "\n",
    "def check_critical_fields(aadhaar_boxes: List[Dict]) -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Check which critical fields are present in the detected Aadhaar fields.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping field name to presence boolean\n",
    "    \"\"\"\n",
    "    detected_labels = {box.get(\"label\", \"\").lower() for box in aadhaar_boxes}\n",
    "    \n",
    "    presence = {}\n",
    "    for critical_field in Config.CRITICAL_FIELDS:\n",
    "        presence[critical_field] = critical_field.lower() in detected_labels\n",
    "    \n",
    "    return presence\n",
    "\n",
    "\n",
    "def map_tampering(aadhaar_boxes: List[Dict], \n",
    "                  splice_boxes: List[Dict], \n",
    "                  iou_threshold: float,\n",
    "                  containment_threshold: float) -> Tuple[Dict, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Map splice detections to Aadhaar fields using CONTAINMENT logic.\n",
    "    \n",
    "    Key Logic Change:\n",
    "    - If a field is >= containment_threshold contained in a splice region,\n",
    "      mark it as 100% tampered (tampering_percentage = 1.0)\n",
    "    - Otherwise, use IoU overlap percentage\n",
    "    \n",
    "    Args:\n",
    "        aadhaar_boxes: List of detected Aadhaar field boxes\n",
    "        splice_boxes: List of detected splice regions\n",
    "        iou_threshold: Minimum IoU to consider overlap\n",
    "        containment_threshold: If field is this % contained, mark as 100% tampered\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (tampered_fields dict, unmapped_splices list)\n",
    "    \"\"\"\n",
    "    tampered_fields = {}\n",
    "    mapped_splice_ids = set()\n",
    "    \n",
    "    for splice_idx, splice in enumerate(splice_boxes):\n",
    "        splice_id = f\"splice_{splice_idx + 1}\"\n",
    "        splice_mapped = False\n",
    "        \n",
    "        for field in aadhaar_boxes:\n",
    "            # Calculate both IoU and containment\n",
    "            iou_overlap = calculate_iou(splice[\"bbox\"], field[\"bbox\"])\n",
    "            containment = calculate_containment(field[\"bbox\"], splice[\"bbox\"])\n",
    "            \n",
    "            # Check if there's significant overlap\n",
    "            if iou_overlap > iou_threshold or containment > iou_threshold:\n",
    "                splice_mapped = True\n",
    "                mapped_splice_ids.add(splice_idx)\n",
    "                field_label = field.get(\"label\", \"unknown\")\n",
    "                \n",
    "                # Determine tampering percentage\n",
    "                if containment >= containment_threshold:\n",
    "                    # Field is substantially contained in splice region\n",
    "                    # Mark as 100% tampered\n",
    "                    tampering_percentage = 1.0\n",
    "                    tampering_type = \"CONTAINED\"\n",
    "                    logger.info(f\"Field '{field_label}' is {containment:.1%} contained in {splice_id} - marking as 100% tampered\")\n",
    "                else:\n",
    "                    # Use IoU as tampering percentage\n",
    "                    tampering_percentage = iou_overlap\n",
    "                    tampering_type = \"OVERLAP\"\n",
    "                \n",
    "                if field_label not in tampered_fields:\n",
    "                    tampered_fields[field_label] = {\n",
    "                        \"max_tampering\": tampering_percentage,\n",
    "                        \"tampering_type\": tampering_type,\n",
    "                        \"containment\": containment,\n",
    "                        \"iou\": iou_overlap,\n",
    "                        \"confidence\": field[\"confidence\"],\n",
    "                        \"bbox\": field[\"bbox\"],\n",
    "                        \"splice_regions\": []\n",
    "                    }\n",
    "                \n",
    "                # Track which splice regions affect this field\n",
    "                tampered_fields[field_label][\"splice_regions\"].append({\n",
    "                    \"splice_id\": splice_id,\n",
    "                    \"containment\": containment,\n",
    "                    \"iou\": iou_overlap,\n",
    "                    \"tampering_percentage\": tampering_percentage,\n",
    "                    \"bbox\": splice[\"bbox\"]\n",
    "                })\n",
    "                \n",
    "                # Update max tampering if this is higher\n",
    "                if tampering_percentage > tampered_fields[field_label][\"max_tampering\"]:\n",
    "                    tampered_fields[field_label][\"max_tampering\"] = tampering_percentage\n",
    "                    tampered_fields[field_label][\"tampering_type\"] = tampering_type\n",
    "                    tampered_fields[field_label][\"containment\"] = containment\n",
    "                    tampered_fields[field_label][\"iou\"] = iou_overlap\n",
    "    \n",
    "    # Identify unmapped splice regions\n",
    "    unmapped_splices = []\n",
    "    for splice_idx, splice in enumerate(splice_boxes):\n",
    "        if splice_idx not in mapped_splice_ids:\n",
    "            unmapped_splices.append({\n",
    "                \"splice_id\": f\"splice_{splice_idx + 1}\",\n",
    "                \"bbox\": splice[\"bbox\"],\n",
    "                \"confidence\": splice[\"confidence\"],\n",
    "                \"reason\": \"MISSING_OR_REMOVED_FIELD\"\n",
    "            })\n",
    "    \n",
    "    return tampered_fields, unmapped_splices\n",
    "\n",
    "\n",
    "def visualize_splice_detection(noiseprint_image_path: Path,\n",
    "                               splice_boxes: List[Dict],\n",
    "                               output_path: Path) -> np.ndarray:\n",
    "    \"\"\"Visualize ONLY the splice detection on the noiseprint image.\"\"\"\n",
    "    image = cv2.imread(str(noiseprint_image_path))\n",
    "    \n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not load noiseprint image: {noiseprint_image_path}\")\n",
    "    \n",
    "    overlay = image.copy()\n",
    "    \n",
    "    for idx, splice in enumerate(splice_boxes):\n",
    "        x1, y1, x2, y2 = splice[\"bbox\"]\n",
    "        confidence = splice.get(\"confidence\", 0)\n",
    "        \n",
    "        cv2.rectangle(overlay, (x1, y1), (x2, y2), Config.COLOR_SPLICE, -1)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), Config.COLOR_SPLICE_BOX, Config.SPLICE_BOX_THICKNESS)\n",
    "        \n",
    "        label = f\"Splice #{idx + 1} [{confidence:.2f}]\"\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "            label, Config.TEXT_FONT, Config.TEXT_SCALE, Config.TEXT_THICKNESS\n",
    "        )\n",
    "        \n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (x1, y1 - text_height - baseline - 5),\n",
    "            (x1 + text_width, y1),\n",
    "            Config.COLOR_SPLICE_BOX,\n",
    "            -1\n",
    "        )\n",
    "        \n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (x1, y1 - baseline - 5),\n",
    "            Config.TEXT_FONT,\n",
    "            Config.TEXT_SCALE,\n",
    "            (255, 255, 255),\n",
    "            Config.TEXT_THICKNESS\n",
    "        )\n",
    "    \n",
    "    cv2.addWeighted(overlay, 0.3, image, 0.7, 0, image)\n",
    "    \n",
    "    header_text = f\"SPLICE DETECTION: {len(splice_boxes)} Region(s) Found\"\n",
    "    cv2.putText(image, header_text, (10, 35), Config.TEXT_FONT, 0.9, (255, 255, 255), 3)\n",
    "    cv2.putText(image, header_text, (10, 35), Config.TEXT_FONT, 0.9, Config.COLOR_SPLICE_BOX, 2)\n",
    "    \n",
    "    cv2.imwrite(str(output_path), image)\n",
    "    logger.info(f\"Splice detection visualization saved: {output_path}\")\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def visualize_aadhaar_fields(image_path: Path,\n",
    "                             aadhaar_boxes: List[Dict],\n",
    "                             critical_field_status: Dict[str, bool],\n",
    "                             output_path: Path) -> np.ndarray:\n",
    "    \"\"\"Visualize Aadhaar field detection with critical field warnings.\"\"\"\n",
    "    image = cv2.imread(str(image_path))\n",
    "    \n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "    for field in aadhaar_boxes:\n",
    "        x1, y1, x2, y2 = field[\"bbox\"]\n",
    "        label = field.get(\"label\", \"field\")\n",
    "        confidence = field.get(\"confidence\", 0)\n",
    "        \n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), Config.COLOR_SAFE, Config.BOX_THICKNESS)\n",
    "        \n",
    "        text = f\"{label} [{confidence:.2f}]\"\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "            text, Config.TEXT_FONT, Config.TEXT_SCALE, Config.TEXT_THICKNESS\n",
    "        )\n",
    "        \n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (x1, y1 - text_height - baseline - 5),\n",
    "            (x1 + text_width, y1),\n",
    "            Config.COLOR_SAFE,\n",
    "            -1\n",
    "        )\n",
    "        \n",
    "        cv2.putText(\n",
    "            image,\n",
    "            text,\n",
    "            (x1, y1 - baseline - 5),\n",
    "            Config.TEXT_FONT,\n",
    "            Config.TEXT_SCALE,\n",
    "            (255, 255, 255),\n",
    "            Config.TEXT_THICKNESS\n",
    "        )\n",
    "    \n",
    "    header = f\"AADHAAR FIELDS DETECTED: {len(aadhaar_boxes)}\"\n",
    "    cv2.putText(image, header, (10, 35), Config.TEXT_FONT, 0.9, (255, 255, 255), 3)\n",
    "    cv2.putText(image, header, (10, 35), Config.TEXT_FONT, 0.9, Config.COLOR_SAFE, 2)\n",
    "    \n",
    "    # Add critical field warnings\n",
    "    missing_critical = [f for f, present in critical_field_status.items() if not present]\n",
    "    if missing_critical:\n",
    "        y_offset = 70\n",
    "        for field in missing_critical:\n",
    "            warning = f\"‚ö† CRITICAL: {field.upper()} NOT DETECTED\"\n",
    "            cv2.putText(image, warning, (10, y_offset), Config.TEXT_FONT, 0.8, (255, 255, 255), 3)\n",
    "            cv2.putText(image, warning, (10, y_offset), Config.TEXT_FONT, 0.8, Config.COLOR_CRITICAL_MISSING, 2)\n",
    "            y_offset += 30\n",
    "    \n",
    "    cv2.imwrite(str(output_path), image)\n",
    "    logger.info(f\"Aadhaar fields visualization saved: {output_path}\")\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def visualize_final_result(image_path: Path, \n",
    "                          aadhaar_boxes: List[Dict],\n",
    "                          splice_boxes: List[Dict],\n",
    "                          tampered_fields: Dict,\n",
    "                          unmapped_splices: List[Dict],\n",
    "                          critical_field_status: Dict[str, bool],\n",
    "                          output_path: Path) -> np.ndarray:\n",
    "    \"\"\"Create final combined visualization with containment-based tampering.\"\"\"\n",
    "    image = cv2.imread(str(image_path))\n",
    "    \n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "    overlay = image.copy()\n",
    "    \n",
    "    # Draw splice regions\n",
    "    for idx, splice in enumerate(splice_boxes):\n",
    "        x1, y1, x2, y2 = splice[\"bbox\"]\n",
    "        cv2.rectangle(overlay, (x1, y1), (x2, y2), Config.COLOR_SPLICE, -1)\n",
    "    \n",
    "    cv2.addWeighted(overlay, 0.25, image, 0.75, 0, image)\n",
    "    \n",
    "    # Draw Aadhaar field boxes\n",
    "    for field in aadhaar_boxes:\n",
    "        x1, y1, x2, y2 = field[\"bbox\"]\n",
    "        label = field.get(\"label\", \"field\")\n",
    "        confidence = field.get(\"confidence\", 0)\n",
    "        \n",
    "        if label in tampered_fields:\n",
    "            color = Config.COLOR_TAMPERED\n",
    "            tampering_pct = tampered_fields[label][\"max_tampering\"]\n",
    "            tampering_type = tampered_fields[label][\"tampering_type\"]\n",
    "            containment = tampered_fields[label][\"containment\"]\n",
    "            \n",
    "            if tampering_type == \"CONTAINED\":\n",
    "                status = f\"‚ö† 100% TAMPERED (CONTAINED)\"\n",
    "                text = f\"{label} ({status}) [{confidence:.2f}]\"\n",
    "            else:\n",
    "                status = f\"‚ö† {tampering_pct:.0%} TAMPERED\"\n",
    "                text = f\"{label} ({status}) [{confidence:.2f}]\"\n",
    "        else:\n",
    "            color = Config.COLOR_SAFE\n",
    "            status = \"‚úì SAFE\"\n",
    "            text = f\"{label} ({status}) [{confidence:.2f}]\"\n",
    "        \n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, Config.BOX_THICKNESS)\n",
    "        \n",
    "        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "            text, Config.TEXT_FONT, Config.TEXT_SCALE, Config.TEXT_THICKNESS\n",
    "        )\n",
    "        \n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (x1, y1 - text_height - baseline - 8),\n",
    "            (x1 + text_width + 4, y1),\n",
    "            color,\n",
    "            -1\n",
    "        )\n",
    "        \n",
    "        cv2.putText(\n",
    "            image,\n",
    "            text,\n",
    "            (x1 + 2, y1 - baseline - 5),\n",
    "            Config.TEXT_FONT,\n",
    "            Config.TEXT_SCALE,\n",
    "            (255, 255, 255),\n",
    "            Config.TEXT_THICKNESS\n",
    "        )\n",
    "    \n",
    "    # Draw unmapped splice regions\n",
    "    for unmapped in unmapped_splices:\n",
    "        x1, y1, x2, y2 = unmapped[\"bbox\"]\n",
    "        confidence = unmapped[\"confidence\"]\n",
    "        \n",
    "        color = (255, 0, 0)\n",
    "        thickness = 4\n",
    "        dash_length = 10\n",
    "        \n",
    "        # Dashed border\n",
    "        for x in range(x1, x2, dash_length * 2):\n",
    "            cv2.line(image, (x, y1), (min(x + dash_length, x2), y1), color, thickness)\n",
    "        for x in range(x1, x2, dash_length * 2):\n",
    "            cv2.line(image, (x, y2), (min(x + dash_length, x2), y2), color, thickness)\n",
    "        for y in range(y1, y2, dash_length * 2):\n",
    "            cv2.line(image, (x1, y), (x1, min(y + dash_length, y2)), color, thickness)\n",
    "        for y in range(y1, y2, dash_length * 2):\n",
    "            cv2.line(image, (x2, y), (x2, min(y + dash_length, y2)), color, thickness)\n",
    "        \n",
    "        text = f\"‚ö† REMOVED/MISSING FIELD [{confidence:.2f}]\"\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "            text, Config.TEXT_FONT, 0.65, Config.TEXT_THICKNESS\n",
    "        )\n",
    "        \n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (x1, y1 - text_height - baseline - 8),\n",
    "            (x1 + text_width + 4, y1),\n",
    "            color,\n",
    "            -1\n",
    "        )\n",
    "        \n",
    "        cv2.putText(\n",
    "            image,\n",
    "            text,\n",
    "            (x1 + 2, y1 - baseline - 5),\n",
    "            Config.TEXT_FONT,\n",
    "            0.65,\n",
    "            (255, 255, 255),\n",
    "            Config.TEXT_THICKNESS\n",
    "        )\n",
    "        \n",
    "        # X mark\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        size = 20\n",
    "        cv2.line(image, (center_x - size, center_y - size), \n",
    "                (center_x + size, center_y + size), color, 5)\n",
    "        cv2.line(image, (center_x + size, center_y - size), \n",
    "                (center_x - size, center_y + size), color, 5)\n",
    "    \n",
    "    # Summary\n",
    "    missing_critical = [f for f, present in critical_field_status.items() if not present]\n",
    "    unmapped_count = len(unmapped_splices)\n",
    "    \n",
    "    summary_lines = [\n",
    "        f\"TAMPERING ANALYSIS RESULT\",\n",
    "        f\"Fields: {len(aadhaar_boxes)} | Splice: {len(splice_boxes)} | Tampered: {len(tampered_fields)} | Removed: {unmapped_count}\"\n",
    "    ]\n",
    "    \n",
    "    if missing_critical:\n",
    "        summary_lines.append(f\"‚ö† MISSING CRITICAL: {', '.join([f.upper() for f in missing_critical])}\")\n",
    "    \n",
    "    is_tampered = (tampered_fields or unmapped_splices or missing_critical)\n",
    "    summary_lines.append(f\"Status: {'DOCUMENT TAMPERED/INVALID' if is_tampered else 'DOCUMENT AUTHENTIC'}\")\n",
    "    \n",
    "    y_offset = 30\n",
    "    for line in summary_lines:\n",
    "        cv2.putText(image, line, (10, y_offset), Config.TEXT_FONT, 0.8, (255, 255, 255), 4)\n",
    "        text_color = Config.COLOR_TAMPERED if is_tampered else Config.COLOR_SAFE\n",
    "        cv2.putText(image, line, (10, y_offset), Config.TEXT_FONT, 0.8, text_color, 2)\n",
    "        y_offset += 30\n",
    "    \n",
    "    cv2.imwrite(str(output_path), image)\n",
    "    logger.info(f\"Final result visualization saved: {output_path}\")\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def generate_report(aadhaar_boxes: List[Dict],\n",
    "                   splice_boxes: List[Dict],\n",
    "                   tampered_fields: Dict,\n",
    "                   unmapped_splices: List[Dict],\n",
    "                   critical_field_status: Dict[str, bool],\n",
    "                   processing_time: float) -> Dict:\n",
    "    \"\"\"Generate comprehensive JSON report with containment logic.\"\"\"\n",
    "    missing_critical = [f for f, present in critical_field_status.items() if not present]\n",
    "    is_tampered = len(tampered_fields) > 0 or len(unmapped_splices) > 0 or len(missing_critical) > 0\n",
    "    \n",
    "    # Determine severity\n",
    "    if missing_critical:\n",
    "        severity = \"CRITICAL\"\n",
    "    elif unmapped_splices:\n",
    "        severity = \"HIGH\"\n",
    "    elif tampered_fields:\n",
    "        severity = \"MEDIUM\"\n",
    "    else:\n",
    "        severity = \"NONE\"\n",
    "    \n",
    "    report = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"processing_time_seconds\": round(processing_time, 3),\n",
    "        \"summary\": {\n",
    "            \"is_tampered\": is_tampered,\n",
    "            \"verdict\": \"DOCUMENT TAMPERED/INVALID\" if is_tampered else \"DOCUMENT AUTHENTIC\",\n",
    "            \"total_fields_detected\": len(aadhaar_boxes),\n",
    "            \"total_splice_regions\": len(splice_boxes),\n",
    "            \"tampered_field_count\": len(tampered_fields),\n",
    "            \"removed_or_missing_fields\": len(unmapped_splices),\n",
    "            \"missing_critical_fields\": missing_critical,\n",
    "            \"tampering_severity\": severity\n",
    "        },\n",
    "        \"critical_field_analysis\": {\n",
    "            field: {\n",
    "                \"present\": present,\n",
    "                \"status\": \"DETECTED\" if present else \"MISSING\"\n",
    "            }\n",
    "            for field, present in critical_field_status.items()\n",
    "        },\n",
    "        \"splice_detections\": [\n",
    "            {\n",
    "                \"splice_id\": f\"splice_{idx + 1}\",\n",
    "                \"confidence\": round(splice[\"confidence\"], 3),\n",
    "                \"bounding_box\": splice[\"bbox\"]\n",
    "            }\n",
    "            for idx, splice in enumerate(splice_boxes)\n",
    "        ],\n",
    "        \"tampered_fields\": [\n",
    "            {\n",
    "                \"field_name\": field_name,\n",
    "                \"tampering_percentage\": round(data[\"max_tampering\"] * 100, 1),\n",
    "                \"tampering_type\": data[\"tampering_type\"],\n",
    "                \"containment\": round(data[\"containment\"] * 100, 1),\n",
    "                \"iou\": round(data[\"iou\"], 3),\n",
    "                \"detection_confidence\": round(data[\"confidence\"], 3),\n",
    "                \"bounding_box\": data[\"bbox\"],\n",
    "                \"affecting_splice_regions\": [\n",
    "                    {\n",
    "                        \"splice_id\": sr[\"splice_id\"],\n",
    "                        \"containment\": round(sr[\"containment\"] * 100, 1),\n",
    "                        \"iou\": round(sr[\"iou\"], 3),\n",
    "                        \"tampering_pct\": round(sr[\"tampering_percentage\"] * 100, 1)\n",
    "                    }\n",
    "                    for sr in data[\"splice_regions\"]\n",
    "                ]\n",
    "            }\n",
    "            for field_name, data in tampered_fields.items()\n",
    "        ],\n",
    "        \"unmapped_splice_regions\": [\n",
    "            {\n",
    "                \"splice_id\": unmapped[\"splice_id\"],\n",
    "                \"confidence\": round(unmapped[\"confidence\"], 3),\n",
    "                \"bounding_box\": unmapped[\"bbox\"],\n",
    "                \"reason\": unmapped[\"reason\"]\n",
    "            }\n",
    "            for unmapped in unmapped_splices\n",
    "        ],\n",
    "        \"all_detected_fields\": [\n",
    "            {\n",
    "                \"label\": box.get(\"label\", \"unknown\"),\n",
    "                \"confidence\": round(box[\"confidence\"], 3),\n",
    "                \"bounding_box\": box[\"bbox\"],\n",
    "                \"status\": \"TAMPERED\" if box.get(\"label\") in tampered_fields else \"SAFE\"\n",
    "            }\n",
    "            for box in aadhaar_boxes\n",
    "        ],\n",
    "        \"detection_parameters\": {\n",
    "            \"confidence_threshold\": Config.CONF_THRESHOLD,\n",
    "            \"iou_threshold\": Config.IOU_THRESHOLD,\n",
    "            \"containment_threshold\": Config.CONTAINMENT_THRESHOLD\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline for tampering detection\"\"\"\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        Config.OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "        \n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(\"AADHAAR TAMPERING DETECTION SYSTEM - ENHANCED\")\n",
    "        logger.info(\"=\"*70)\n",
    "        \n",
    "        if not Config.IMAGE_PATH.exists():\n",
    "            raise FileNotFoundError(f\"Image not found: {Config.IMAGE_PATH}\")\n",
    "        if not Config.SPLICED_IMAGE_PATH.exists():\n",
    "            raise FileNotFoundError(f\"Spliced image not found: {Config.SPLICED_IMAGE_PATH}\")\n",
    "        \n",
    "        # Load models\n",
    "        aadhaar_model, splice_model = load_models(\n",
    "            Config.AADHAAR_MODEL_PATH,\n",
    "            Config.SPLICE_MODEL_PATH\n",
    "        )\n",
    "        \n",
    "        # Run inference\n",
    "        logger.info(\"Running Aadhaar field detection...\")\n",
    "        aadhaar_results = aadhaar_model(\n",
    "            str(Config.IMAGE_PATH),\n",
    "            conf=Config.CONF_THRESHOLD,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Running splice detection on noiseprint image...\")\n",
    "        splice_results = splice_model(\n",
    "            str(Config.SPLICED_IMAGE_PATH),\n",
    "            conf=Config.CONF_THRESHOLD,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Extract detections\n",
    "        aadhaar_boxes = extract_detections(aadhaar_results, \"Aadhaar Model\")\n",
    "        splice_boxes = extract_detections(splice_results, \"Splice Model\")\n",
    "        \n",
    "        # Check critical fields\n",
    "        logger.info(\"Checking for critical fields...\")\n",
    "        critical_field_status = check_critical_fields(aadhaar_boxes)\n",
    "        \n",
    "        missing_critical = [f for f, present in critical_field_status.items() if not present]\n",
    "        if missing_critical:\n",
    "            logger.warning(f\"üö® CRITICAL FIELDS MISSING: {', '.join([f.upper() for f in missing_critical])}\")\n",
    "        \n",
    "        # Visualize splice detection separately\n",
    "        logger.info(\"Creating splice detection visualization...\")\n",
    "        splice_viz_path = Config.OUTPUT_DIR / Config.SPLICE_DETECTION_NAME\n",
    "        visualize_splice_detection(\n",
    "            Config.SPLICED_IMAGE_PATH,\n",
    "            splice_boxes,\n",
    "            splice_viz_path\n",
    "        )\n",
    "        \n",
    "        # Visualize Aadhaar fields with critical field status\n",
    "        logger.info(\"Creating Aadhaar fields visualization...\")\n",
    "        fields_viz_path = Config.OUTPUT_DIR / Config.AADHAAR_FIELDS_NAME\n",
    "        visualize_aadhaar_fields(\n",
    "            Config.IMAGE_PATH,\n",
    "            aadhaar_boxes,\n",
    "            critical_field_status,\n",
    "            fields_viz_path\n",
    "        )\n",
    "        \n",
    "        # Map tampering with containment logic\n",
    "        logger.info(\"Analyzing tampering patterns (containment-based)...\")\n",
    "        tampered_fields, unmapped_splices = map_tampering(\n",
    "            aadhaar_boxes,\n",
    "            splice_boxes,\n",
    "            Config.IOU_THRESHOLD,\n",
    "            Config.CONTAINMENT_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Log unmapped splices\n",
    "        if unmapped_splices:\n",
    "            logger.warning(f\"‚ö†Ô∏è  Found {len(unmapped_splices)} unmapped splice region(s)!\")\n",
    "            logger.warning(\"This indicates field removal or severe tampering.\")\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report = generate_report(\n",
    "            aadhaar_boxes,\n",
    "            splice_boxes,\n",
    "            tampered_fields,\n",
    "            unmapped_splices,\n",
    "            critical_field_status,\n",
    "            processing_time\n",
    "        )\n",
    "        \n",
    "        # Save JSON report\n",
    "        json_path = Config.OUTPUT_DIR / Config.OUTPUT_JSON_NAME\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"JSON report saved: {json_path}\")\n",
    "        \n",
    "        # Create final combined visualization\n",
    "        logger.info(\"Creating final tampering analysis visualization...\")\n",
    "        output_image_path = Config.OUTPUT_DIR / Config.OUTPUT_IMAGE_NAME\n",
    "        visualize_final_result(\n",
    "            Config.IMAGE_PATH,\n",
    "            aadhaar_boxes,\n",
    "            splice_boxes,\n",
    "            tampered_fields,\n",
    "            unmapped_splices,\n",
    "            critical_field_status,\n",
    "            output_image_path\n",
    "        )\n",
    "        \n",
    "        # Print detailed summary\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(\"DETECTION SUMMARY\")\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"Verdict: {report['summary']['verdict']}\")\n",
    "        logger.info(f\"Severity: {report['summary']['tampering_severity']}\")\n",
    "        logger.info(f\"Status: {'‚ö†Ô∏è  TAMPERED/INVALID' if report['summary']['is_tampered'] else '‚úì AUTHENTIC'}\")\n",
    "        \n",
    "        # Critical fields status\n",
    "        logger.info(f\"\\nCritical Fields Status:\")\n",
    "        for field, present in critical_field_status.items():\n",
    "            status_icon = \"‚úì\" if present else \"‚úó\"\n",
    "            status_text = \"DETECTED\" if present else \"MISSING\"\n",
    "            logger.info(f\"  {status_icon} {field.upper()}: {status_text}\")\n",
    "        \n",
    "        logger.info(f\"\\nDetection Statistics:\")\n",
    "        logger.info(f\"  ‚Ä¢ Total Aadhaar Fields: {report['summary']['total_fields_detected']}\")\n",
    "        logger.info(f\"  ‚Ä¢ Splice Regions Found: {report['summary']['total_splice_regions']}\")\n",
    "        logger.info(f\"  ‚Ä¢ Tampered Fields: {report['summary']['tampered_field_count']}\")\n",
    "        logger.info(f\"  ‚Ä¢ Removed/Missing Fields: {report['summary']['removed_or_missing_fields']}\")\n",
    "        \n",
    "        if splice_boxes:\n",
    "            logger.info(f\"\\nSplice Detection Details:\")\n",
    "            for idx, splice in enumerate(splice_boxes):\n",
    "                logger.info(f\"  Splice #{idx + 1}: Confidence {splice['confidence']:.3f}\")\n",
    "        \n",
    "        if tampered_fields:\n",
    "            logger.info(\"\\n‚ö†Ô∏è  Tampered Fields Identified:\")\n",
    "            for field_name, data in tampered_fields.items():\n",
    "                logger.info(f\"  ‚Ä¢ {field_name.upper()}\")\n",
    "                logger.info(f\"    - Tampering: {data['max_tampering']:.0%} ({data['tampering_type']})\")\n",
    "                logger.info(f\"    - Containment: {data['containment']:.1%}\")\n",
    "                logger.info(f\"    - IoU: {data['iou']:.1%}\")\n",
    "                logger.info(f\"    - Affected by {len(data['splice_regions'])} splice region(s)\")\n",
    "                for sr in data['splice_regions']:\n",
    "                    logger.info(f\"      ‚Üí {sr['splice_id']}: {sr['tampering_percentage']:.0%} tampering\")\n",
    "        \n",
    "        if unmapped_splices:\n",
    "            logger.info(\"\\nüö® CRITICAL: Unmapped Splice Regions (Field Removal Suspected):\")\n",
    "            for unmapped in unmapped_splices:\n",
    "                logger.info(f\"  ‚Ä¢ {unmapped['splice_id'].upper()}\")\n",
    "                logger.info(f\"    - Confidence: {unmapped['confidence']:.3f}\")\n",
    "                logger.info(f\"    - Location: {unmapped['bbox']}\")\n",
    "                logger.info(f\"    - Reason: {unmapped['reason']}\")\n",
    "                logger.info(f\"    ‚ö†Ô∏è  No corresponding Aadhaar field detected at this location!\")\n",
    "        \n",
    "        if missing_critical:\n",
    "            logger.info(\"\\nüö® CRITICAL: Missing Essential Fields:\")\n",
    "            for field in missing_critical:\n",
    "                logger.info(f\"  ‚Ä¢ {field.upper()} - NOT DETECTED\")\n",
    "                logger.info(f\"    ‚Üí This field is required for valid Aadhaar verification\")\n",
    "        \n",
    "        logger.info(f\"\\nProcessing Time: {processing_time:.3f} seconds\")\n",
    "        logger.info(\"\\nGenerated Files:\")\n",
    "        logger.info(f\"  1. {Config.SPLICE_DETECTION_NAME} - Shows splice regions detected\")\n",
    "        logger.info(f\"  2. {Config.AADHAAR_FIELDS_NAME} - Shows Aadhaar fields detected\")\n",
    "        logger.info(f\"  3. {Config.OUTPUT_IMAGE_NAME} - Final tampering analysis\")\n",
    "        logger.info(f\"  4. {Config.OUTPUT_JSON_NAME} - Detailed JSON report\")\n",
    "        logger.info(\"=\"*70)\n",
    "        \n",
    "        # Final verdict\n",
    "        if missing_critical:\n",
    "            logger.error(\"‚ùå DOCUMENT INVALID: Critical fields missing\")\n",
    "        elif tampered_fields or unmapped_splices:\n",
    "            logger.error(\"‚ùå DOCUMENT TAMPERED: Forgery detected\")\n",
    "        else:\n",
    "            logger.info(\"‚úÖ DOCUMENT AUTHENTIC: No tampering detected\")\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during processing: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f2543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp': '2026-01-07T14:26:16.792022', 'verdict': 'DOCUMENT_TAMPERED', 'severity': 'HIGH', 'critical_fields_missing': False, 'tampered_fields': {}, 'unmapped_splice_regions': [{'bbox': [3, 66, 375, 140], 'issue': 'REMOVED_FIELD_DETECTED'}, {'bbox': [0, 69, 374, 141], 'issue': 'REMOVED_FIELD_DETECTED'}, {'bbox': [371, 71, 466, 137], 'issue': 'REMOVED_FIELD_DETECTED'}]}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "317794b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 800x800 small 101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Noiseprint_Images'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from Noiseprint import *\n",
    "import os\n",
    "\n",
    "def noiseprint_creation(img_path):\n",
    "    examples = []\n",
    "    noiseprints = []\n",
    "\n",
    "    for img_path in glob.glob(f'{img_path}\\\\*'):\n",
    "        example, noise_print = getNoiseprint(img_path)\n",
    "        examples.append(example)\n",
    "        noiseprints.append([img_path.split('\\\\')[-1].split('.')[0], noise_print])\n",
    "\n",
    "    output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Noiseprint_Images\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    border = 34\n",
    "\n",
    "    for name, res in noiseprints:\n",
    "        original_shape = res.shape\n",
    "        \n",
    "        # Crop border to get clean noiseprint\n",
    "        if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "            crop = res[border:-border, border:-border]\n",
    "        else:\n",
    "            crop = res\n",
    "            border = 0  # No padding needed if image too small\n",
    "        \n",
    "        # Normalize the cropped region\n",
    "        vmin = np.min(crop)\n",
    "        vmax = np.max(crop)\n",
    "        norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "        \n",
    "        # Pad back to original size to maintain dimensions\n",
    "        if border > 0:\n",
    "            # Use 'edge' mode to replicate edge values\n",
    "            padded = np.pad(norm_crop, pad_width=border, mode='edge')\n",
    "        else:\n",
    "            padded = norm_crop\n",
    "        \n",
    "        # Ensure exact original size (in case of rounding issues)\n",
    "        if padded.shape != original_shape:\n",
    "            # Resize to exact original dimensions\n",
    "            from skimage.transform import resize\n",
    "            padded = resize(padded, original_shape, mode='edge', anti_aliasing=False)\n",
    "        \n",
    "        # Save with name - same size as original\n",
    "        filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "        plt.imsave(filename, padded, cmap='gray')\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "noiseprint_creation(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Upload_Photo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e866b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 800x800 small 101\n",
      "Noiseprints saved to: C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Noiseprint_Images\n",
      "Cropped originals saved to: C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Original_Cropped_Images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Noiseprint_Images',\n",
       " 'C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Original_Cropped_Images')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from Noiseprint import *\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def noiseprint_creation(img_path):\n",
    "    examples = []\n",
    "    noiseprints = []\n",
    "\n",
    "    for file_path in glob.glob(f'{img_path}\\\\*'):\n",
    "        example, noise_print = getNoiseprint(file_path)\n",
    "        examples.append(example)\n",
    "        noiseprints.append([file_path.split('\\\\')[-1].split('.')[0], noise_print, example, file_path])\n",
    "\n",
    "    output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Noiseprint_Images\"\n",
    "    original_cropped_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Original_Cropped_Images\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(original_cropped_dir, exist_ok=True)\n",
    "\n",
    "    border = 2\n",
    "\n",
    "    for name, res, original_img, file_path in noiseprints:\n",
    "        original_h, original_w = res.shape[:2]\n",
    "        \n",
    "        # Crop border from BOTH noiseprint and original image\n",
    "        if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "            crop_noiseprint = res[border:-border, border:-border]\n",
    "            crop_original = original_img[border:-border, border:-border]\n",
    "        else:\n",
    "            crop_noiseprint = res\n",
    "            crop_original = original_img\n",
    "            border = 0\n",
    "        \n",
    "        # === Process Noiseprint ===\n",
    "        # Normalize the cropped noiseprint\n",
    "        vmin = np.min(crop_noiseprint)\n",
    "        vmax = np.max(crop_noiseprint)\n",
    "        norm_crop = np.clip((crop_noiseprint - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "        \n",
    "        # Save cropped noiseprint (without padding back)\n",
    "        filename_noiseprint = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "        plt.imsave(filename_noiseprint, norm_crop, cmap='gray')\n",
    "        \n",
    "        # === Process Original Image ===\n",
    "        # Read the original image directly from file to preserve color\n",
    "        original_img_color = cv2.imread(file_path)\n",
    "        \n",
    "        if original_img_color is not None:\n",
    "            # Convert BGR to RGB\n",
    "            original_img_color = cv2.cvtColor(original_img_color, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Apply same crop\n",
    "            if original_img_color.shape[0] > 2 * border and original_img_color.shape[1] > 2 * border:\n",
    "                crop_original_color = original_img_color[border:-border, border:-border]\n",
    "            else:\n",
    "                crop_original_color = original_img_color\n",
    "            \n",
    "            # Save as RGB\n",
    "            filename_original = os.path.join(original_cropped_dir, f\"{name}.png\")\n",
    "            plt.imsave(filename_original, crop_original_color)\n",
    "        else:\n",
    "            # Fallback to using the provided original_img\n",
    "            filename_original = os.path.join(original_cropped_dir, f\"{name}.png\")\n",
    "            if len(crop_original.shape) == 3:\n",
    "                plt.imsave(filename_original, np.clip(crop_original, 0, 1))\n",
    "            else:\n",
    "                plt.imsave(filename_original, crop_original, cmap='gray')\n",
    "    \n",
    "    print(f\"Noiseprints saved to: {output_dir}\")\n",
    "    print(f\"Cropped originals saved to: {original_cropped_dir}\")\n",
    "    \n",
    "    return output_dir, original_cropped_dir\n",
    "\n",
    "noiseprint_creation(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Upload_Photo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db5759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "number=2374971804270526477833002468783965837992554564899874087591661303561346432389832047870524302186901344489362368642972767716416349990805756094923115719687656090691368051627957878187788907419297818953295185555346288172578594637886352753543271000481717080003254556962148594350559820352806251787713278744047402230989238559317351232114240089849934148895256488140236015024800731753594740948640957680138566468247224859669467819596919398964809164399637893729212452791889199675715949918925838319591794702333094022248132120531152523331442741730158840977243402215102904932650832502847295644794421419704633765033761284508863534321317394686768650111457751139630853448637215423705157211510636160227953566227527799608082928846103264491539001327407775670834868948113753614112563650255058316849200536533335903554984254814901522086937767458409075617572843449110393213525925388131214952874629655799772119820372255291052673056372346072235458198199995637720424196884145247220163810790179386390283738429482893152518286247124911446073389185062482901364671389605727763080854673156754021728522287806275420847159574631844674460263574901590412679291518508010087116598357407343835408554094619585212373168435612645646129147973594416508676872819776522537778717985070402222824965034768103900739105784663244748432502180989441389718131079445941981681118258324511923246198334046020123727749408128519721102477302359413240175102907322619462289965085963377744024233678337951462006962521823224880199210318367946130004264196899778609815012001799773327514133268825910089483612283510244566484854597156100473055413090101948456959122378865704840756793122956663218517626099291311352417342899623681483097817511136427210593032393600010728324905512596767095096153856032112835755780472808814199620390836980020899858288860556611564167406292139646289142056168261133256777093245980048335918156712295254776487472431445495668303900536289283098315798552328294391152828182614909451410115516297083658174657554955228963550255866282688308751041517464999930825273776417639569977754844191402927594739069037851707477839207593911886893016618794870530622356073909077832279869798641545167528509966656120623184120128052588408742941658045827255866966100249857968956536613250770326334844204927432961924987891433020671754710428050564671868464658436926086493709176888821257183419013229795869757265111599482263223604228286513011751601176504567030118257385997460972803240338899836840030438830725520798480181575861397469056536579877274090338750406459700907704031830137890544492015701251066934352867527112361743047684237105216779177819594030160887368311805926405114938744235859610328064947158936962470654636736991567663705830950312548447653861922078087824048793236971354828540758657075837209006713701763902429652486225300535997260665898927924843608750347193892239342462507130025307878412116604096773706728162016134101751551184021079984480254041743057914746472840768175369369852937574401874295943063507273467384747124843744395375119899278823903202010381949145094804675442110869084589592876721655764753871572233276245590041302887094585204427900634246823674277680009401177473636685542700515621164233992970974893989913447733956146698563285998205950467321954304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ccfe164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Dataset\\Real_Images\\94.jpg: 640x640 2 qr_codes, 68.4ms\n",
      "Speed: 5.2ms preprocess, 68.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\runs\\detect\\predict44\u001b[0m\n",
      "Inference done. Detection image saved.\n",
      "[‚úî] High-quality QR saved ‚Üí extracted_qr\\qr_1_hq.png\n",
      "[‚úî] High-quality QR saved ‚Üí extracted_qr\\qr_2_hq.png\n",
      "‚úÖ QR extraction completed.\n",
      "No QR code detected\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "import zxingcpp\n",
    "\n",
    "# -----------------------------\n",
    "# Load YOLO model\n",
    "# -----------------------------\n",
    "model = YOLO(r\"C:\\\\Users\\\\dhruv\\\\Downloads\\\\qr_runs\\\\detect\\\\train\\\\weights\\\\best.pt\")\n",
    "\n",
    "# -----------------------------\n",
    "# Input image\n",
    "# -----------------------------\n",
    "# img_path = r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Real_Images\\\\10.jpg\"\n",
    "img_path = r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Real_Images\\\\94.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# -----------------------------\n",
    "# YOLO inference\n",
    "# -----------------------------\n",
    "results = model(img_path, conf=0.25, save=True)\n",
    "print(\"Inference done. Detection image saved.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Output directory\n",
    "# -----------------------------\n",
    "output_dir = \"extracted_qr\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "qr_count = 0\n",
    "\n",
    "# -----------------------------\n",
    "# Process detections\n",
    "# -----------------------------\n",
    "for result in results:\n",
    "    if result.boxes is None:\n",
    "        continue\n",
    "\n",
    "    for box in result.boxes:\n",
    "        cls_id = int(box.cls[0])\n",
    "        class_name = model.names[cls_id]\n",
    "\n",
    "        if class_name.lower() == \"qr_code\":\n",
    "            qr_count += 1\n",
    "\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "\n",
    "            h, w, _ = img.shape\n",
    "\n",
    "            # -----------------------------\n",
    "            # Add padding (VERY IMPORTANT)\n",
    "            # -----------------------------\n",
    "            pad = 15\n",
    "            x1 = max(0, x1 - pad)\n",
    "            y1 = max(0, y1 - pad)\n",
    "            x2 = min(w, x2 + pad)\n",
    "            y2 = min(h, y2 + pad)\n",
    "\n",
    "            qr_crop = img[y1:y2, x1:x2]\n",
    "\n",
    "            # -----------------------------\n",
    "            # Convert to grayscale\n",
    "            # -----------------------------\n",
    "            qr_gray = cv2.cvtColor(qr_crop, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Upscale (simulate higher DPI)\n",
    "            # -----------------------------\n",
    "            scale = 4  # 2 or 3 is ideal\n",
    "            qr_upscaled = cv2.resize(\n",
    "                qr_gray,\n",
    "                None,\n",
    "                fx=scale,\n",
    "                fy=scale,\n",
    "                interpolation=cv2.INTER_CUBIC\n",
    "            )\n",
    "\n",
    "            # -----------------------------\n",
    "            # Optional: Sharpen\n",
    "            # -----------------------------\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "            qr_sharp = cv2.morphologyEx(qr_upscaled, cv2.MORPH_GRADIENT, kernel)\n",
    " \n",
    "            # -----------------------------\n",
    "            # Save as PNG (lossless)\n",
    "            # -----------------------------\n",
    "            save_path = os.path.join(output_dir, f\"qr_{qr_count}_hq.png\")\n",
    "            cv2.imwrite(save_path, qr_upscaled)\n",
    "\n",
    "            print(f\"[‚úî] High-quality QR saved ‚Üí {save_path}\")\n",
    "\n",
    "if qr_count == 0:\n",
    "    print(\"‚ùå No QR extracted.\")\n",
    "else:\n",
    "    print(\"‚úÖ QR extraction completed.\")\n",
    "\n",
    "img = cv2.imread(\n",
    "    r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\extracted_qr\\\\qr_1_hq.png\"\n",
    ")\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(\"Image not loaded. Check path.\")\n",
    "\n",
    "result = zxingcpp.read_barcode(img)\n",
    "\n",
    "if result:\n",
    "    print(\"Format:\", result.format)\n",
    "    print(\"QR Text:\", result.text)\n",
    "else:\n",
    "    print(\"No QR code detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "650111cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No QR code detected\n"
     ]
    }
   ],
   "source": [
    "import zxingcpp\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(\n",
    "    r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Real_Images\\\\94.jpg\"\n",
    ")\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(\"Image not loaded. Check path.\")\n",
    "\n",
    "result = zxingcpp.read_barcode(img)\n",
    "\n",
    "if result:\n",
    "    print(\"Format:\", result.format)\n",
    "    print(\"QR Text:\", result.text)\n",
    "else:\n",
    "    print(\"No QR code detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc247059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Trust\\data\\input\\9_forged9.png: 640x640 1 qr_code, 138.0ms\n",
      "Speed: 13.1ms preprocess, 138.0ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Trust\\notebooks\\runs\\detect\\predict8\u001b[0m\n",
      "Inference done. Detection image saved.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "import zxingcpp\n",
    "\n",
    "# -----------------------------\n",
    "# Load YOLO model\n",
    "# -----------------------------\n",
    "model = YOLO(r\"C:\\\\Users\\\\dhruv\\\\Downloads\\\\qr_runs\\\\detect\\\\train\\\\weights\\\\best.pt\")\n",
    "\n",
    "# -----------------------------\n",
    "# Input image\n",
    "# -----------------------------\n",
    "# img_path = r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Real_Images\\\\28.jpeg\"\n",
    "img_path = r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Trust\\data\\input\\9_forged9.png\"\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# -----------------------------\n",
    "# YOLO inference\n",
    "# -----------------------------\n",
    "results = model(img_path, conf=0.25, save=True)\n",
    "print(\"Inference done. Detection image saved.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Output directory\n",
    "# -----------------------------\n",
    "output_dir = \"extracted_qr\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "qr_count = 0\n",
    "\n",
    "# -----------------------------\n",
    "# Process detections\n",
    "# -----------------------------\n",
    "for result in results:\n",
    "    if result.boxes is None:\n",
    "        continue\n",
    "\n",
    "    for box in result.boxes:\n",
    "        cls_id = int(box.cls[0])\n",
    "        class_name = model.names[cls_id]\n",
    "\n",
    "        if class_name.lower() == \"qr_code\":\n",
    "            qr_count += 1\n",
    "\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "\n",
    "            h, w, _ = img.shape\n",
    "\n",
    "            # -----------------------------\n",
    "            # Add padding (VERY IMPORTANT)\n",
    "            # -----------------------------\n",
    "            pad = 15\n",
    "            x1 = max(0, x1 - pad)\n",
    "            y1 = max(0, y1 - pad)\n",
    "            x2 = min(w, x2 + pad)\n",
    "            y2 = min(h, y2 + pad)\n",
    "\n",
    "            qr_crop = img[y1:y2, x1:x2]\n",
    "\n",
    "            # -----------------------------\n",
    "            # Convert to grayscale\n",
    "            # -----------------------------\n",
    "            # -----------------------------\n",
    "            # Convert to grayscale\n",
    "            # -----------------------------\n",
    "            qr_gray = cv2.cvtColor(qr_crop, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Contrast enhancement\n",
    "            # -----------------------------\n",
    "            qr_gray = cv2.normalize(qr_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Adaptive threshold (BEST for QR)\n",
    "            # -----------------------------\n",
    "            qr_bin = cv2.adaptiveThreshold(\n",
    "                qr_gray,\n",
    "                255,\n",
    "                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                cv2.THRESH_BINARY,\n",
    "                31,\n",
    "                5\n",
    "            )\n",
    "\n",
    "            # -----------------------------\n",
    "            # Save as PNG (native resolution)\n",
    "            # -----------------------------\n",
    "            save_path = os.path.join(output_dir, f\"qr_{qr_count}_clean.png\")\n",
    "            cv2.imwrite(save_path, qr_bin)\n",
    "\n",
    "            print(f\"[‚úî] Clean QR saved ‚Üí {save_path}\")\n",
    "\n",
    "\n",
    "if qr_count == 0:\n",
    "    print(\"‚ùå No QR extracted.\")\n",
    "else:\n",
    "    print(\"‚úÖ QR extraction completed.\")\n",
    "\n",
    "img = cv2.imread(\n",
    "    r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\extracted_qr\\\\qr_1_clean.png\",\n",
    "    cv2.IMREAD_GRAYSCALE\n",
    ")\n",
    "\n",
    "result = zxingcpp.read_barcode(img)\n",
    "\n",
    "if result:\n",
    "    print(\"Format:\", result.format)\n",
    "    print(\"QR Text:\", result.text)\n",
    "else:\n",
    "    print(\"‚ùå QR not decoded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9071426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.99      1.00      1.00       100\n",
      "      Forged       1.00      0.99      0.99       100\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       1.00      0.99      0.99       200\n",
      "weighted avg       1.00      0.99      0.99       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Balanced_Data\\\\test\"     # <-- change this\n",
    "class_names = [\"Fake\", \"Real\"]\n",
    "class_to_idx = {\"Fake\": 0, \"Real\": 1}\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# -----------------------------\n",
    "# TRANSFORM (same as training)\n",
    "# -----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((572, 572)),   # or your training transform\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD MODEL\n",
    "# -----------------------------\n",
    "# model = RealFakeCNN().to(device)\n",
    "# model = torch.jit.load(\"detection_model.pt\", map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# model = torch.jit.load(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Noiseprint_Dataset\\\\best_model.pth\", map_location=device)\n",
    "model = torch.jit.load(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\detection_model.pt\", map_location=device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# INFERENCE\n",
    "# -----------------------------\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(test_dir, class_name)\n",
    "        label = class_to_idx[class_name]\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "            except:\n",
    "                continue  # skip unreadable files\n",
    "\n",
    "            img = transform(img).unsqueeze(0).to(device)  # [1, C, H, W]\n",
    "\n",
    "            output = model(img)          # [1, 1]\n",
    "            pred = (output >= THRESHOLD).long().item()\n",
    "            # print(img_name,' ',pred)\n",
    "            all_preds.append(pred)\n",
    "            all_labels.append(label)\n",
    "\n",
    "# -----------------------------\n",
    "# CONFUSION MATRIX\n",
    "# -----------------------------\n",
    "\n",
    "# acc = accuracy_score(all_labels, all_preds)\n",
    "# prec = precision_score(all_labels, all_preds)\n",
    "# rec = recall_score(all_labels, all_preds)\n",
    "# f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "# print(\"\\n================ RESULTS ================\\n\")\n",
    "# print(f\"Accuracy  : {acc:.4f}\")\n",
    "# print(f\"Precision : {prec:.4f}\")\n",
    "# print(f\"Recall    : {rec:.4f}\")\n",
    "# print(f\"F1 Score  : {f1:.4f}\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Forged\"]))\n",
    "\n",
    "# ----------------------------\n",
    "# CONFUSION MATRIX\n",
    "# ----------------------------\n",
    "# cm = confusion_matrix(all_labels, all_preds)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb846c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating REAL images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating FORGED images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 13.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ RESULTS ================\n",
      "\n",
      "Accuracy  : 0.9900\n",
      "Precision : 0.9900\n",
      "Recall    : 0.9900\n",
      "F1 Score  : 0.9900\n",
      "\n",
      "Confusion Matrix:\n",
      "[[99  1]\n",
      " [ 1 99]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.99      0.99      0.99       100\n",
      "      Forged       0.99      0.99      0.99       100\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       0.99      0.99      0.99       200\n",
      "weighted avg       0.99      0.99      0.99       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD MODEL\n",
    "# ----------------------------\n",
    "model = YOLO(r\"C:\\Users\\dhruv\\Downloads\\yolo_runsnew_2\\detect\\train\\weights\\best.pt\")\n",
    "\n",
    "# ----------------------------\n",
    "# DATASET PATH\n",
    "# ----------------------------\n",
    "TEST_ROOT = r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Dataset\\Balanced_Data\\test\"\n",
    "\n",
    "REAL_DIR = os.path.join(TEST_ROOT, \"real\")\n",
    "FORGED_DIR = os.path.join(TEST_ROOT, \"fake\")\n",
    "\n",
    "# ----------------------------\n",
    "# LABELS\n",
    "# ----------------------------\n",
    "# 0 = real, 1 = forged\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# ----------------------------\n",
    "# FUNCTION TO PREDICT ONE IMAGE\n",
    "# ----------------------------\n",
    "def predict_image(img_path):\n",
    "    results = model(img_path, conf=0.25, verbose=False)\n",
    "\n",
    "    r = results[0]\n",
    "\n",
    "    # If any detection exists ‚Üí forged\n",
    "    if r.boxes is not None and len(r.boxes) > 0:\n",
    "        return 1   # forged\n",
    "    else:\n",
    "        return 0   # real\n",
    "\n",
    "# ----------------------------\n",
    "# EVALUATE REAL IMAGES\n",
    "# ----------------------------\n",
    "print(\"Evaluating REAL images...\")\n",
    "for img in tqdm(os.listdir(REAL_DIR)):\n",
    "    img_path = os.path.join(REAL_DIR, img)\n",
    "\n",
    "    pred = predict_image(img_path)\n",
    "\n",
    "    y_true.append(0)   # real\n",
    "    y_pred.append(pred)\n",
    "\n",
    "# ----------------------------\n",
    "# EVALUATE FORGED IMAGES\n",
    "# ----------------------------\n",
    "print(\"Evaluating FORGED images...\")\n",
    "for img in tqdm(os.listdir(FORGED_DIR)):\n",
    "    img_path = os.path.join(FORGED_DIR, img)\n",
    "\n",
    "    pred = predict_image(img_path)\n",
    "\n",
    "    y_true.append(1)   # forged\n",
    "    y_pred.append(pred)\n",
    "\n",
    "# ----------------------------\n",
    "# METRICS\n",
    "# ----------------------------\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\n================ RESULTS ================\\n\")\n",
    "print(f\"Accuracy  : {acc:.4f}\")\n",
    "print(f\"Precision : {prec:.4f}\")\n",
    "print(f\"Recall    : {rec:.4f}\")\n",
    "print(f\"F1 Score  : {f1:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# CONFUSION MATRIX\n",
    "# ----------------------------\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Real\", \"Forged\"]))\n",
    "\n",
    "# ----------------------------\n",
    "# PLOT CONFUSION MATRIX\n",
    "# ----------------------------\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "plt.xticks([0,1], [\"Real\", \"Forged\"])\n",
    "plt.yticks([0,1], [\"Real\", \"Forged\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d12495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=RealFakeCNN\n",
      "  (model): RecursiveScriptModule(\n",
      "    original_name=Sequential\n",
      "    (0): RecursiveScriptModule(original_name=Conv2d)\n",
      "    (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "    (2): RecursiveScriptModule(original_name=ReLU)\n",
      "    (3): RecursiveScriptModule(original_name=Conv2d)\n",
      "    (4): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "    (5): RecursiveScriptModule(original_name=ReLU)\n",
      "    (6): RecursiveScriptModule(original_name=MaxPool2d)\n",
      "    (7): RecursiveScriptModule(original_name=Conv2d)\n",
      "    (8): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "    (9): RecursiveScriptModule(original_name=ReLU)\n",
      "    (10): RecursiveScriptModule(original_name=Conv2d)\n",
      "    (11): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "    (12): RecursiveScriptModule(original_name=ReLU)\n",
      "    (13): RecursiveScriptModule(original_name=MaxPool2d)\n",
      "    (14): RecursiveScriptModule(original_name=Conv2d)\n",
      "    (15): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "    (16): RecursiveScriptModule(original_name=ReLU)\n",
      "    (17): RecursiveScriptModule(original_name=Conv2d)\n",
      "    (18): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "    (19): RecursiveScriptModule(original_name=ReLU)\n",
      "    (20): RecursiveScriptModule(original_name=MaxPool2d)\n",
      "    (21): RecursiveScriptModule(original_name=Conv2d)\n",
      "    (22): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "    (23): RecursiveScriptModule(original_name=ReLU)\n",
      "    (24): RecursiveScriptModule(original_name=Conv2d)\n",
      "    (25): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "    (26): RecursiveScriptModule(original_name=ReLU)\n",
      "    (27): RecursiveScriptModule(original_name=AdaptiveAvgPool2d)\n",
      "    (28): RecursiveScriptModule(original_name=Flatten)\n",
      "    (29): RecursiveScriptModule(original_name=Linear)\n",
      "    (30): RecursiveScriptModule(original_name=ReLU)\n",
      "    (31): RecursiveScriptModule(original_name=Dropout)\n",
      "    (32): RecursiveScriptModule(original_name=Linear)\n",
      "    (33): RecursiveScriptModule(original_name=ReLU)\n",
      "    (34): RecursiveScriptModule(original_name=Dropout)\n",
      "    (35): RecursiveScriptModule(original_name=Linear)\n",
      "    (36): RecursiveScriptModule(original_name=Sigmoid)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.jit.load(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\detection_model.pt\", map_location=\"cpu\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7650e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
