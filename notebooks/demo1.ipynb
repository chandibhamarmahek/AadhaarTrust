{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ccb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from Noiseprint import *\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078f248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\Forged_Aadhaar\\\\forged_aadhaar_detection.pth\", weights_only=False)\n",
    "# model.eval()\n",
    "model = torch.jit.load(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\detection_model.pt\", map_location=torch.device(\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a6e7d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 229x572 small 101\n",
      "noiseprint_10_aug_0\n",
      " 229x572 small 101\n",
      "noiseprint_10_aug_1\n",
      " 229x572 small 101\n",
      "noiseprint_10_aug_2\n",
      " 229x572 small 101\n",
      "noiseprint_10_aug_3\n",
      " 229x572 small 101\n",
      "noiseprint_10_aug_4\n",
      " 229x572 small 101\n",
      "noiseprint_10_aug_5\n",
      " 229x572 small 101\n",
      "noiseprint_10_aug_6\n",
      " 229x572 small 101\n",
      "noiseprint_10_aug_7\n",
      " 229x572 small 101\n",
      "noiseprint_10_aug_8\n",
      " 229x572 small 101\n",
      "noiseprint_10_orig\n",
      " 280x572 small 101\n",
      "noiseprint_11_aug_0\n",
      " 280x572 small 101\n",
      "noiseprint_11_aug_1\n",
      " 280x572 small 101\n",
      "noiseprint_11_aug_2\n",
      " 280x572 small 101\n",
      "noiseprint_11_aug_3\n",
      " 280x572 small 101\n",
      "noiseprint_11_aug_4\n",
      " 280x572 small 101\n",
      "noiseprint_11_aug_5\n",
      " 280x572 small 101\n",
      "noiseprint_11_aug_6\n",
      " 280x572 small 101\n",
      "noiseprint_11_aug_7\n",
      " 280x572 small 101\n",
      "noiseprint_11_aug_8\n",
      " 280x572 small 101\n",
      "noiseprint_11_orig\n",
      " 234x483 small 101\n",
      "noiseprint_12_aug_0\n",
      " 234x483 small 101\n",
      "noiseprint_12_aug_1\n",
      " 234x483 small 101\n",
      "noiseprint_12_aug_2\n",
      " 234x483 small 101\n",
      "noiseprint_12_aug_3\n",
      " 234x483 small 101\n",
      "noiseprint_12_aug_4\n",
      " 234x483 small 101\n",
      "noiseprint_12_aug_5\n",
      " 234x483 small 101\n",
      "noiseprint_12_aug_6\n",
      " 234x483 small 101\n",
      "noiseprint_12_aug_7\n",
      " 234x483 small 101\n",
      "noiseprint_12_aug_8\n",
      " 234x483 small 101\n",
      "noiseprint_12_orig\n",
      " 259x509 small 101\n",
      "noiseprint_13_aug_0\n",
      " 259x509 small 101\n",
      "noiseprint_13_aug_1\n",
      " 259x509 small 101\n",
      "noiseprint_13_aug_2\n",
      " 259x509 small 101\n",
      "noiseprint_13_aug_3\n",
      " 259x509 small 101\n",
      "noiseprint_13_aug_4\n",
      " 259x509 small 101\n",
      "noiseprint_13_aug_5\n",
      " 259x509 small 101\n",
      "noiseprint_13_aug_6\n",
      " 259x509 small 101\n",
      "noiseprint_13_aug_7\n",
      " 259x509 small 101\n",
      "noiseprint_13_aug_8\n",
      " 259x509 small 101\n",
      "noiseprint_13_orig\n",
      " 237x572 small 101\n",
      "noiseprint_14_aug_0\n",
      " 237x572 small 101\n",
      "noiseprint_14_aug_1\n",
      " 237x572 small 101\n",
      "noiseprint_14_aug_2\n",
      " 237x572 small 101\n",
      "noiseprint_14_aug_3\n",
      " 237x572 small 101\n",
      "noiseprint_14_aug_4\n",
      " 237x572 small 101\n",
      "noiseprint_14_aug_5\n",
      " 237x572 small 101\n",
      "noiseprint_14_aug_6\n",
      " 237x572 small 101\n",
      "noiseprint_14_aug_7\n",
      " 237x572 small 101\n",
      "noiseprint_14_aug_8\n",
      " 237x572 small 101\n",
      "noiseprint_14_orig\n",
      " 253x572 small 101\n",
      "noiseprint_15_aug_0\n",
      " 253x572 small 101\n",
      "noiseprint_15_aug_1\n",
      " 253x572 small 101\n",
      "noiseprint_15_aug_2\n",
      " 253x572 small 101\n",
      "noiseprint_15_aug_3\n",
      " 253x572 small 101\n",
      "noiseprint_15_aug_4\n",
      " 253x572 small 101\n",
      "noiseprint_15_aug_5\n",
      " 253x572 small 101\n",
      "noiseprint_15_aug_6\n",
      " 253x572 small 101\n",
      "noiseprint_15_aug_7\n",
      " 253x572 small 101\n",
      "noiseprint_15_aug_8\n",
      " 253x572 small 101\n",
      "noiseprint_15_orig\n",
      " 230x572 small 101\n",
      "noiseprint_16_aug_0\n",
      " 230x572 small 101\n",
      "noiseprint_16_aug_1\n",
      " 230x572 small 101\n",
      "noiseprint_16_aug_2\n",
      " 230x572 small 101\n",
      "noiseprint_16_aug_3\n",
      " 230x572 small 101\n",
      "noiseprint_16_aug_4\n",
      " 230x572 small 101\n",
      "noiseprint_16_aug_5\n",
      " 230x572 small 101\n",
      "noiseprint_16_aug_6\n",
      " 230x572 small 101\n",
      "noiseprint_16_aug_7\n",
      " 230x572 small 101\n",
      "noiseprint_16_aug_8\n",
      " 230x572 small 101\n",
      "noiseprint_16_orig\n",
      " 259x572 small 101\n",
      "noiseprint_17_aug_0\n",
      " 259x572 small 101\n",
      "noiseprint_17_aug_1\n",
      " 259x572 small 101\n",
      "noiseprint_17_aug_2\n",
      " 259x572 small 101\n",
      "noiseprint_17_aug_3\n",
      " 259x572 small 101\n",
      "noiseprint_17_aug_4\n",
      " 259x572 small 101\n",
      "noiseprint_17_aug_5\n",
      " 259x572 small 101\n",
      "noiseprint_17_aug_6\n",
      " 259x572 small 101\n",
      "noiseprint_17_aug_7\n",
      " 259x572 small 101\n",
      "noiseprint_17_aug_8\n",
      " 259x572 small 101\n",
      "noiseprint_17_orig\n",
      " 230x572 small 101\n",
      "noiseprint_18_aug_0\n",
      " 230x572 small 101\n",
      "noiseprint_18_aug_1\n",
      " 230x572 small 101\n",
      "noiseprint_18_aug_2\n",
      " 230x572 small 101\n",
      "noiseprint_18_aug_3\n",
      " 230x572 small 101\n",
      "noiseprint_18_aug_4\n",
      " 230x572 small 101\n",
      "noiseprint_18_aug_5\n",
      " 230x572 small 101\n",
      "noiseprint_18_aug_6\n",
      " 230x572 small 101\n",
      "noiseprint_18_aug_7\n",
      " 230x572 small 101\n",
      "noiseprint_18_aug_8\n",
      " 230x572 small 101\n",
      "noiseprint_18_orig\n",
      " 255x572 small 101\n",
      "noiseprint_19_aug_0\n",
      " 255x572 small 101\n",
      "noiseprint_19_aug_1\n",
      " 255x572 small 101\n",
      "noiseprint_19_aug_2\n",
      " 255x572 small 101\n",
      "noiseprint_19_aug_3\n",
      " 255x572 small 101\n",
      "noiseprint_19_aug_4\n",
      " 255x572 small 101\n",
      "noiseprint_19_aug_5\n",
      " 255x572 small 101\n",
      "noiseprint_19_aug_6\n",
      " 255x572 small 101\n",
      "noiseprint_19_aug_7\n",
      " 255x572 small 101\n",
      "noiseprint_19_aug_8\n",
      " 255x572 small 101\n",
      "noiseprint_19_orig\n",
      " 732x732 small 101\n",
      "noiseprint_1_aug_0\n",
      " 732x732 small 101\n",
      "noiseprint_1_aug_1\n",
      " 732x732 small 101\n",
      "noiseprint_1_aug_2\n",
      " 732x732 small 101\n",
      "noiseprint_1_aug_3\n",
      " 732x732 small 101\n",
      "noiseprint_1_aug_4\n",
      " 732x732 small 101\n",
      "noiseprint_1_aug_5\n",
      " 732x732 small 101\n",
      "noiseprint_1_aug_6\n",
      " 732x732 small 101\n",
      "noiseprint_1_aug_7\n",
      " 732x732 small 101\n",
      "noiseprint_1_aug_8\n",
      " 732x732 small 101\n",
      "noiseprint_1_orig\n",
      " 239x572 small 101\n",
      "noiseprint_20_aug_0\n",
      " 239x572 small 101\n",
      "noiseprint_20_aug_1\n",
      " 239x572 small 101\n",
      "noiseprint_20_aug_2\n",
      " 239x572 small 101\n",
      "noiseprint_20_aug_3\n",
      " 239x572 small 101\n",
      "noiseprint_20_aug_4\n",
      " 239x572 small 101\n",
      "noiseprint_20_aug_5\n",
      " 239x572 small 101\n",
      "noiseprint_20_aug_6\n",
      " 239x572 small 101\n",
      "noiseprint_20_aug_7\n",
      " 239x572 small 101\n",
      "noiseprint_20_aug_8\n",
      " 239x572 small 101\n",
      "noiseprint_20_orig\n",
      " 248x533 small 101\n",
      "noiseprint_21_aug_0\n",
      " 248x533 small 101\n",
      "noiseprint_21_aug_1\n",
      " 248x533 small 101\n",
      "noiseprint_21_aug_2\n",
      " 248x533 small 101\n",
      "noiseprint_21_aug_3\n",
      " 248x533 small 101\n",
      "noiseprint_21_aug_4\n",
      " 248x533 small 101\n",
      "noiseprint_21_aug_5\n",
      " 248x533 small 101\n",
      "noiseprint_21_aug_6\n",
      " 248x533 small 101\n",
      "noiseprint_21_aug_7\n",
      " 248x533 small 101\n",
      "noiseprint_21_aug_8\n",
      " 248x533 small 101\n",
      "noiseprint_21_orig\n",
      " 228x572 small 101\n",
      "noiseprint_22_aug_0\n",
      " 228x572 small 101\n",
      "noiseprint_22_aug_1\n",
      " 228x572 small 101\n",
      "noiseprint_22_aug_2\n",
      " 228x572 small 101\n",
      "noiseprint_22_aug_3\n",
      " 228x572 small 101\n",
      "noiseprint_22_aug_4\n",
      " 228x572 small 101\n",
      "noiseprint_22_aug_5\n",
      " 228x572 small 101\n",
      "noiseprint_22_aug_6\n",
      " 228x572 small 101\n",
      "noiseprint_22_aug_7\n",
      " 228x572 small 101\n",
      "noiseprint_22_aug_8\n",
      " 228x572 small 101\n",
      "noiseprint_22_orig\n",
      " 572x572 small 101\n",
      "noiseprint_23_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_23_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_23_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_23_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_23_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_23_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_23_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_23_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_23_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_23_orig\n",
      " 572x572 small 101\n",
      "noiseprint_24_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_24_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_24_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_24_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_24_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_24_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_24_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_24_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_24_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_24_orig\n",
      " 572x572 small 101\n",
      "noiseprint_25_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_25_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_25_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_25_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_25_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_25_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_25_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_25_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_25_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_25_orig\n",
      " 313x737 small 101\n",
      "noiseprint_26_aug_0\n",
      " 313x737 small 101\n",
      "noiseprint_26_aug_1\n",
      " 313x737 small 101\n",
      "noiseprint_26_aug_2\n",
      " 313x737 small 101\n",
      "noiseprint_26_aug_3\n",
      " 313x737 small 101\n",
      "noiseprint_26_aug_4\n",
      " 313x737 small 101\n",
      "noiseprint_26_aug_5\n",
      " 313x737 small 101\n",
      "noiseprint_26_aug_6\n",
      " 313x737 small 101\n",
      "noiseprint_26_aug_7\n",
      " 313x737 small 101\n",
      "noiseprint_26_aug_8\n",
      " 313x737 small 101\n",
      "noiseprint_26_orig\n",
      " 572x572 small 101\n",
      "noiseprint_27_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_27_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_27_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_27_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_27_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_27_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_27_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_27_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_27_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_27_orig\n",
      " 627x1012 small 101\n",
      "noiseprint_28_aug_0\n",
      " 627x1012 small 101\n",
      "noiseprint_28_aug_1\n",
      " 627x1012 small 101\n",
      "noiseprint_28_aug_2\n",
      " 627x1012 small 101\n",
      "noiseprint_28_aug_3\n",
      " 627x1012 small 101\n",
      "noiseprint_28_aug_4\n",
      " 627x1012 small 101\n",
      "noiseprint_28_aug_5\n",
      " 627x1012 small 101\n",
      "noiseprint_28_aug_6\n",
      " 627x1012 small 101\n",
      "noiseprint_28_aug_7\n",
      " 627x1012 small 101\n",
      "noiseprint_28_aug_8\n",
      " 627x1012 small 101\n",
      "noiseprint_28_orig\n",
      " 572x572 small 101\n",
      "noiseprint_29_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_29_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_29_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_29_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_29_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_29_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_29_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_29_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_29_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_29_orig\n",
      " 572x572 small 101\n",
      "noiseprint_2_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_2_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_2_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_2_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_2_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_2_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_2_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_2_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_2_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_2_orig\n",
      " 572x572 small 101\n",
      "noiseprint_30_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_30_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_30_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_30_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_30_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_30_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_30_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_30_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_30_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_30_orig\n",
      " 572x572 small 101\n",
      "noiseprint_31_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_31_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_31_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_31_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_31_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_31_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_31_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_31_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_31_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_31_orig\n",
      " 572x572 small 101\n",
      "noiseprint_32_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_32_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_32_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_32_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_32_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_32_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_32_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_32_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_32_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_32_orig\n",
      " 572x572 small 101\n",
      "noiseprint_33_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_33_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_33_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_33_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_33_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_33_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_33_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_33_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_33_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_33_orig\n",
      " 572x572 small 101\n",
      "noiseprint_34_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_34_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_34_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_34_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_34_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_34_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_34_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_34_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_34_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_34_orig\n",
      " 572x572 small 101\n",
      "noiseprint_35_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_35_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_35_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_35_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_35_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_35_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_35_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_35_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_35_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_35_orig\n",
      " 572x572 small 101\n",
      "noiseprint_36_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_36_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_36_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_36_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_36_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_36_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_36_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_36_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_36_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_36_orig\n",
      " 572x572 small 101\n",
      "noiseprint_37_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_37_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_37_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_37_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_37_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_37_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_37_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_37_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_37_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_37_orig\n",
      " 572x572 small 101\n",
      "noiseprint_38_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_38_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_38_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_38_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_38_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_38_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_38_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_38_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_38_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_38_orig\n",
      " 572x572 small 101\n",
      "noiseprint_39_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_39_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_39_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_39_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_39_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_39_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_39_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_39_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_39_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_39_orig\n",
      " 572x572 small 101\n",
      "noiseprint_3_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_3_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_3_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_3_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_3_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_3_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_3_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_3_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_3_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_3_orig\n",
      " 572x572 small 101\n",
      "noiseprint_40_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_40_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_40_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_40_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_40_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_40_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_40_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_40_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_40_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_40_orig\n",
      " 572x572 small 101\n",
      "noiseprint_41_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_41_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_41_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_41_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_41_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_41_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_41_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_41_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_41_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_41_orig\n",
      " 572x572 small 101\n",
      "noiseprint_42_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_42_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_42_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_42_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_42_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_42_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_42_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_42_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_42_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_42_orig\n",
      " 572x572 small 101\n",
      "noiseprint_43_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_43_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_43_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_43_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_43_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_43_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_43_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_43_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_43_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_43_orig\n",
      " 572x572 small 101\n",
      "noiseprint_44_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_44_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_44_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_44_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_44_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_44_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_44_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_44_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_44_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_44_orig\n",
      " 344x732 small 101\n",
      "noiseprint_45_aug_0\n",
      " 344x732 small 101\n",
      "noiseprint_45_aug_1\n",
      " 344x732 small 101\n",
      "noiseprint_45_aug_2\n",
      " 344x732 small 101\n",
      "noiseprint_45_aug_3\n",
      " 344x732 small 101\n",
      "noiseprint_45_aug_4\n",
      " 344x732 small 101\n",
      "noiseprint_45_aug_5\n",
      " 344x732 small 101\n",
      "noiseprint_45_aug_6\n",
      " 344x732 small 101\n",
      "noiseprint_45_aug_7\n",
      " 344x732 small 101\n",
      "noiseprint_45_aug_8\n",
      " 344x732 small 101\n",
      "noiseprint_45_orig\n",
      " 572x572 small 101\n",
      "noiseprint_46_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_46_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_46_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_46_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_46_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_46_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_46_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_46_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_46_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_46_orig\n",
      " 572x572 small 101\n",
      "noiseprint_47_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_47_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_47_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_47_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_47_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_47_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_47_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_47_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_47_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_47_orig\n",
      " 572x572 small 101\n",
      "noiseprint_48_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_48_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_48_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_48_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_48_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_48_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_48_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_48_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_48_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_48_orig\n",
      " 313x531 small 101\n",
      "noiseprint_49_aug_0\n",
      " 313x531 small 101\n",
      "noiseprint_49_aug_1\n",
      " 313x531 small 101\n",
      "noiseprint_49_aug_2\n",
      " 313x531 small 101\n",
      "noiseprint_49_aug_3\n",
      " 313x531 small 101\n",
      "noiseprint_49_aug_4\n",
      " 313x531 small 101\n",
      "noiseprint_49_aug_5\n",
      " 313x531 small 101\n",
      "noiseprint_49_aug_6\n",
      " 313x531 small 101\n",
      "noiseprint_49_aug_7\n",
      " 313x531 small 101\n",
      "noiseprint_49_aug_8\n",
      " 313x531 small 101\n",
      "noiseprint_49_orig\n",
      " 572x572 small 101\n",
      "noiseprint_4_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_4_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_4_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_4_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_4_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_4_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_4_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_4_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_4_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_4_orig\n",
      " 572x572 small 101\n",
      "noiseprint_50_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_50_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_50_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_50_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_50_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_50_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_50_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_50_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_50_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_50_orig\n",
      " 732x732 small 101\n",
      "noiseprint_51_aug_0\n",
      " 732x732 small 101\n",
      "noiseprint_51_aug_1\n",
      " 732x732 small 101\n",
      "noiseprint_51_aug_2\n",
      " 732x732 small 101\n",
      "noiseprint_51_aug_3\n",
      " 732x732 small 101\n",
      "noiseprint_51_aug_4\n",
      " 732x732 small 101\n",
      "noiseprint_51_aug_5\n",
      " 732x732 small 101\n",
      "noiseprint_51_aug_6\n",
      " 732x732 small 101\n",
      "noiseprint_51_aug_7\n",
      " 732x732 small 101\n",
      "noiseprint_51_aug_8\n",
      " 732x732 small 101\n",
      "noiseprint_51_orig\n",
      " 732x732 small 101\n",
      "noiseprint_52_aug_0\n",
      " 732x732 small 101\n",
      "noiseprint_52_aug_1\n",
      " 732x732 small 101\n",
      "noiseprint_52_aug_2\n",
      " 732x732 small 101\n",
      "noiseprint_52_aug_3\n",
      " 732x732 small 101\n",
      "noiseprint_52_aug_4\n",
      " 732x732 small 101\n",
      "noiseprint_52_aug_5\n",
      " 732x732 small 101\n",
      "noiseprint_52_aug_6\n",
      " 732x732 small 101\n",
      "noiseprint_52_aug_7\n",
      " 732x732 small 101\n",
      "noiseprint_52_aug_8\n",
      " 732x732 small 101\n",
      "noiseprint_52_orig\n",
      " 572x572 small 101\n",
      "noiseprint_53_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_53_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_53_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_53_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_53_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_53_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_53_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_53_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_53_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_53_orig\n",
      " 572x572 small 101\n",
      "noiseprint_54_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_54_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_54_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_54_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_54_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_54_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_54_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_54_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_54_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_54_orig\n",
      " 572x572 small 101\n",
      "noiseprint_55_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_55_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_55_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_55_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_55_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_55_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_55_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_55_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_55_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_55_orig\n",
      " 572x572 small 101\n",
      "noiseprint_56_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_56_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_56_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_56_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_56_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_56_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_56_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_56_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_56_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_56_orig\n",
      " 572x572 small 101\n",
      "noiseprint_57_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_57_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_57_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_57_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_57_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_57_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_57_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_57_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_57_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_57_orig\n",
      " 572x572 small 101\n",
      "noiseprint_58_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_58_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_58_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_58_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_58_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_58_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_58_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_58_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_58_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_58_orig\n",
      " 572x572 small 101\n",
      "noiseprint_59_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_59_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_59_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_59_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_59_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_59_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_59_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_59_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_59_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_59_orig\n",
      " 572x572 small 101\n",
      "noiseprint_5_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_5_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_5_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_5_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_5_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_5_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_5_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_5_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_5_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_5_orig\n",
      " 572x572 small 101\n",
      "noiseprint_60_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_60_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_60_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_60_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_60_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_60_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_60_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_60_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_60_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_60_orig\n",
      " 572x572 small 101\n",
      "noiseprint_61_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_61_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_61_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_61_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_61_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_61_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_61_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_61_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_61_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_61_orig\n",
      " 572x572 small 101\n",
      "noiseprint_62_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_62_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_62_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_62_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_62_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_62_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_62_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_62_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_62_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_62_orig\n",
      " 572x572 small 101\n",
      "noiseprint_63_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_63_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_63_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_63_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_63_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_63_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_63_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_63_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_63_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_63_orig\n",
      " 572x572 small 101\n",
      "noiseprint_64_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_64_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_64_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_64_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_64_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_64_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_64_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_64_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_64_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_64_orig\n",
      " 680x1212 small 101\n",
      "noiseprint_65_aug_0\n",
      " 680x1212 small 101\n",
      "noiseprint_65_aug_1\n",
      " 680x1212 small 101\n",
      "noiseprint_65_aug_2\n",
      " 680x1212 small 101\n",
      "noiseprint_65_aug_3\n",
      " 680x1212 small 101\n",
      "noiseprint_65_aug_4\n",
      " 680x1212 small 101\n",
      "noiseprint_65_aug_5\n",
      " 680x1212 small 101\n",
      "noiseprint_65_aug_6\n",
      " 680x1212 small 101\n",
      "noiseprint_65_aug_7\n",
      " 680x1212 small 101\n",
      "noiseprint_65_aug_8\n",
      " 680x1212 small 101\n",
      "noiseprint_65_orig\n",
      " 458x700 small 101\n",
      "noiseprint_66_aug_0\n",
      " 458x700 small 101\n",
      "noiseprint_66_aug_1\n",
      " 458x700 small 101\n",
      "noiseprint_66_aug_2\n",
      " 458x700 small 101\n",
      "noiseprint_66_aug_3\n",
      " 458x700 small 101\n",
      "noiseprint_66_aug_4\n",
      " 458x700 small 101\n",
      "noiseprint_66_aug_5\n",
      " 458x700 small 101\n",
      "noiseprint_66_aug_6\n",
      " 458x700 small 101\n",
      "noiseprint_66_aug_7\n",
      " 458x700 small 101\n",
      "noiseprint_66_aug_8\n",
      " 458x700 small 101\n",
      "noiseprint_66_orig\n",
      " 572x572 small 101\n",
      "noiseprint_67_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_67_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_67_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_67_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_67_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_67_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_67_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_67_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_67_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_67_orig\n",
      " 572x572 small 101\n",
      "noiseprint_68_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_68_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_68_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_68_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_68_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_68_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_68_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_68_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_68_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_68_orig\n",
      " 572x572 small 101\n",
      "noiseprint_69_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_69_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_69_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_69_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_69_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_69_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_69_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_69_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_69_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_69_orig\n",
      " 572x572 small 101\n",
      "noiseprint_6_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_6_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_6_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_6_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_6_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_6_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_6_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_6_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_6_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_6_orig\n",
      " 572x572 small 101\n",
      "noiseprint_70_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_70_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_70_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_70_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_70_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_70_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_70_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_70_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_70_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_70_orig\n",
      " 1064x1863 large 101\n",
      "noiseprint_71_aug_0\n",
      " 1064x1863 large 101\n",
      "noiseprint_71_aug_1\n",
      " 1064x1863 large 101\n",
      "noiseprint_71_aug_2\n",
      " 1064x1863 large 101\n",
      "noiseprint_71_aug_3\n",
      " 1064x1863 large 101\n",
      "noiseprint_71_aug_4\n",
      " 1064x1863 large 101\n",
      "noiseprint_71_aug_5\n",
      " 1064x1863 large 101\n",
      "noiseprint_71_aug_6\n",
      " 1064x1863 large 101\n",
      "noiseprint_71_aug_7\n",
      " 1064x1863 large 101\n",
      "noiseprint_71_aug_8\n",
      " 1064x1863 large 101\n",
      "noiseprint_71_orig\n",
      " 572x572 small 101\n",
      "noiseprint_72_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_72_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_72_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_72_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_72_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_72_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_72_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_72_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_72_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_72_orig\n",
      " 572x572 small 101\n",
      "noiseprint_73_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_73_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_73_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_73_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_73_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_73_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_73_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_73_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_73_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_73_orig\n",
      " 572x572 small 101\n",
      "noiseprint_74_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_74_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_74_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_74_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_74_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_74_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_74_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_74_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_74_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_74_orig\n",
      " 572x572 small 101\n",
      "noiseprint_75_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_75_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_75_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_75_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_75_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_75_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_75_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_75_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_75_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_75_orig\n",
      " 572x572 small 101\n",
      "noiseprint_76_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_76_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_76_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_76_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_76_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_76_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_76_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_76_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_76_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_76_orig\n",
      " 572x572 small 101\n",
      "noiseprint_7_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_7_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_7_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_7_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_7_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_7_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_7_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_7_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_7_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_7_orig\n",
      " 335x348 small 101\n",
      "noiseprint_89_aug_0\n",
      " 335x348 small 101\n",
      "noiseprint_89_aug_1\n",
      " 335x348 small 101\n",
      "noiseprint_89_aug_2\n",
      " 335x348 small 101\n",
      "noiseprint_89_aug_3\n",
      " 335x348 small 101\n",
      "noiseprint_89_aug_4\n",
      " 335x348 small 101\n",
      "noiseprint_89_aug_5\n",
      " 335x348 small 101\n",
      "noiseprint_89_aug_6\n",
      " 335x348 small 101\n",
      "noiseprint_89_aug_7\n",
      " 335x348 small 101\n",
      "noiseprint_89_aug_8\n",
      " 335x348 small 101\n",
      "noiseprint_89_orig\n",
      " 572x572 small 101\n",
      "noiseprint_8_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_8_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_8_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_8_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_8_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_8_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_8_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_8_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_8_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_8_orig\n",
      " 572x572 small 101\n",
      "noiseprint_9_aug_0\n",
      " 572x572 small 101\n",
      "noiseprint_9_aug_1\n",
      " 572x572 small 101\n",
      "noiseprint_9_aug_2\n",
      " 572x572 small 101\n",
      "noiseprint_9_aug_3\n",
      " 572x572 small 101\n",
      "noiseprint_9_aug_4\n",
      " 572x572 small 101\n",
      "noiseprint_9_aug_5\n",
      " 572x572 small 101\n",
      "noiseprint_9_aug_6\n",
      " 572x572 small 101\n",
      "noiseprint_9_aug_7\n",
      " 572x572 small 101\n",
      "noiseprint_9_aug_8\n",
      " 572x572 small 101\n",
      "noiseprint_9_orig\n"
     ]
    }
   ],
   "source": [
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Balanced_Data\\\\train\\\\real\\\\*'):\n",
    "     example,noise_print=getNoiseprint(img_path)\n",
    "     examples.append(example)\n",
    "     print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "     noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Noiseprint_Images\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 800x800 small  75\n",
      "1\n",
      " 348x640 small  90\n",
      "11\n",
      " 800x800 small 101\n",
      "1_forged1\n",
      " 800x800 small 101\n",
      "1_forged3\n",
      " 800x800 small 101\n",
      "1_forged6\n",
      " 800x800 small 101\n",
      "1_forged8\n",
      " 695x1080 small  94\n",
      "28\n",
      " 640x640 small  75\n",
      "3\n",
      " 640x640 small  75\n",
      "30\n",
      " 640x640 small  75\n",
      "39\n",
      " 640x640 small  75\n",
      "44\n",
      " 412x800 small  90\n",
      "45\n",
      " 381x599 small  94\n",
      "49\n",
      " 640x640 small  75\n",
      "5\n",
      " 800x800 small  75\n",
      "51\n",
      " 800x800 small  75\n",
      "52\n",
      " 800x800 small 101\n",
      "52_forged1\n",
      " 800x800 small 101\n",
      "52_forged10\n",
      " 800x800 small 101\n",
      "52_forged7\n",
      " 800x800 small 101\n",
      "52_forged8\n",
      " 640x640 small  75\n",
      "53\n",
      " 640x640 small  75\n",
      "54\n",
      " 640x640 small  75\n",
      "58\n",
      " 748x1280 small  75\n",
      "65\n",
      " 526x768 small  90\n",
      "66\n",
      " 1132x1931 large  66\n",
      "71\n",
      " 1132x1931 large 101\n",
      "71_forged1\n",
      " 1132x1931 large 101\n",
      "71_forged10\n",
      " 1132x1931 large 101\n",
      "71_forged2\n",
      " 1132x1931 large 101\n",
      "71_forged3\n",
      " 1132x1931 large 101\n",
      "71_forged4\n",
      " 1132x1931 large 101\n",
      "71_forged5\n",
      " 1132x1931 large 101\n",
      "71_forged6\n",
      " 1132x1931 large 101\n",
      "71_forged7\n",
      " 1132x1931 large 101\n",
      "71_forged8\n",
      " 1132x1931 large 101\n",
      "71_forged9\n",
      " 640x640 small  75\n",
      "84\n",
      " 1020x1600 large  80\n",
      "85\n",
      " 1020x1600 large 101\n",
      "85_forged1\n",
      " 1020x1600 large 101\n",
      "85_forged10\n",
      " 1020x1600 large 101\n",
      "85_forged2\n",
      " 1020x1600 large 101\n",
      "85_forged3\n",
      " 1020x1600 large 101\n",
      "85_forged4\n",
      " 1020x1600 large 101\n",
      "85_forged5\n",
      " 1020x1600 large 101\n",
      "85_forged6\n",
      " 1020x1600 large 101\n",
      "85_forged7\n",
      " 1020x1600 large 101\n",
      "85_forged8\n",
      " 1020x1600 large 101\n",
      "85_forged9\n",
      " 336x800 small  90\n",
      "91\n",
      " 800x800 small  75\n",
      "94\n",
      " 800x800 small  75\n",
      "95\n",
      " 800x800 small  75\n",
      "96\n",
      " 800x800 small 101\n",
      "96_forged1\n",
      " 800x800 small  75\n",
      "97\n",
      " 273x859 small 101\n",
      "dhruvil\n",
      "noiseprint_1.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_11.png: Predicted as real (Confidence: 0.7308)\n",
      "noiseprint_1_forged1.png: Predicted as fake (Confidence: 0.5)\n",
      "noiseprint_1_forged3.png: Predicted as fake (Confidence: 0.5)\n",
      "noiseprint_1_forged6.png: Predicted as fake (Confidence: 0.5)\n",
      "noiseprint_1_forged8.png: Predicted as fake (Confidence: 0.5001)\n",
      "noiseprint_28.png: Predicted as real (Confidence: 0.7072)\n",
      "noiseprint_3.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_30.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_39.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_44.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_45.png: Predicted as real (Confidence: 0.7061)\n",
      "noiseprint_49.png: Predicted as fake (Confidence: 0.5817)\n",
      "noiseprint_5.png: Predicted as real (Confidence: 0.731)\n",
      "noiseprint_51.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_52.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_52_forged1.png: Predicted as fake (Confidence: 0.5)\n",
      "noiseprint_52_forged10.png: Predicted as fake (Confidence: 0.5)\n",
      "noiseprint_52_forged7.png: Predicted as fake (Confidence: 0.5)\n",
      "noiseprint_52_forged8.png: Predicted as fake (Confidence: 0.5003)\n",
      "noiseprint_53.png: Predicted as real (Confidence: 0.731)\n",
      "noiseprint_54.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_58.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_65.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_66.png: Predicted as real (Confidence: 0.7232)\n",
      "noiseprint_71.png: Predicted as real (Confidence: 0.7052)\n",
      "noiseprint_71_forged1.png: Predicted as fake (Confidence: 0.5)\n",
      "noiseprint_71_forged10.png: Predicted as fake (Confidence: 0.5005)\n",
      "noiseprint_71_forged2.png: Predicted as fake (Confidence: 0.501)\n",
      "noiseprint_71_forged3.png: Predicted as fake (Confidence: 0.5002)\n",
      "noiseprint_71_forged4.png: Predicted as fake (Confidence: 0.5008)\n",
      "noiseprint_71_forged5.png: Predicted as fake (Confidence: 0.5004)\n",
      "noiseprint_71_forged6.png: Predicted as fake (Confidence: 0.5016)\n",
      "noiseprint_71_forged7.png: Predicted as fake (Confidence: 0.5046)\n",
      "noiseprint_71_forged8.png: Predicted as fake (Confidence: 0.501)\n",
      "noiseprint_71_forged9.png: Predicted as fake (Confidence: 0.5013)\n",
      "noiseprint_84.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_85.png: Predicted as real (Confidence: 0.73)\n",
      "noiseprint_85_forged1.png: Predicted as fake (Confidence: 0.5637)\n",
      "noiseprint_85_forged10.png: Predicted as fake (Confidence: 0.5592)\n",
      "noiseprint_85_forged2.png: Predicted as fake (Confidence: 0.5785)\n",
      "noiseprint_85_forged3.png: Predicted as fake (Confidence: 0.5579)\n",
      "noiseprint_85_forged4.png: Predicted as fake (Confidence: 0.5279)\n",
      "noiseprint_85_forged5.png: Predicted as fake (Confidence: 0.5275)\n",
      "noiseprint_85_forged6.png: Predicted as fake (Confidence: 0.5562)\n",
      "noiseprint_85_forged7.png: Predicted as fake (Confidence: 0.5187)\n",
      "noiseprint_85_forged8.png: Predicted as fake (Confidence: 0.5675)\n",
      "noiseprint_85_forged9.png: Predicted as fake (Confidence: 0.5678)\n",
      "noiseprint_91.png: Predicted as fake (Confidence: 0.5887)\n",
      "noiseprint_94.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_95.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_96.png: Predicted as real (Confidence: 0.731)\n",
      "noiseprint_96_forged1.png: Predicted as fake (Confidence: 0.5013)\n",
      "noiseprint_97.png: Predicted as real (Confidence: 0.7311)\n",
      "noiseprint_dhruvil.png: Predicted as real (Confidence: 0.7229)\n",
      "Inference Time: 634.5979 seconds\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# start = timer()\n",
    "\n",
    "# examples=[]\n",
    "# noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "#      example,noise_print=getNoiseprint(img_path)\n",
    "#      examples.append(example)\n",
    "#      print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "#      noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "# output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# border = 34\n",
    "\n",
    "# # Save cropped and normalized noiseprints\n",
    "# for name, res in noiseprints:\n",
    "#     if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "#         crop = res[border:-border, border:-border]\n",
    "#     else:\n",
    "#         crop = res\n",
    "\n",
    "#     # Normalize for saving\n",
    "#     vmin = np.min(crop)\n",
    "#     vmax = np.max(crop)\n",
    "#     norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "#     # Save with name\n",
    "#     filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "#     plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "# class RealFakeCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RealFakeCNN, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(128 * 16 * 16, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Pad(padding=10, fill=0),\n",
    "#     transforms.CenterCrop(128),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "# ])\n",
    "# model = RealFakeCNN()\n",
    "# def predict_image(image_path, model, class_names):\n",
    "    \n",
    "#     model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "#     image = image.to(next(model.parameters()))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         output = model(image)\n",
    "#         prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "#         predicted = int(prob > 0.7)\n",
    "\n",
    "#     return class_names[predicted], prob\n",
    "\n",
    "# folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "# class_names = ['fake','real']  # or model.class_names if stored\n",
    "# results = []\n",
    "\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "#         img_path = os.path.join(folder_path, filename)\n",
    "#         label, score = predict_image(img_path, model, class_names)\n",
    "#         results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# # Display\n",
    "# for fname, label, prob in results:\n",
    "#     print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "#     os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "# end = timer()\n",
    "\n",
    "# print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3976ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noiseprint_1.png: Predicted as fake (Confidence: 0.3247)\n",
      "noiseprint_11_forged9.png: Predicted as fake (Confidence: 0.0)\n",
      "noiseprint_12_forged2.png: Predicted as fake (Confidence: 0.0)\n",
      "noiseprint_2.png: Predicted as real (Confidence: 1.0)\n",
      "noiseprint_9_forged8.png: Predicted as fake (Confidence: 0.0)\n",
      "noiseprint_9_forged9.png: Predicted as fake (Confidence: 0.0)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "# from Noiseprint import *\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((572, 572)),   # Resize to fixed 572x572\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "def predict_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    class_names = ['fake','real']\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()).device)  # Send to same device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # shape: [1,1]\n",
    "        prob = output.item()   # probability from sigmoid\n",
    "\n",
    "    # Decide class\n",
    "    predicted_class = 1 if prob >= 0.5 else 0  # threshold at 0.5\n",
    "    return class_names[predicted_class], prob\n",
    "\n",
    "# folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\Forged_Aadhaar\\\\Balanced_Data\\\\Balanced_Data\\\\train\\\\real\"\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Noiseprint_Images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "model = torch.jit.load(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\detection_model.pt\", map_location=torch.device(\"cpu\"))\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85548811",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Forged_Aadhaar\\\\pretrained_weights\\\\model_qf101.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\u001b[39;00m\n\u001b[32m      7\u001b[39m img_path=\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mdhruv\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mKenexAI\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mProjects\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mAadhaar_X_Marksheet\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mAadhaar\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m71_forged10.png\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m example,noise_print=\u001b[43mgetNoiseprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m examples.append(example)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(img_path.split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhruv\\OneDrive\\Desktop\\KenexAI\\Projects\\Aadhaar_X_Marksheet\\streamlitapp\\Forged_Aadhaar\\Noiseprint.py:120\u001b[39m, in \u001b[36mgetNoiseprint\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m    118\u001b[39m net = FullConvNet(\u001b[32m0.9\u001b[39m, torch.tensor(\u001b[38;5;28;01mFalse\u001b[39;00m), num_levels=\u001b[32m17\u001b[39m)\n\u001b[32m    119\u001b[39m file_path=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mForged_Aadhaar\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mpretrained_weights\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mmodel_qf\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(QF)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m net.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    121\u001b[39m net.eval()\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhruv\\OneDrive\\Desktop\\KenexAI\\Projects\\Aadhaar_X_Marksheet\\streamlitapp\\Env\\Lib\\site-packages\\torch\\serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhruv\\OneDrive\\Desktop\\KenexAI\\Projects\\Aadhaar_X_Marksheet\\streamlitapp\\Env\\Lib\\site-packages\\torch\\serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhruv\\OneDrive\\Desktop\\KenexAI\\Projects\\Aadhaar_X_Marksheet\\streamlitapp\\Env\\Lib\\site-packages\\torch\\serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Forged_Aadhaar\\\\pretrained_weights\\\\model_qf101.pth'"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\71_forged10.png'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa2b80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1020x1600 large 101\n",
      "85_forged10\n",
      "noiseprint_85_forged10.png: Predicted as fake (Confidence: 0.5272)\n",
      "Inference Time: 9.8474 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\85_forged10.png'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e21bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1132x1931 large 101\n",
      "71_forged1\n",
      "noiseprint_71_forged1.png: Predicted as fake (Confidence: 0.5007)\n",
      "Inference Time: 13.2909 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\71_forged1.png'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef16c8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1132x1931 large 101\n",
      "71_forged10\n",
      "noiseprint_71_forged10.png: Predicted as fake (Confidence: 0.5008)\n",
      "Inference Time: 13.5394 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\71_forged10.png'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5862758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 381x599 small  94\n",
      "49\n",
      "noiseprint_49.png: Predicted as fake (Confidence: 0.5864)\n",
      "Inference Time: 1.2501 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\49.jpeg'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ac51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy, opencv-python\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.2.6 opencv-python-4.12.0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded successfully on CPU\n",
      "Found 55 images to process\\n\n",
      "Processing images sequentially on CPU...\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 1/55: 1.jpg\n",
      "   Noiseprint: 0.2921s | CNN: 0.0829s | Total: 0.375s\n",
      "   Prediction: real (Confidence: 0.7309)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 2/55: 11.jpg\n",
      "   Noiseprint: 0.0728s | CNN: 0.0147s | Total: 0.0875s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 3/55: 1_forged1.png\n",
      "   Noiseprint: 0.1714s | CNN: 0.023s | Total: 0.1944s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 4/55: 1_forged3.png\n",
      "   Noiseprint: 0.1812s | CNN: 0.027s | Total: 0.2082s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 5/55: 1_forged6.png\n",
      "   Noiseprint: 0.2125s | CNN: 0.0236s | Total: 0.2362s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 6/55: 1_forged8.png\n",
      "   Noiseprint: 0.2102s | CNN: 0.0277s | Total: 0.2379s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 7/55: 28.jpeg\n",
      "   Noiseprint: 0.2388s | CNN: 0.0279s | Total: 0.2667s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 8/55: 3.jpg\n",
      "   Noiseprint: 0.1325s | CNN: 0.0206s | Total: 0.1531s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 9/55: 30.jpg\n",
      "   Noiseprint: 0.0921s | CNN: 0.0199s | Total: 0.112s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 10/55: 39.jpg\n",
      "   Noiseprint: 0.0982s | CNN: 0.0176s | Total: 0.1159s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 11/55: 44.jpg\n",
      "   Noiseprint: 0.0862s | CNN: 0.015s | Total: 0.1013s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 12/55: 45.jpg\n",
      "   Noiseprint: 0.0832s | CNN: 0.0167s | Total: 0.0999s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 13/55: 49.jpeg\n",
      "   Noiseprint: 0.0534s | CNN: 0.0156s | Total: 0.069s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 14/55: 5.jpg\n",
      "   Noiseprint: 0.0895s | CNN: 0.0207s | Total: 0.1103s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 15/55: 51.jpg\n",
      "   Noiseprint: 0.1768s | CNN: 0.0257s | Total: 0.2025s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 16/55: 52.jpg\n",
      "   Noiseprint: 0.2012s | CNN: 0.0215s | Total: 0.2227s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 17/55: 52_forged1.png\n",
      "   Noiseprint: 0.1964s | CNN: 0.0268s | Total: 0.2232s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 18/55: 52_forged10.png\n",
      "   Noiseprint: 0.1972s | CNN: 0.0278s | Total: 0.2249s\n",
      "   Prediction: real (Confidence: 0.7309)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 19/55: 52_forged7.png\n",
      "   Noiseprint: 0.1751s | CNN: 0.0271s | Total: 0.2022s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 20/55: 52_forged8.png\n",
      "   Noiseprint: 0.2232s | CNN: 0.0232s | Total: 0.2463s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 21/55: 53.jpg\n",
      "   Noiseprint: 0.0915s | CNN: 0.0187s | Total: 0.1102s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 22/55: 54.jpg\n",
      "   Noiseprint: 0.1144s | CNN: 0.0192s | Total: 0.1336s\n",
      "   Prediction: real (Confidence: 0.7297)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 23/55: 58.jpg\n",
      "   Noiseprint: 0.1163s | CNN: 0.0273s | Total: 0.1436s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 24/55: 65.jpeg\n",
      "   Noiseprint: 0.2801s | CNN: 0.0424s | Total: 0.3225s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 25/55: 66.jpg\n",
      "   Noiseprint: 0.1161s | CNN: 0.0204s | Total: 0.1365s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 26/55: 71.jpeg\n",
      "   Noiseprint: 0.5625s | CNN: 0.0471s | Total: 0.6096s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 27/55: 71_forged1.png\n",
      "   Noiseprint: 0.6231s | CNN: 0.0475s | Total: 0.6706s\n",
      "   Prediction: real (Confidence: 0.7306)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 28/55: 71_forged10.png\n",
      "   Noiseprint: 0.592s | CNN: 0.0417s | Total: 0.6337s\n",
      "   Prediction: real (Confidence: 0.7309)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 29/55: 71_forged2.png\n",
      "   Noiseprint: 0.5905s | CNN: 0.0465s | Total: 0.6371s\n",
      "   Prediction: real (Confidence: 0.7309)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 30/55: 71_forged3.png\n",
      "   Noiseprint: 0.5798s | CNN: 0.0501s | Total: 0.6299s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 31/55: 71_forged4.png\n",
      "   Noiseprint: 0.5622s | CNN: 0.0414s | Total: 0.6036s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 32/55: 71_forged5.png\n",
      "   Noiseprint: 0.7232s | CNN: 0.0502s | Total: 0.7734s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 33/55: 71_forged6.png\n",
      "   Noiseprint: 0.6941s | CNN: 0.0553s | Total: 0.7494s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 34/55: 71_forged7.png\n",
      "   Noiseprint: 0.6138s | CNN: 0.0621s | Total: 0.6759s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 35/55: 71_forged8.png\n",
      "   Noiseprint: 0.5728s | CNN: 0.0487s | Total: 0.6216s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 36/55: 71_forged9.png\n",
      "   Noiseprint: 0.5694s | CNN: 0.0568s | Total: 0.6263s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 37/55: 84.jpg\n",
      "   Noiseprint: 0.0878s | CNN: 0.0194s | Total: 0.1072s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 38/55: 85.jpeg\n",
      "   Noiseprint: 0.4373s | CNN: 0.0385s | Total: 0.4758s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 39/55: 85_forged1.png\n",
      "   Noiseprint: 0.4448s | CNN: 0.0431s | Total: 0.488s\n",
      "   Prediction: real (Confidence: 0.7309)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 40/55: 85_forged10.png\n",
      "   Noiseprint: 0.4385s | CNN: 0.0393s | Total: 0.4778s\n",
      "   Prediction: real (Confidence: 0.7309)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 41/55: 85_forged2.png\n",
      "   Noiseprint: 0.5007s | CNN: 0.0366s | Total: 0.5373s\n",
      "   Prediction: real (Confidence: 0.7306)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 42/55: 85_forged3.png\n",
      "   Noiseprint: 0.4757s | CNN: 0.0314s | Total: 0.5071s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 43/55: 85_forged4.png\n",
      "   Noiseprint: 0.4653s | CNN: 0.0486s | Total: 0.5139s\n",
      "   Prediction: real (Confidence: 0.7307)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 44/55: 85_forged5.png\n",
      "   Noiseprint: 0.4421s | CNN: 0.0469s | Total: 0.489s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 45/55: 85_forged6.png\n",
      "   Noiseprint: 0.4671s | CNN: 0.0638s | Total: 0.5308s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 46/55: 85_forged7.png\n",
      "   Noiseprint: 0.4814s | CNN: 0.0649s | Total: 0.5463s\n",
      "   Prediction: real (Confidence: 0.7308)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 47/55: 85_forged8.png\n",
      "   Noiseprint: 0.4583s | CNN: 0.0352s | Total: 0.4935s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 48/55: 85_forged9.png\n",
      "   Noiseprint: 0.4396s | CNN: 0.0389s | Total: 0.4785s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 49/55: 91.jpg\n",
      "   Noiseprint: 0.0797s | CNN: 0.0184s | Total: 0.0981s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 50/55: 94.jpg\n",
      "   Noiseprint: 0.1494s | CNN: 0.0228s | Total: 0.1723s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 51/55: 95.jpg\n",
      "   Noiseprint: 0.1632s | CNN: 0.0302s | Total: 0.1934s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 52/55: 96.jpg\n",
      "   Noiseprint: 0.1777s | CNN: 0.0235s | Total: 0.2012s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 53/55: 96_forged1.png\n",
      "   Noiseprint: 0.1964s | CNN: 0.0266s | Total: 0.2229s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 54/55: 97.jpg\n",
      "   Noiseprint: 0.1579s | CNN: 0.0278s | Total: 0.1857s\n",
      "   Prediction: real (Confidence: 0.731)\n",
      "--------------------------------------------------------------------------------\n",
      "Processing image 55/55: dhruvil.png\n",
      "   Noiseprint: 0.0636s | CNN: 0.0132s | Total: 0.0768s\n",
      "   Prediction: real (Confidence: 0.7311)\n",
      "--------------------------------------------------------------------------------\n",
      "\\nCleaned up temporary directory: C:\\Users\\dhruv\\OneDrive\\Desktop\\KenexAI\\Projects\\Aadhaar_X_Marksheet\\Forged_Aadhaar\\Forged_images\n",
      "\\n================================================================================\n",
      "PROCESSING SUMMARY (CPU)\n",
      "================================================================================\n",
      "Successfully processed: 55/55 images\n",
      "Failed to process: 0 images\n",
      "\n",
      "TIMING STATISTICS:\n",
      "  Total noiseprint time: 16.7123s\n",
      "  Total CNN time: 1.8495s\n",
      "  Average noiseprint time per image: 0.3039s\n",
      "  Average CNN time per image: 0.0336s\n",
      "  Average total time per image: 0.3375s\n",
      "  Overall execution time: 19.1234s\n",
      "\n",
      "PREDICTION SUMMARY:\n",
      "  Images classified as FAKE: 0\n",
      "  Images classified as REAL: 55\n",
      "\n",
      "DETAILED RESULTS:\n",
      "  1: real (Conf: 0.7309, Noiseprint: 0.2921s, CNN: 0.0829s, Total: 0.375s)\n",
      "  11: real (Conf: 0.7311, Noiseprint: 0.0728s, CNN: 0.0147s, Total: 0.0875s)\n",
      "  1_forged1: real (Conf: 0.731, Noiseprint: 0.1714s, CNN: 0.023s, Total: 0.1944s)\n",
      "  1_forged3: real (Conf: 0.731, Noiseprint: 0.1812s, CNN: 0.027s, Total: 0.2082s)\n",
      "  1_forged6: real (Conf: 0.7311, Noiseprint: 0.2125s, CNN: 0.0236s, Total: 0.2362s)\n",
      "  1_forged8: real (Conf: 0.731, Noiseprint: 0.2102s, CNN: 0.0277s, Total: 0.2379s)\n",
      "  28: real (Conf: 0.731, Noiseprint: 0.2388s, CNN: 0.0279s, Total: 0.2667s)\n",
      "  3: real (Conf: 0.7311, Noiseprint: 0.1325s, CNN: 0.0206s, Total: 0.1531s)\n",
      "  30: real (Conf: 0.731, Noiseprint: 0.0921s, CNN: 0.0199s, Total: 0.112s)\n",
      "  39: real (Conf: 0.731, Noiseprint: 0.0982s, CNN: 0.0176s, Total: 0.1159s)\n",
      "  44: real (Conf: 0.7311, Noiseprint: 0.0862s, CNN: 0.015s, Total: 0.1013s)\n",
      "  45: real (Conf: 0.731, Noiseprint: 0.0832s, CNN: 0.0167s, Total: 0.0999s)\n",
      "  49: real (Conf: 0.731, Noiseprint: 0.0534s, CNN: 0.0156s, Total: 0.069s)\n",
      "  5: real (Conf: 0.731, Noiseprint: 0.0895s, CNN: 0.0207s, Total: 0.1103s)\n",
      "  51: real (Conf: 0.731, Noiseprint: 0.1768s, CNN: 0.0257s, Total: 0.2025s)\n",
      "  52: real (Conf: 0.731, Noiseprint: 0.2012s, CNN: 0.0215s, Total: 0.2227s)\n",
      "  52_forged1: real (Conf: 0.7311, Noiseprint: 0.1964s, CNN: 0.0268s, Total: 0.2232s)\n",
      "  52_forged10: real (Conf: 0.7309, Noiseprint: 0.1972s, CNN: 0.0278s, Total: 0.2249s)\n",
      "  52_forged7: real (Conf: 0.7311, Noiseprint: 0.1751s, CNN: 0.0271s, Total: 0.2022s)\n",
      "  52_forged8: real (Conf: 0.731, Noiseprint: 0.2232s, CNN: 0.0232s, Total: 0.2463s)\n",
      "  53: real (Conf: 0.7311, Noiseprint: 0.0915s, CNN: 0.0187s, Total: 0.1102s)\n",
      "  54: real (Conf: 0.7297, Noiseprint: 0.1144s, CNN: 0.0192s, Total: 0.1336s)\n",
      "  58: real (Conf: 0.731, Noiseprint: 0.1163s, CNN: 0.0273s, Total: 0.1436s)\n",
      "  65: real (Conf: 0.731, Noiseprint: 0.2801s, CNN: 0.0424s, Total: 0.3225s)\n",
      "  66: real (Conf: 0.7311, Noiseprint: 0.1161s, CNN: 0.0204s, Total: 0.1365s)\n",
      "  71: real (Conf: 0.731, Noiseprint: 0.5625s, CNN: 0.0471s, Total: 0.6096s)\n",
      "  71_forged1: real (Conf: 0.7306, Noiseprint: 0.6231s, CNN: 0.0475s, Total: 0.6706s)\n",
      "  71_forged10: real (Conf: 0.7309, Noiseprint: 0.592s, CNN: 0.0417s, Total: 0.6337s)\n",
      "  71_forged2: real (Conf: 0.7309, Noiseprint: 0.5905s, CNN: 0.0465s, Total: 0.6371s)\n",
      "  71_forged3: real (Conf: 0.731, Noiseprint: 0.5798s, CNN: 0.0501s, Total: 0.6299s)\n",
      "  71_forged4: real (Conf: 0.731, Noiseprint: 0.5622s, CNN: 0.0414s, Total: 0.6036s)\n",
      "  71_forged5: real (Conf: 0.731, Noiseprint: 0.7232s, CNN: 0.0502s, Total: 0.7734s)\n",
      "  71_forged6: real (Conf: 0.731, Noiseprint: 0.6941s, CNN: 0.0553s, Total: 0.7494s)\n",
      "  71_forged7: real (Conf: 0.731, Noiseprint: 0.6138s, CNN: 0.0621s, Total: 0.6759s)\n",
      "  71_forged8: real (Conf: 0.731, Noiseprint: 0.5728s, CNN: 0.0487s, Total: 0.6216s)\n",
      "  71_forged9: real (Conf: 0.731, Noiseprint: 0.5694s, CNN: 0.0568s, Total: 0.6263s)\n",
      "  84: real (Conf: 0.7311, Noiseprint: 0.0878s, CNN: 0.0194s, Total: 0.1072s)\n",
      "  85: real (Conf: 0.731, Noiseprint: 0.4373s, CNN: 0.0385s, Total: 0.4758s)\n",
      "  85_forged1: real (Conf: 0.7309, Noiseprint: 0.4448s, CNN: 0.0431s, Total: 0.488s)\n",
      "  85_forged10: real (Conf: 0.7309, Noiseprint: 0.4385s, CNN: 0.0393s, Total: 0.4778s)\n",
      "  85_forged2: real (Conf: 0.7306, Noiseprint: 0.5007s, CNN: 0.0366s, Total: 0.5373s)\n",
      "  85_forged3: real (Conf: 0.731, Noiseprint: 0.4757s, CNN: 0.0314s, Total: 0.5071s)\n",
      "  85_forged4: real (Conf: 0.7307, Noiseprint: 0.4653s, CNN: 0.0486s, Total: 0.5139s)\n",
      "  85_forged5: real (Conf: 0.731, Noiseprint: 0.4421s, CNN: 0.0469s, Total: 0.489s)\n",
      "  85_forged6: real (Conf: 0.731, Noiseprint: 0.4671s, CNN: 0.0638s, Total: 0.5308s)\n",
      "  85_forged7: real (Conf: 0.7308, Noiseprint: 0.4814s, CNN: 0.0649s, Total: 0.5463s)\n",
      "  85_forged8: real (Conf: 0.731, Noiseprint: 0.4583s, CNN: 0.0352s, Total: 0.4935s)\n",
      "  85_forged9: real (Conf: 0.731, Noiseprint: 0.4396s, CNN: 0.0389s, Total: 0.4785s)\n",
      "  91: real (Conf: 0.731, Noiseprint: 0.0797s, CNN: 0.0184s, Total: 0.0981s)\n",
      "  94: real (Conf: 0.7311, Noiseprint: 0.1494s, CNN: 0.0228s, Total: 0.1723s)\n",
      "  95: real (Conf: 0.731, Noiseprint: 0.1632s, CNN: 0.0302s, Total: 0.1934s)\n",
      "  96: real (Conf: 0.731, Noiseprint: 0.1777s, CNN: 0.0235s, Total: 0.2012s)\n",
      "  96_forged1: real (Conf: 0.731, Noiseprint: 0.1964s, CNN: 0.0266s, Total: 0.2229s)\n",
      "  97: real (Conf: 0.731, Noiseprint: 0.1579s, CNN: 0.0278s, Total: 0.1857s)\n",
      "  dhruvil: real (Conf: 0.7311, Noiseprint: 0.0636s, CNN: 0.0132s, Total: 0.0768s)\n",
      "================================================================================\n",
      "\\nTotal Inference Time: 19.1234 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install opencv-python\n",
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# from timeit import default_timer as timer\n",
    "# import cv2\n",
    "\n",
    "# # ------------------ Noiseprint Extraction ------------------ #\n",
    "# def getNoiseprint(img_path):\n",
    "#     def extract_noiseprint(image_array):\n",
    "#         height, width = image_array.shape[:2]\n",
    "#         noise = np.random.randn(height, width).astype(np.float32) * 20\n",
    "#         return noise\n",
    "\n",
    "#     img = cv2.imread(img_path)\n",
    "#     if img is None:\n",
    "#         raise ValueError(f\"Image at {img_path} could not be loaded.\")\n",
    "\n",
    "#     grayscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     noiseprint = extract_noiseprint(grayscale)\n",
    "#     return img, noiseprint\n",
    "\n",
    "# def save_noiseprint(noiseprint, name, output_dir):\n",
    "#     \"\"\"Save noiseprint as image file\"\"\"\n",
    "#     border = 34\n",
    "#     if noiseprint.shape[0] > 2 * border and noiseprint.shape[1] > 2 * border:\n",
    "#         crop = noiseprint[border:-border, border:-border]\n",
    "#     else:\n",
    "#         crop = noiseprint\n",
    "\n",
    "#     vmin = np.min(crop)\n",
    "#     vmax = np.max(crop)\n",
    "#     norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "#     filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "#     plt.imsave(filename, norm_crop, cmap='gray')\n",
    "#     return filename\n",
    "\n",
    "# # ------------------ CNN Model Definition ------------------ #\n",
    "# class RealFakeCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RealFakeCNN, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(128 * 16 * 16, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ Prediction Function ------------------ #\n",
    "def predict_image(image_path, model, class_names, transform):\n",
    "    \"\"\"Predict single image without reloading model weights\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "# ------------------ Sequential Processing Function ------------------ #\n",
    "def process_single_image(img_path, model, class_names, transform, output_dir):\n",
    "    \"\"\"Process a single image: noiseprint extraction -> CNN prediction\"\"\"\n",
    "    image_name = img_path.split('\\\\')[-1].split('.')[0]\n",
    "    \n",
    "    # Time noiseprint extraction\n",
    "    noiseprint_start = timer()\n",
    "    try:\n",
    "        example, noiseprint = getNoiseprint(img_path)\n",
    "        noiseprint_saved_path = save_noiseprint(noiseprint, image_name, output_dir)\n",
    "        noiseprint_time = timer() - noiseprint_start\n",
    "    except Exception as e:\n",
    "        print(f\"Error during noiseprint extraction for {image_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Time CNN prediction\n",
    "    cnn_start = timer()\n",
    "    try:\n",
    "        label, score = predict_image(noiseprint_saved_path, model, class_names, transform)\n",
    "        cnn_time = timer() - cnn_start\n",
    "    except Exception as e:\n",
    "        print(f\"Error during CNN prediction for {image_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    total_time = noiseprint_time + cnn_time\n",
    "    \n",
    "    return {\n",
    "        'image_name': image_name,\n",
    "        'label': label,\n",
    "        'confidence': round(score, 4),\n",
    "        'noiseprint_time': round(noiseprint_time, 4),\n",
    "        'cnn_time': round(cnn_time, 4),\n",
    "        'total_time': round(total_time, 4)\n",
    "    }\n",
    "\n",
    "# ------------------ Main Sequential Processing Pipeline ------------------ #\n",
    "if __name__ == \"__main__\":\n",
    "    overall_start = timer()\n",
    "    \n",
    "    # Force CPU usage\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Setup directories\n",
    "    output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Setup transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Pad(padding=10, fill=0),\n",
    "        transforms.CenterCrop(128),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    # Load model once and keep it loaded\n",
    "    class_names = ['fake', 'real']\n",
    "    # model = RealFakeCNN().to(device)\n",
    "    \n",
    "    try:\n",
    "        # model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully on CPU\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Get all image paths\n",
    "    image_paths = glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*')\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"No images found in the specified directory\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images to process\\\\n\")\n",
    "\n",
    "    # Process images sequentially\n",
    "    results = []\n",
    "    total_noiseprint_time = 0\n",
    "    total_cnn_time = 0\n",
    "    \n",
    "    print(\"Processing images sequentially on CPU...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, img_path in enumerate(image_paths, 1):\n",
    "        print(f\"Processing image {i}/{len(image_paths)}: {os.path.basename(img_path)}\")\n",
    "        \n",
    "        result = process_single_image(img_path, model, class_names, transform, output_dir)\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "            total_noiseprint_time += result['noiseprint_time']\n",
    "            total_cnn_time += result['cnn_time']\n",
    "            \n",
    "            print(f\"   Noiseprint: {result['noiseprint_time']}s | CNN: {result['cnn_time']}s | Total: {result['total_time']}s\")\n",
    "            print(f\"   Prediction: {result['label']} (Confidence: {result['confidence']})\")\n",
    "        else:\n",
    "            print(f\"   Failed to process\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    # Clean up temporary files\n",
    "    try:\n",
    "        import shutil\n",
    "        if os.path.exists(output_dir):\n",
    "            shutil.rmtree(output_dir)\n",
    "            print(f\"\\\\nCleaned up temporary directory: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not clean up directory {output_dir}: {e}\")\n",
    "\n",
    "    # Summary\n",
    "    overall_end = timer()\n",
    "    overall_time = overall_end - overall_start\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"PROCESSING SUMMARY (CPU)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"Successfully processed: {len(results)}/{len(image_paths)} images\")\n",
    "        print(f\"Failed to process: {len(image_paths) - len(results)} images\")\n",
    "        print()\n",
    "        \n",
    "        # Time statistics\n",
    "        avg_noiseprint_time = total_noiseprint_time / len(results)\n",
    "        avg_cnn_time = total_cnn_time / len(results)\n",
    "        avg_total_time = (total_noiseprint_time + total_cnn_time) / len(results)\n",
    "        \n",
    "        print(\"TIMING STATISTICS:\")\n",
    "        print(f\"  Total noiseprint time: {total_noiseprint_time:.4f}s\")\n",
    "        print(f\"  Total CNN time: {total_cnn_time:.4f}s\")\n",
    "        print(f\"  Average noiseprint time per image: {avg_noiseprint_time:.4f}s\")\n",
    "        print(f\"  Average CNN time per image: {avg_cnn_time:.4f}s\")\n",
    "        print(f\"  Average total time per image: {avg_total_time:.4f}s\")\n",
    "        print(f\"  Overall execution time: {overall_time:.4f}s\")\n",
    "        print()\n",
    "        \n",
    "        # Prediction summary\n",
    "        fake_count = sum(1 for r in results if r['label'] == 'fake')\n",
    "        real_count = sum(1 for r in results if r['label'] == 'real')\n",
    "        \n",
    "        print(\"PREDICTION SUMMARY:\")\n",
    "        print(f\"  Images classified as FAKE: {fake_count}\")\n",
    "        print(f\"  Images classified as REAL: {real_count}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"DETAILED RESULTS:\")\n",
    "        for result in results:\n",
    "            print(f\"  {result['image_name']}: {result['label']} \"\n",
    "                  f\"(Conf: {result['confidence']}, \"\n",
    "                  f\"Noiseprint: {result['noiseprint_time']}s, \"\n",
    "                  f\"CNN: {result['cnn_time']}s, \"\n",
    "                  f\"Total: {result['total_time']}s)\")\n",
    "    else:\n",
    "        print(\"No images were successfully processed!\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\\\nTotal Inference Time: {overall_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7bd6b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 800x800 small 101\n",
      "52_forged7\n",
      "noiseprint_52_forged7.png: Predicted as fake (Confidence: 0.5)\n",
      "noiseprint_91.png: Predicted as real (Confidence: 0.731)\n",
      "Inference Time: 6.0534 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "examples=[]\n",
    "noiseprints=[]\n",
    "\n",
    "# for img_path in glob.glob('C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\*'):\n",
    "img_path='C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Aadhaar\\\\52_forged7.png'\n",
    "example,noise_print=getNoiseprint(img_path)\n",
    "examples.append(example)\n",
    "print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "border = 34\n",
    "\n",
    "# Save cropped and normalized noiseprints\n",
    "for name, res in noiseprints:\n",
    "    if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "        crop = res[border:-border, border:-border]\n",
    "    else:\n",
    "        crop = res\n",
    "\n",
    "    # Normalize for saving\n",
    "    vmin = np.min(crop)\n",
    "    vmax = np.max(crop)\n",
    "    norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    # Save with name\n",
    "    filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "    plt.imsave(filename, norm_crop, cmap='gray')\n",
    "\n",
    "class RealFakeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFakeCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(padding=10, fill=0),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "model = RealFakeCNN()\n",
    "def predict_image(image_path, model, class_names):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob = torch.sigmoid(output).item()  # Apply sigmoid manually\n",
    "        predicted = int(prob > 0.7)\n",
    "\n",
    "    return class_names[predicted], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")\n",
    "    # os.system(f'rmdir /S /Q \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\Forged_Aadhaar\\\\Forged_images\"')\n",
    "\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f\"Inference Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db221781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noiseprint_1_orig.png: Predicted as fake (Confidence: 0.3247)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((572, 572)),   # Resize to fixed 572x572\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "def predict_image(image_path, model, class_names):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(next(model.parameters()).device)  # Send to same device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # shape: [1,1]\n",
    "        prob = output.item()   # probability from sigmoid\n",
    "\n",
    "    # Decide class\n",
    "    predicted_class = 1 if prob >= 0.5 else 0  # threshold at 0.5\n",
    "    return class_names[predicted_class], prob\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\KenexAI\\\\Projects\\\\Aadhaar_X_Marksheet\\\\streamlitapp\\\\tempdir\\\\new_temp\"\n",
    "class_names = ['fake','real']  # or model.class_names if stored\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        label, score = predict_image(img_path, model, class_names)\n",
    "        results.append((filename, label, round(score, 4)))\n",
    "\n",
    "# Display\n",
    "for fname, label, prob in results:\n",
    "    print(f\"{fname}: Predicted as {label} (Confidence: {prob})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55cd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_num = int(\"6979414848205548481619299442879901900893978332594614407044767717485407280104077714658698163325401659212830920734233047578454701810567032015270223682917915825234703754712504887921309181789607809168884583848396456653007022479356336240198130363930881632367124738541517499494458139647378808680614169273221404741476596583953169248831376224396335169577064812987140578144885819479190173537644970232125142253963784979138011318798385442436099901621998283624816070080504830712594525760596934341576755626791590403636878139861665599383319429228364434183913197958738697001410493839281298692342829951566712530309758759364649701153639921979798429707566199261950037418171329283207372048014948669160666776198414040633384677104717697507521717586776709084200364956178863636105988867260929887577092955570407803783021397897341999914616790441029837229129746669225095633201097644321593502503404440714110515167034889128258965583435965030225845348564582051521348800742574442877087774194668983516629631073341202705453382780613775427336949283388084891654484225446940941660942440637784744293259916479841407088189462964489670231866481904237338494872813098890875845640034370370387108798950180220865436012752487216677041817312930119747601017807577565413977545693375480131324240696099879479436722576566447939593195590684591261809038023122178172006150499569185218838749337238281597037288924464009997530938336798176023597292328320965086990184531426188862965408313308973495924965144113396593829090645266653313774582036138982013368561474719154447134894466611560589758251829063226370300282175823479569847261439348404558251402273730865053482214589180028302043821438357583302818374143973997002745047526405755760407045006694423501337081780299815080324840337828812644300041900356816429114261098230198976752026002079876882796597235615015594486182057781476152918170746403157005216896239428521706033466061587608065036133153074432195952131368564234168005447770190345777024917629879639171161719929852078265309160759260989590618158889891835294735614366674503961584445497685736312628248483551986529867423016255476553691922054241686230968975229511700928171281549902682365302333677412951788839806869796040512235899311734337858684531156721416280114473368826463098485252394260075790386415875290922570568686439586036262465414002334117870088922801660529414759784318799843806130096998190881240404138869293309782335305296720666220243304175086358278211355789957998014801209332293458940463859106591986434520433810583569309224929264228263841477378949329312443958215939294432669464260216534074560882723006838459792812340253078330291135526952675203790833430237852831740601433198364243363569730205351077393441691141240055900819091229931605146865520183001810239708464322588389956036291760175558843819105418234580239610174323636606095262722940143706063698846499673285377621180570537788160304936809915237889489342387891057012783726694920184573202789672963922380028271124448024265644396686341508447830351380242127542393849410283830409594988503246799544444687606954881510597515686410993828907588979699141180160893062603338104857903239845856783130275935413569275439908789983311663211937449259444259898972766208\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe733ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_array = qr_num.to_bytes(\n",
    "    (qr_num.bit_length() + 7) // 8,\n",
    "    byteorder=\"big\",\n",
    "    signed=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcb427cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement gzip (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for gzip\n"
     ]
    }
   ],
   "source": [
    "!pip install gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b42a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from io import BytesIO\n",
    "\n",
    "# byte_array contains the gzip-compressed data\n",
    "with gzip.GzipFile(fileobj=BytesIO(byte_array)) as f:\n",
    "    decompressed_bytes = f.read()\n",
    "\n",
    "# Now you can process the decompressed bytes\n",
    "end_index = decompressed_bytes.index(255)\n",
    "value_bytes = decompressed_bytes[:end_index]\n",
    "\n",
    "# Get the indicator as integer (0,1,2,3)\n",
    "Email_mobile_present_bit_indicator_value = value_bytes[0] & 0b11\n",
    "\n",
    "print(Email_mobile_present_bit_indicator_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3517a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890820190305150137123\n"
     ]
    }
   ],
   "source": [
    "# Assume decompressed_bytes is your decompressed byte array\n",
    "\n",
    "# 1 Find the first delimiter\n",
    "first_delim_index = decompressed_bytes.index(255)\n",
    "\n",
    "# 2 Extract first field (already done)\n",
    "first_field_bytes = decompressed_bytes[:first_delim_index]\n",
    "Email_mobile_present_bit_indicator_value = first_field_bytes[0] & 0b11  # 0-3\n",
    "\n",
    "# 3 Find the second field (next segment)\n",
    "# Start from the next index after the first delimiter\n",
    "start_index = first_delim_index + 1\n",
    "\n",
    "# Find the next delimiter 255 from start_index\n",
    "second_delim_index = decompressed_bytes.index(255, start_index)\n",
    "\n",
    "# Extract second field bytes\n",
    "second_field_bytes = decompressed_bytes[start_index:second_delim_index]\n",
    "\n",
    "# Convert to string if required (ISO-8859-1)\n",
    "second_field_value = second_field_bytes.decode(\"ISO-8859-1\")\n",
    "\n",
    "print(second_field_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90f78cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "doc = fitz.open(\"EAadhaar_0124314111220120220707163620_23052025201834.pdf\")\n",
    "\n",
    "if doc.needs_pass:\n",
    "    doc.authenticate(\"JOSH2004\")\n",
    "\n",
    "page = doc[0]\n",
    "\n",
    "# for x in range(0, int(page.rect.width), 50):\n",
    "#     page.draw_line((x, 0), (x, page.rect.height))\n",
    "\n",
    "# for y in range(0, int(page.rect.height), 50):\n",
    "#     page.draw_line((0, y), (page.rect.width, y))\n",
    "\n",
    "doc.save(\"grid_overlay.pdf\")\n",
    "\n",
    "qr_rect = fitz.Rect(\n",
    "    470, 610,    # x0, y0\n",
    "    550, 690    # x1, y1\n",
    ")\n",
    "\n",
    "pix = page.get_pixmap(clip=qr_rect, dpi=400)\n",
    "pix.save(\"qr.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2a681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields extracted up to VTC: [2, '2', '584720250523201830889', 'Joshi Dhruvil Sandipkumar', '19-10-2004', 'M', '', 'Kheda', 'Pij Road', 'A-29', 'Opp Ramev Mandir', '387002', 'Patel Society Area Tso Nad', 'Gujarat', 'Trishul Society', 'Nadiad', 'Nadiad', 'XXXXXX1401']\n",
      "VTC value: XXXXXX1401\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from io import BytesIO\n",
    "\n",
    "# bigint_str = int(\"6979414848205548481619299442879901900893978332594614407044767717485407280104077714658698163325401659212830920734233047578454701810567032015270223682917915825234703754712504887921309181789607809168884583848396456653007022479356336240198130363930881632367124738541517499494458139647378808680614169273221404741476596583953169248831376224396335169577064812987140578144885819479190173537644970232125142253963784979138011318798385442436099901621998283624816070080504830712594525760596934341576755626791590403636878139861665599383319429228364434183913197958738697001410493839281298692342829951566712530309758759364649701153639921979798429707566199261950037418171329283207372048014948669160666776198414040633384677104717697507521717586776709084200364956178863636105988867260929887577092955570407803783021397897341999914616790441029837229129746669225095633201097644321593502503404440714110515167034889128258965583435965030225845348564582051521348800742574442877087774194668983516629631073341202705453382780613775427336949283388084891654484225446940941660942440637784744293259916479841407088189462964489670231866481904237338494872813098890875845640034370370387108798950180220865436012752487216677041817312930119747601017807577565413977545693375480131324240696099879479436722576566447939593195590684591261809038023122178172006150499569185218838749337238281597037288924464009997530938336798176023597292328320965086990184531426188862965408313308973495924965144113396593829090645266653313774582036138982013368561474719154447134894466611560589758251829063226370300282175823479569847261439348404558251402273730865053482214589180028302043821438357583302818374143973997002745047526405755760407045006694423501337081780299815080324840337828812644300041900356816429114261098230198976752026002079876882796597235615015594486182057781476152918170746403157005216896239428521706033466061587608065036133153074432195952131368564234168005447770190345777024917629879639171161719929852078265309160759260989590618158889891835294735614366674503961584445497685736312628248483551986529867423016255476553691922054241686230968975229511700928171281549902682365302333677412951788839806869796040512235899311734337858684531156721416280114473368826463098485252394260075790386415875290922570568686439586036262465414002334117870088922801660529414759784318799843806130096998190881240404138869293309782335305296720666220243304175086358278211355789957998014801209332293458940463859106591986434520433810583569309224929264228263841477378949329312443958215939294432669464260216534074560882723006838459792812340253078330291135526952675203790833430237852831740601433198364243363569730205351077393441691141240055900819091229931605146865520183001810239708464322588389956036291760175558843819105418234580239610174323636606095262722940143706063698846499673285377621180570537788160304936809915237889489342387891057012783726694920184573202789672963922380028271124448024265644396686341508447830351380242127542393849410283830409594988503246799544444687606954881510597515686410993828907588979699141180160893062603338104857903239845856783130275935413569275439908789983311663211937449259444259898972766208\")\n",
    "# bigint_str = int(\"1011253456373559996825775946156673208806218874773777002636410696573689858368855101030525940264639310330984319879826413048482326461605177443122012341323374405169067276171649032009896329787302657256744819245733598246872579611273232103490186905047155735349255046700055714481845656375094480364885674395294657582236418733145942843607320312035180776657906110577656290551807293091818684488498528529821885788340352532299861918349499181216689010832548298274897819284506558961506420617513722243130896081282473951995532256761504765072420912146389301840084936082246213642567113919454510928594727455348217562061238797133699562492753682799582554244985657361980170477536111130918490604316845232156505834420763670409620288228825101945037028272465431754760674639715026207000203562984776269739277005216990700073094407095029215123813619244282064355089449969527356853336781651483166483422371677629769273727663497561674349126591030495610533934205243787150434321380583114852456566278088671049832422108122945065114962303440901403059713183809001340166646345510141151706195500347487593368641940453698477605885790606890758245827030484449689922325740329613038528061351424696836604538813729433479232389381250843825750748855270206568587378590939841400511645473265987746897212463087290739173996658961443731463367943832612568964936816836032024326329911925570884901918107566456736041814711609046368211410039985611159032517299044901199589369945077306825923951759467404276802634597893652342574646622703752914076921973899523891929148047821548867569965082511688531678338269324675555136964861966286934917790805057312465716377099095604276059231156938641000895750179279172465732529675908641256806557036450079083131221518178380567996252964519232363931737950568833336694330543049152276251484882164630846502786018916882581760840453146516105000915125900341109543031928220881939782845113459461104663151470206773878340570702101567909777608259422558167907492584823497270113878099304424103148536099378745445673574702686588613643720367459670911066804381792377041415758270639762404323655341090662545019160743615488147271148004032238808475930405647609952430141921347389914487527204615835357365739342031343435369639500351103920158671742298713692250745632408362541173292449902069874248171651982232144215514044815558339957328703348848181364834202991781993312970816073082184177579192635360725650812692362012913313089757121862267015013449592571833300557947730896643433805285183370135915597674455300339409125649253721161933879300829258273166494595613900530019862649534528991994874835909272047197797417424704643923064699342629931503526858926069922207449703407452538921185123223729981797105572698408183737252170917989995285428631559784493279998335926547833510211372505698148427685248606725926915566674404344202890992903174955971858696563416488190576182471746840449298336749975602478976393532209486257509505538559991596669444727019787685232920566217724105845346651445085885288401002934571098447796636724556835145881145790404744982667869374424181237332163762087479349607592366595789892179065363921362428681949762271891986012352872448\")\n",
    "bigint_str = 1011253456373559996825775946156673208806218874773777002636410696573689858368855101030525940264639310330984319879826413048482326461605177443122012341323374405169067276171649032009896329787302657256744819245733598246872579611273232103490186905047155735349255046700055714481845656375094480364885674395294657582236418733145942843607320312035180776657906110577656290551807293091818684488498528529821885788340352532299861918349499181216689010832548298274897819284506558961506420617513722243130896081282473951995532256761504765072420912146389301840084936082246213642567113919454510928594727455348217562061238797133699562492753682799582554244985657361980170477536111130918490604316845232156505834420763670409620288228825101945037028272465431754760674639715026207000203562984776269739277005216990700073094407095029215123813619244282064355089449969527356853336781651483166483422371677629769273727663497561674349126591030495610533934205243787150434321380583114852456566278088671049832422108122945065114962303440901403059713183809001340166646345510141151706195500347487593368641940453698477605885790606890758245827030484449689922325740329613038528061351424696836604538813729433479232389381250843825750748855270206568587378590939841400511645473265987746897212463087290739173996658961443731463367943832612568964936816836032024326329911925570884901918107566456736041814711609046368211410039985611159032517299044901199589369945077306825923951759467404276802634597893652342574646622703752914076921973899523891929148047821548867569965082511688531678338269324675555136964861966286934917790805057312465716377099095604276059231156938641000895750179279172465732529675908641256806557036450079083131221518178380567996252964519232363931737950568833336694330543049152276251484882164630846502786018916882581760840453146516105000915125900341109543031928220881939782845113459461104663151470206773878340570702101567909777608259422558167907492584823497270113878099304424103148536099378745445673574702686588613643720367459670911066804381792377041415758270639762404323655341090662545019160743615488147271148004032238808475930405647609952430141921347389914487527204615835357365739342031343435369639500351103920158671742298713692250745632408362541173292449902069874248171651982232144215514044815558339957328703348848181364834202991781993312970816073082184177579192635360725650812692362012913313089757121862267015013449592571833300557947730896643433805285183370135915597674455300339409125649253721161933879300829258273166494595613900530019862649534528991994874835909272047197797417424704643923064699342629931503526858926069922207449703407452538921185123223729981797105572698408183737252170917989995285428631559784493279998335926547833510211372505698148427685248606725926915566674404344202890992903174955971858696563416488190576182471746840449298336749975602478976393532209486257509505538559991596669444727019787685232920566217724105845346651445085885288401002934571098447796636724556835145881145790404744982667869374424181237332163762087479349607592366595789892179065363921362428681949762271891986012352872448\n",
    "# bigint_str = 2374971804270526477833002468783965837992554564899874087591661303561346432389832047870524302186901344489362368642972767716416349990805756094923115719687656090691368051627957878187788907419297818953295185555346288172578594637886352753543271000481717080003254556962148594350559820352806251787713278744047402230989238559317351232114240089849934148895256488140236015024800731753594740948640957680138566468247224859669467819596919398964809164399637893729212452791889199675715949918925838319591794702333094022248132120531152523331442741730158840977243402215102904932650832502847295644794421419704633765033761284508863534321317394686768650111457751139630853448637215423705157211510636160227953566227527799608082928846103264491539001327407775670834868948113753614112563650255058316849200536533335903554984254814901522086937767458409075617572843449110393213525925388131214952874629655799772119820372255291052673056372346072235458198199995637720424196884145247220163810790179386390283738429482893152518286247124911446073389185062482901364671389605727763080854673156754021728522287806275420847159574631844674460263574901590412679291518508010087116598357407343835408554094619585212373168435612645646129147973594416508676872819776522537778717985070402222824965034768103900739105784663244748432502180989441389718131079445941981681118258324511923246198334046020123727749408128519721102477302359413240175102907322619462289965085963377744024233678337951462006962521823224880199210318367946130004264196899778609815012001799773327514133268825910089483612283510244566484854597156100473055413090101948456959122378865704840756793122956663218517626099291311352417342899623681483097817511136427210593032393600010728324905512596767095096153856032112835755780472808814199620390836980020899858288860556611564167406292139646289142056168261133256777093245980048335918156712295254776487472431445495668303900536289283098315798552328294391152828182614909451410115516297083658174657554955228963550255866282688308751041517464999930825273776417639569977754844191402927594739069037851707477839207593911886893016618794870530622356073909077832279869798641545167528509966656120623184120128052588408742941658045827255866966100249857968956536613250770326334844204927432961924987891433020671754710428050564671868464658436926086493709176888821257183419013229795869757265111599482263223604228286513011751601176504567030118257385997460972803240338899836840030438830725520798480181575861397469056536579877274090338750406459700907704031830137890544492015701251066934352867527112361743047684237105216779177819594030160887368311805926405114938744235859610328064947158936962470654636736991567663705830950312548447653861922078087824048793236971354828540758657075837209006713701763902429652486225300535997260665898927924843608750347193892239342462507130025307878412116604096773706728162016134101751551184021079984480254041743057914746472840768175369369852937574401874295943063507273467384747124843744395375119899278823903202010381949145094804675442110869084589592876721655764753871572233276245590041302887094585204427900634246823674277680009401177473636685542700515621164233992970974893989913447733956146698563285998205950467321954304\n",
    "bigint_value = int(bigint_str)\n",
    "\n",
    "# -----------------------------\n",
    "# 2 Convert BigInteger to byte array\n",
    "# -----------------------------\n",
    "byte_array = bigint_value.to_bytes((bigint_value.bit_length() + 7) // 8, byteorder=\"big\", signed=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 3 GZIP decompress\n",
    "# -----------------------------\n",
    "with gzip.GzipFile(fileobj=BytesIO(byte_array)) as f:\n",
    "    decompressed_bytes = f.read()\n",
    "\n",
    "# -----------------------------\n",
    "# 4 Parse fields separated by delimiter 255\n",
    "# -----------------------------\n",
    "DELIM = 255\n",
    "fields = []\n",
    "start_index = 0\n",
    "\n",
    "# Replace with the correct index of VTC field in your sequence\n",
    "FIELD_INDEX_VTC = 17  # example, change as per actual sequence\n",
    "\n",
    "while start_index < len(decompressed_bytes):\n",
    "    try:\n",
    "        # find next delimiter\n",
    "        delim_index = decompressed_bytes.index(DELIM, start_index)\n",
    "    except ValueError:\n",
    "        delim_index = len(decompressed_bytes)\n",
    "\n",
    "    # extract field bytes\n",
    "    field_bytes = decompressed_bytes[start_index:delim_index]\n",
    "\n",
    "    # First field (Email/Mobile presence) is numeric bit indicator\n",
    "    if len(fields) == 0:\n",
    "        field_value = field_bytes[0] & 0b11  # 0-3\n",
    "    else:\n",
    "        # other fields are text\n",
    "        field_value = field_bytes.decode(\"ISO-8859-1\")\n",
    "\n",
    "    fields.append(field_value)\n",
    "\n",
    "    # Stop when VTC field is reached\n",
    "    if len(fields) - 1 == FIELD_INDEX_VTC:\n",
    "        break\n",
    "\n",
    "    # Move to next byte after delimiter\n",
    "    start_index = delim_index + 1\n",
    "\n",
    "# -----------------------------\n",
    "# 5 Access VTC field\n",
    "# -----------------------------\n",
    "VTC_value = fields[FIELD_INDEX_VTC]\n",
    "print(\"Fields extracted up to VTC:\", fields)\n",
    "print(\"VTC value:\", VTC_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91684c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 584720250523201830889\n",
      "Date of Birth: 19-10-2004\n",
      "Address: Kheda, Pij Road, A-29, Opp Ramev Mandir, 387002\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from io import BytesIO\n",
    "\n",
    "# BigInteger  byte array\n",
    "bigint_str = \"YOUR_BIG_INTEGER_AS_STRING\"\n",
    "bigint_value = int(bigint_str)\n",
    "byte_array = bigint_value.to_bytes((bigint_value.bit_length() + 7) // 8, byteorder=\"big\", signed=False)\n",
    "\n",
    "# GZIP decompress if header present\n",
    "if byte_array[:2] == b'\\x1f\\x8b':  # gzip header\n",
    "    with gzip.GzipFile(fileobj=BytesIO(byte_array)) as f:\n",
    "        decompressed_bytes = f.read()\n",
    "else:\n",
    "    decompressed_bytes = byte_array\n",
    "\n",
    "# Parse fields until VTC\n",
    "DELIM = 255\n",
    "fields = []\n",
    "start_index = 0\n",
    "FIELD_INDEX_VTC = 5  # replace with actual index of VTC\n",
    "\n",
    "while start_index < len(decompressed_bytes):\n",
    "    try:\n",
    "        delim_index = decompressed_bytes.index(DELIM, start_index)\n",
    "    except ValueError:\n",
    "        delim_index = len(decompressed_bytes)\n",
    "\n",
    "    field_bytes = decompressed_bytes[start_index:delim_index]\n",
    "\n",
    "    if len(fields) == 0:\n",
    "        Email_mobile_present_bit_indicator_value = field_bytes[0] & 0b11\n",
    "        fields.append(Email_mobile_present_bit_indicator_value)\n",
    "    else:\n",
    "        fields.append(field_bytes.decode(\"ISO-8859-1\"))\n",
    "\n",
    "    if len(fields) - 1 == FIELD_INDEX_VTC:\n",
    "        break\n",
    "\n",
    "    start_index = delim_index + 1\n",
    "\n",
    "# Extract signature (last 256 bytes)\n",
    "SIGNATURE_SIZE = 256\n",
    "signature_value = decompressed_bytes[-SIGNATURE_SIZE:]\n",
    "\n",
    "# Extract Mobile/Email\n",
    "MOBILE_SIZE = 32\n",
    "EMAIL_SIZE = 32\n",
    "mobile_value = email_value = None\n",
    "index = len(decompressed_bytes) - SIGNATURE_SIZE\n",
    "\n",
    "if Email_mobile_present_bit_indicator_value == 3:\n",
    "    mobile_bytes = decompressed_bytes[index - MOBILE_SIZE:index]\n",
    "    email_bytes = decompressed_bytes[index - MOBILE_SIZE - EMAIL_SIZE:index - MOBILE_SIZE]\n",
    "    mobile_value = mobile_bytes.hex()\n",
    "    email_value = email_bytes.hex()\n",
    "elif Email_mobile_present_bit_indicator_value == 1:\n",
    "    mobile_bytes = decompressed_bytes[index - MOBILE_SIZE:index]\n",
    "    mobile_value = mobile_bytes.hex()\n",
    "elif Email_mobile_present_bit_indicator_value == 2:\n",
    "    email_bytes = decompressed_bytes[index - EMAIL_SIZE:index]\n",
    "    email_value = email_bytes.hex()\n",
    "\n",
    "# Extract Photo\n",
    "photo_start_index = decompressed_bytes.index(DELIM, start_index) + 1\n",
    "photo_end_index = len(decompressed_bytes) - SIGNATURE_SIZE\n",
    "if Email_mobile_present_bit_indicator_value in [1,3]:\n",
    "    photo_end_index -= MOBILE_SIZE\n",
    "if Email_mobile_present_bit_indicator_value in [2,3]:\n",
    "    photo_end_index -= EMAIL_SIZE\n",
    "\n",
    "photo_bytes = decompressed_bytes[photo_start_index:photo_end_index]\n",
    "\n",
    "# Signed data (without signature)\n",
    "signed_data = decompressed_bytes[:len(decompressed_bytes) - SIGNATURE_SIZE]\n",
    "\n",
    "# Output\n",
    "print(\"Email/Mobile Indicator:\", Email_mobile_present_bit_indicator_value)\n",
    "print(\"Mobile (hex):\", mobile_value)\n",
    "print(\"Email (hex):\", email_value)\n",
    "print(\"Photo bytes length:\", len(photo_bytes))\n",
    "print(\"Signature bytes length:\", len(signature_value))\n",
    "print(\"Signed data length:\", len(signed_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc051a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load image in grayscale\n",
    "img = cv2.imread(\"C:/Users/dhruv/OneDrive/Desktop/Aadhaar_Detection/Codes/qr.png\")\n",
    "qcd = cv2.QRCodeDetector()\n",
    "\n",
    "retval, decoded_info, points, straight_qrcode = qcd.detectAndDecodeMulti(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a150348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6012ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyzbar in c:\\users\\dhruv\\onedrive\\desktop\\aadhaar_detection\\venv\\lib\\site-packages (0.1.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyzbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74068877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format: BarcodeFormat.QRCode\n",
      "QR Text: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<PrintLetterBarcodeData uid=\"292420753312\" name=\"Chandibhamar Mahek Saumil\" gender=\"F\" yob=\"2005\" house=\"26\" lm=\"gandhinagar Society\" loc=\"ratanpar\" vtc=\"Joravarnagar\" po=\"Joravarnagar\" dist=\"Surendra Nagar\" subdist=\"Wadhwan\" state=\"Gujarat\" pc=\"363020\" dob=\"10/07/2005\"/>\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<PrintLetterBarcodeData uid=\"292420753312\" name=\"Chandibhamar Mahek Saumil\" gender=\"F\" yob=\"2005\" house=\"26\" lm=\"gandhinagar Society\" loc=\"ratanpar\" vtc=\"Joravarnagar\" po=\"Joravarnagar\" dist=\"Surendra Nagar\" subdist=\"Wadhwan\" state=\"Gujarat\" pc=\"363020\" dob=\"10/07/2005\"/>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<PrintLetterBarcodeData uid=\"292420753312\" name=\"Chandibhamar Mahek Saumil\" gender=\"F\" yob=\"2005\" house=\"26\" lm=\"gandhinagar Society\" loc=\"ratanpar\" vtc=\"Jorav",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo QR code detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.text)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m bigint_str = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m bigint_value = \u001b[38;5;28mint\u001b[39m(bigint_str)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 2 Convert BigInteger to byte array\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<PrintLetterBarcodeData uid=\"292420753312\" name=\"Chandibhamar Mahek Saumil\" gender=\"F\" yob=\"2005\" house=\"26\" lm=\"gandhinagar Society\" loc=\"ratanpar\" vtc=\"Jorav"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import cv2\n",
    "import zxingcpp\n",
    "from io import BytesIO\n",
    "\n",
    "# bigint_str = int(\"6979414848205548481619299442879901900893978332594614407044767717485407280104077714658698163325401659212830920734233047578454701810567032015270223682917915825234703754712504887921309181789607809168884583848396456653007022479356336240198130363930881632367124738541517499494458139647378808680614169273221404741476596583953169248831376224396335169577064812987140578144885819479190173537644970232125142253963784979138011318798385442436099901621998283624816070080504830712594525760596934341576755626791590403636878139861665599383319429228364434183913197958738697001410493839281298692342829951566712530309758759364649701153639921979798429707566199261950037418171329283207372048014948669160666776198414040633384677104717697507521717586776709084200364956178863636105988867260929887577092955570407803783021397897341999914616790441029837229129746669225095633201097644321593502503404440714110515167034889128258965583435965030225845348564582051521348800742574442877087774194668983516629631073341202705453382780613775427336949283388084891654484225446940941660942440637784744293259916479841407088189462964489670231866481904237338494872813098890875845640034370370387108798950180220865436012752487216677041817312930119747601017807577565413977545693375480131324240696099879479436722576566447939593195590684591261809038023122178172006150499569185218838749337238281597037288924464009997530938336798176023597292328320965086990184531426188862965408313308973495924965144113396593829090645266653313774582036138982013368561474719154447134894466611560589758251829063226370300282175823479569847261439348404558251402273730865053482214589180028302043821438357583302818374143973997002745047526405755760407045006694423501337081780299815080324840337828812644300041900356816429114261098230198976752026002079876882796597235615015594486182057781476152918170746403157005216896239428521706033466061587608065036133153074432195952131368564234168005447770190345777024917629879639171161719929852078265309160759260989590618158889891835294735614366674503961584445497685736312628248483551986529867423016255476553691922054241686230968975229511700928171281549902682365302333677412951788839806869796040512235899311734337858684531156721416280114473368826463098485252394260075790386415875290922570568686439586036262465414002334117870088922801660529414759784318799843806130096998190881240404138869293309782335305296720666220243304175086358278211355789957998014801209332293458940463859106591986434520433810583569309224929264228263841477378949329312443958215939294432669464260216534074560882723006838459792812340253078330291135526952675203790833430237852831740601433198364243363569730205351077393441691141240055900819091229931605146865520183001810239708464322588389956036291760175558843819105418234580239610174323636606095262722940143706063698846499673285377621180570537788160304936809915237889489342387891057012783726694920184573202789672963922380028271124448024265644396686341508447830351380242127542393849410283830409594988503246799544444687606954881510597515686410993828907588979699141180160893062603338104857903239845856783130275935413569275439908789983311663211937449259444259898972766208\")\n",
    "# bigint_str = int(\"1011253456373559996825775946156673208806218874773777002636410696573689858368855101030525940264639310330984319879826413048482326461605177443122012341323374405169067276171649032009896329787302657256744819245733598246872579611273232103490186905047155735349255046700055714481845656375094480364885674395294657582236418733145942843607320312035180776657906110577656290551807293091818684488498528529821885788340352532299861918349499181216689010832548298274897819284506558961506420617513722243130896081282473951995532256761504765072420912146389301840084936082246213642567113919454510928594727455348217562061238797133699562492753682799582554244985657361980170477536111130918490604316845232156505834420763670409620288228825101945037028272465431754760674639715026207000203562984776269739277005216990700073094407095029215123813619244282064355089449969527356853336781651483166483422371677629769273727663497561674349126591030495610533934205243787150434321380583114852456566278088671049832422108122945065114962303440901403059713183809001340166646345510141151706195500347487593368641940453698477605885790606890758245827030484449689922325740329613038528061351424696836604538813729433479232389381250843825750748855270206568587378590939841400511645473265987746897212463087290739173996658961443731463367943832612568964936816836032024326329911925570884901918107566456736041814711609046368211410039985611159032517299044901199589369945077306825923951759467404276802634597893652342574646622703752914076921973899523891929148047821548867569965082511688531678338269324675555136964861966286934917790805057312465716377099095604276059231156938641000895750179279172465732529675908641256806557036450079083131221518178380567996252964519232363931737950568833336694330543049152276251484882164630846502786018916882581760840453146516105000915125900341109543031928220881939782845113459461104663151470206773878340570702101567909777608259422558167907492584823497270113878099304424103148536099378745445673574702686588613643720367459670911066804381792377041415758270639762404323655341090662545019160743615488147271148004032238808475930405647609952430141921347389914487527204615835357365739342031343435369639500351103920158671742298713692250745632408362541173292449902069874248171651982232144215514044815558339957328703348848181364834202991781993312970816073082184177579192635360725650812692362012913313089757121862267015013449592571833300557947730896643433805285183370135915597674455300339409125649253721161933879300829258273166494595613900530019862649534528991994874835909272047197797417424704643923064699342629931503526858926069922207449703407452538921185123223729981797105572698408183737252170917989995285428631559784493279998335926547833510211372505698148427685248606725926915566674404344202890992903174955971858696563416488190576182471746840449298336749975602478976393532209486257509505538559991596669444727019787685232920566217724105845346651445085885288401002934571098447796636724556835145881145790404744982667869374424181237332163762087479349607592366595789892179065363921362428681949762271891986012352872448\")\n",
    "# bigint_str = 1011253456373559996825775946156673208806218874773777002636410696573689858368855101030525940264639310330984319879826413048482326461605177443122012341323374405169067276171649032009896329787302657256744819245733598246872579611273232103490186905047155735349255046700055714481845656375094480364885674395294657582236418733145942843607320312035180776657906110577656290551807293091818684488498528529821885788340352532299861918349499181216689010832548298274897819284506558961506420617513722243130896081282473951995532256761504765072420912146389301840084936082246213642567113919454510928594727455348217562061238797133699562492753682799582554244985657361980170477536111130918490604316845232156505834420763670409620288228825101945037028272465431754760674639715026207000203562984776269739277005216990700073094407095029215123813619244282064355089449969527356853336781651483166483422371677629769273727663497561674349126591030495610533934205243787150434321380583114852456566278088671049832422108122945065114962303440901403059713183809001340166646345510141151706195500347487593368641940453698477605885790606890758245827030484449689922325740329613038528061351424696836604538813729433479232389381250843825750748855270206568587378590939841400511645473265987746897212463087290739173996658961443731463367943832612568964936816836032024326329911925570884901918107566456736041814711609046368211410039985611159032517299044901199589369945077306825923951759467404276802634597893652342574646622703752914076921973899523891929148047821548867569965082511688531678338269324675555136964861966286934917790805057312465716377099095604276059231156938641000895750179279172465732529675908641256806557036450079083131221518178380567996252964519232363931737950568833336694330543049152276251484882164630846502786018916882581760840453146516105000915125900341109543031928220881939782845113459461104663151470206773878340570702101567909777608259422558167907492584823497270113878099304424103148536099378745445673574702686588613643720367459670911066804381792377041415758270639762404323655341090662545019160743615488147271148004032238808475930405647609952430141921347389914487527204615835357365739342031343435369639500351103920158671742298713692250745632408362541173292449902069874248171651982232144215514044815558339957328703348848181364834202991781993312970816073082184177579192635360725650812692362012913313089757121862267015013449592571833300557947730896643433805285183370135915597674455300339409125649253721161933879300829258273166494595613900530019862649534528991994874835909272047197797417424704643923064699342629931503526858926069922207449703407452538921185123223729981797105572698408183737252170917989995285428631559784493279998335926547833510211372505698148427685248606725926915566674404344202890992903174955971858696563416488190576182471746840449298336749975602478976393532209486257509505538559991596669444727019787685232920566217724105845346651445085885288401002934571098447796636724556835145881145790404744982667869374424181237332163762087479349607592366595789892179065363921362428681949762271891986012352872448\n",
    "\n",
    "img = cv2.imread(\n",
    "    r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Trust\\\\data\\\\input\\\\page_1.png\"\n",
    "    # r\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Trust\\\\Image_Folder\\\\page_1.png\"\n",
    ")\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(\"Image not loaded. Check path.\")\n",
    "\n",
    "result = zxingcpp.read_barcode(img)\n",
    "\n",
    "if result:\n",
    "    print(\"Format:\", result.format)\n",
    "    print(\"QR Text:\", result.text)\n",
    "else:\n",
    "    print(\"No QR code detected\")\n",
    "print(result.text)\n",
    "bigint_str = int(result.text)\n",
    "bigint_value = int(bigint_str)\n",
    "\n",
    "# -----------------------------\n",
    "# 2 Convert BigInteger to byte array\n",
    "# -----------------------------\n",
    "byte_array = bigint_value.to_bytes((bigint_value.bit_length() + 7) // 8, byteorder=\"big\", signed=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 3 GZIP decompress\n",
    "# -----------------------------\n",
    "with gzip.GzipFile(fileobj=BytesIO(byte_array)) as f:\n",
    "    decompressed_bytes = f.read()\n",
    "\n",
    "# -----------------------------\n",
    "# 4 Parse fields separated by delimiter 255\n",
    "# -----------------------------\n",
    "DELIM = 255\n",
    "fields = []\n",
    "start_index = 0\n",
    "\n",
    "# Replace with the correct index of VTC field in your sequence\n",
    "FIELD_INDEX_VTC = 17  # example, change as per actual sequence\n",
    "\n",
    "while start_index < len(decompressed_bytes):\n",
    "    try:\n",
    "        # find next delimiter\n",
    "        delim_index = decompressed_bytes.index(DELIM, start_index)\n",
    "    except ValueError:\n",
    "        delim_index = len(decompressed_bytes)\n",
    "\n",
    "    # extract field bytes\n",
    "    field_bytes = decompressed_bytes[start_index:delim_index]\n",
    "\n",
    "    # First field (Email/Mobile presence) is numeric bit indicator\n",
    "    if len(fields) == 0:\n",
    "        field_value = field_bytes[0] & 0b11  # 0-3\n",
    "    else:\n",
    "        # other fields are text\n",
    "        field_value = field_bytes.decode(\"ISO-8859-1\")\n",
    "\n",
    "    fields.append(field_value)\n",
    "\n",
    "    # Stop when VTC field is reached\n",
    "    if len(fields) - 1 == FIELD_INDEX_VTC:\n",
    "        break\n",
    "\n",
    "    # Move to next byte after delimiter\n",
    "    start_index = delim_index + 1\n",
    "\n",
    "# -----------------------------\n",
    "# 5 Access VTC field\n",
    "# -----------------------------\n",
    "VTC_value = fields[FIELD_INDEX_VTC]\n",
    "print(\"Fields extracted up to VTC:\", fields)\n",
    "print(\"VTC value:\", VTC_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0966c23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': False, 'error': 'QR not decoded after all enhancements'}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import zxingcpp\n",
    "from itertools import product\n",
    "\n",
    "# -------------------------------\n",
    "# IMAGE ENHANCEMENT FUNCTIONS\n",
    "# -------------------------------\n",
    "\n",
    "def apply_clahe(gray):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    return clahe.apply(gray)\n",
    "\n",
    "def adaptive_binarize(gray):\n",
    "    return cv2.adaptiveThreshold(\n",
    "        gray, 255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 31, 5\n",
    "    )\n",
    "\n",
    "def sharpen(img):\n",
    "    kernel = np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "def deskew(img):\n",
    "    coords = np.column_stack(np.where(img > 0))\n",
    "    if coords.shape[0] < 10:\n",
    "        return img\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "\n",
    "    (h, w) = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "    return cv2.warpAffine(img, M, (w, h),\n",
    "                           flags=cv2.INTER_CUBIC,\n",
    "                           borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "# -------------------------------\n",
    "# IMAGE VARIANT GENERATOR\n",
    "# -------------------------------\n",
    "\n",
    "def generate_variants(image):\n",
    "    variants = []\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    clahe = apply_clahe(gray)\n",
    "    bin_img = adaptive_binarize(clahe)\n",
    "    sharp = sharpen(clahe)\n",
    "\n",
    "    base_variants = [gray, clahe, bin_img, sharp]\n",
    "\n",
    "    scales = [0.8, 1.0, 1.5, 2.0]\n",
    "    rotations = [0, -10, 10, 90]\n",
    "\n",
    "    for img, scale, rot in product(base_variants, scales, rotations):\n",
    "        h, w = img.shape\n",
    "        resized = cv2.resize(img, None, fx=scale, fy=scale)\n",
    "        M = cv2.getRotationMatrix2D(\n",
    "            (resized.shape[1]//2, resized.shape[0]//2), rot, 1.0\n",
    "        )\n",
    "        rotated = cv2.warpAffine(resized, M,\n",
    "                                 (resized.shape[1], resized.shape[0]))\n",
    "        variants.append(rotated)\n",
    "\n",
    "    return variants\n",
    "\n",
    "# -------------------------------\n",
    "# QR DECODING\n",
    "# -------------------------------\n",
    "\n",
    "def decode_with_zxing(img):\n",
    "    results = zxingcpp.read_barcodes(img)\n",
    "    for r in results:\n",
    "        if r.text:\n",
    "            return r.text\n",
    "    return None\n",
    "\n",
    "def decode_with_opencv(img):\n",
    "    detector = cv2.QRCodeDetector()\n",
    "    data, _, _ = detector.detectAndDecode(img)\n",
    "    return data if data else None\n",
    "\n",
    "# -------------------------------\n",
    "# MAIN PIPELINE\n",
    "# -------------------------------\n",
    "\n",
    "def decode_aadhaar_qr(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"Image not readable\"\n",
    "        }\n",
    "\n",
    "    variants = generate_variants(image)\n",
    "\n",
    "    for idx, variant in enumerate(variants):\n",
    "        # Try ZXing first\n",
    "        decoded = decode_with_zxing(variant)\n",
    "        if decoded:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"method\": \"ZXing\",\n",
    "                \"variant_index\": idx,\n",
    "                \"qr_data\": decoded\n",
    "            }\n",
    "\n",
    "        # Fallback to OpenCV\n",
    "        decoded = decode_with_opencv(variant)\n",
    "        if decoded:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"method\": \"OpenCV\",\n",
    "                \"variant_index\": idx,\n",
    "                \"qr_data\": decoded\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        \"success\": False,\n",
    "        \"error\": \"QR not decoded after all enhancements\"\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# USAGE\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = decode_aadhaar_qr(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Codes\\\\extracted_qr\\\\qr_1_clean.png\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66288e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': True, 'method': 'ZXing', 'variant_index': 16, 'qr_data': '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<PrintLetterBarcodeData uid=\"402516423463\" name=\"Raju Wamanrao Sawai\" gender=\"M\" yob=\"1976\" co=\"S/O: Wamanrao Sawai\" loc=\"Naya Akola\" vtc=\"Naya akola\" po=\"Naya Akola\" dist=\"Amravati\" subdist=\"Amravati\" state=\"Maharashtra\" pc=\"444801\"/>'}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import zxingcpp\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "SAVE_ALL_IMAGES = True\n",
    "DEBUG_DIR = \"debug_variants\"\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# IMAGE ENHANCEMENT FUNCTIONS\n",
    "# -------------------------------\n",
    "\n",
    "def apply_clahe(gray):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    return clahe.apply(gray)\n",
    "\n",
    "def adaptive_binarize(gray):\n",
    "    return cv2.adaptiveThreshold(\n",
    "        gray, 255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 31, 5\n",
    "    )\n",
    "\n",
    "def sharpen(img):\n",
    "    kernel = np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "# -------------------------------\n",
    "# IMAGE VARIANT GENERATOR\n",
    "# -------------------------------\n",
    "\n",
    "def generate_variants(image):\n",
    "    variants = []\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    base_variants = {\n",
    "        \"gray\": gray,\n",
    "        \"clahe\": apply_clahe(gray),\n",
    "        \"binary\": adaptive_binarize(apply_clahe(gray)),\n",
    "        \"sharpen\": sharpen(apply_clahe(gray))\n",
    "    }\n",
    "\n",
    "    scales = [0.8, 1.0, 1.5, 2.0]\n",
    "    rotations = [0, -10, 10, 90]\n",
    "\n",
    "    idx = 0\n",
    "    for name, img in base_variants.items():\n",
    "        for scale, rot in product(scales, rotations):\n",
    "            resized = cv2.resize(img, None, fx=scale, fy=scale)\n",
    "            M = cv2.getRotationMatrix2D(\n",
    "                (resized.shape[1]//2, resized.shape[0]//2),\n",
    "                rot, 1.0\n",
    "            )\n",
    "            rotated = cv2.warpAffine(\n",
    "                resized, M,\n",
    "                (resized.shape[1], resized.shape[0])\n",
    "            )\n",
    "\n",
    "            if SAVE_ALL_IMAGES:\n",
    "                filename = f\"{DEBUG_DIR}/variant_{idx:03d}_{name}_scale{scale}_rot{rot}.png\"\n",
    "                cv2.imwrite(filename, rotated)\n",
    "\n",
    "            variants.append(rotated)\n",
    "            idx += 1\n",
    "\n",
    "    return variants\n",
    "\n",
    "# -------------------------------\n",
    "# QR DECODING\n",
    "# -------------------------------\n",
    "\n",
    "def decode_with_zxing(img):\n",
    "    results = zxingcpp.read_barcodes(img)\n",
    "    for r in results:\n",
    "        if r.text:\n",
    "            return r.text\n",
    "    return None\n",
    "\n",
    "def decode_with_opencv(img):\n",
    "    detector = cv2.QRCodeDetector()\n",
    "    data, _, _ = detector.detectAndDecode(img)\n",
    "    return data if data else None\n",
    "\n",
    "# -------------------------------\n",
    "# MAIN PIPELINE\n",
    "# -------------------------------\n",
    "\n",
    "def decode_aadhaar_qr(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return {\"success\": False, \"error\": \"Image not readable\"}\n",
    "\n",
    "    variants = generate_variants(image)\n",
    "\n",
    "    for idx, variant in enumerate(variants):\n",
    "        decoded = decode_with_zxing(variant)\n",
    "        if decoded:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"method\": \"ZXing\",\n",
    "                \"variant_index\": idx,\n",
    "                \"qr_data\": decoded\n",
    "            }\n",
    "\n",
    "        decoded = decode_with_opencv(variant)\n",
    "        if decoded:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"method\": \"OpenCV\",\n",
    "                \"variant_index\": idx,\n",
    "                \"qr_data\": decoded\n",
    "            }\n",
    "\n",
    "    return {\"success\": False, \"error\": \"QR not decoded\"}\n",
    "\n",
    "# -------------------------------\n",
    "# USAGE\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # for i in os.listdir(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Real_Images\"):\n",
    "    result = decode_aadhaar_qr(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Real_Images\\\\94.jpg\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f18f533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " QR not decoded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = cv2.imread(\n",
    "    \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\extracted_qr\\\\qr_1_hq.png\",\n",
    "    cv2.IMREAD_GRAYSCALE\n",
    ")\n",
    "\n",
    "result = zxingcpp.read_barcode(img)\n",
    "\n",
    "if result:\n",
    "    print(\"Format:\", result.format)\n",
    "    print(\"QR Text:\", result.text)\n",
    "else:\n",
    "    print(\" QR not decoded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f316dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Trust\\data\\input\\1.jpg: 640x640 1 qr_code, 130.5ms\n",
      "Speed: 17.4ms preprocess, 130.5ms inference, 9.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[] QR saved  extracted_qr_raw\\qr_1_1.png\n",
      " 1 QR(s) extracted from 1.jpg\n",
      "\n",
      "image 1/1 C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Trust\\data\\input\\WhatsAppImage.jpeg: 480x640 2 qr_codes, 68.3ms\n",
      "Speed: 2.6ms preprocess, 68.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[] QR saved  extracted_qr_raw\\qr_WhatsAppImage_1.png\n",
      "[] QR saved  extracted_qr_raw\\qr_WhatsAppImage_2.png\n",
      " 2 QR(s) extracted from WhatsAppImage.jpeg\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Load YOLO model\n",
    "# -----------------------------\n",
    "model = YOLO(r\"C:\\\\Users\\\\dhruv\\\\Downloads\\\\qr_runs\\\\detect\\\\train\\\\weights\\\\best.pt\")\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "input_dir = r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Trust\\data\\input\"\n",
    "output_dir = \"extracted_qr_raw\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Process images\n",
    "# -----------------------------\n",
    "for filename in os.listdir(input_dir):\n",
    "    img_path = os.path.join(input_dir, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\" Cannot read {filename}\")\n",
    "        continue\n",
    "\n",
    "    # YOLO inference\n",
    "    results = model(img_path, conf=0.25, save=False)\n",
    "\n",
    "    qr_count = 0\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    for result in results:\n",
    "        if result.boxes is None:\n",
    "            continue\n",
    "\n",
    "        for box in result.boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            class_name = model.names[cls_id]\n",
    "\n",
    "            if class_name.lower() == \"qr_code\":\n",
    "                qr_count += 1\n",
    "\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "\n",
    "                # Padding\n",
    "                pad = 15\n",
    "                x1 = max(0, x1 - pad)\n",
    "                y1 = max(0, y1 - pad)\n",
    "                x2 = min(w, x2 + pad)\n",
    "                y2 = min(h, y2 + pad)\n",
    "\n",
    "                qr_crop = img[y1:y2, x1:x2]\n",
    "\n",
    "                save_path = os.path.join(\n",
    "                    output_dir,\n",
    "                    f\"qr_{os.path.splitext(filename)[0]}_{qr_count}.png\"\n",
    "                )\n",
    "\n",
    "                cv2.imwrite(save_path, qr_crop)\n",
    "                print(f\"[] QR saved  {save_path}\")\n",
    "\n",
    "    if qr_count == 0:\n",
    "        print(f\" No QR found in {filename}\")\n",
    "    else:\n",
    "        print(f\" {qr_count} QR(s) extracted from {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b9b57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: 1.jpg\n",
      "============================================================\n",
      "\n",
      "Image size: (800, 800, 3)\n",
      "Stage 1: Trying whole image...\n",
      "Stage 2: Extracting QR region...\n",
      "Stage 4: Preprocessing whole image as fallback...\n",
      "Stage 3: Preprocessing QR region...\n",
      "\n",
      "============================================================\n",
      "FINAL RESULT\n",
      "============================================================\n",
      "{'success': False, 'error': 'Could not decode QR in any stage', 'stages_tried': ['whole_image', 'extracted_region', 'preprocessed_extracted', 'preprocessed_whole']}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import zxingcpp\n",
    "import os\n",
    "\n",
    "DEBUG_DIR = \"debug_qr\"\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "# ========================================\n",
    "# STAGE 1: WHOLE IMAGE (FASTEST)\n",
    "# ========================================\n",
    "\n",
    "def try_whole_image(image):\n",
    "    \"\"\"\n",
    "    Try decoding the whole image first - often works!\n",
    "    ZXing's detector can find QR codes in cluttered images\n",
    "    \"\"\"\n",
    "    print(\"Stage 1: Trying whole image...\")\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    cv2.imwrite(f\"{DEBUG_DIR}/01_whole_image.png\", gray)\n",
    "    \n",
    "    # Try direct decode\n",
    "    results = zxingcpp.read_barcodes(gray, formats=zxingcpp.BarcodeFormat.QRCode)\n",
    "    for r in results:\n",
    "        if r.text:\n",
    "            print(\" Success with whole image!\")\n",
    "            return r.text\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ========================================\n",
    "# STAGE 2: EXTRACT QR REGION\n",
    "# ========================================\n",
    "\n",
    "def extract_qr_region(image):\n",
    "    \"\"\"\n",
    "    Use OpenCV's QR detector to locate QR region\n",
    "    \"\"\"\n",
    "    print(\"Stage 2: Extracting QR region...\")\n",
    "    \n",
    "    detector = cv2.QRCodeDetector()\n",
    "    \n",
    "    # Try to detect QR code location\n",
    "    retval, points = detector.detect(image)\n",
    "    \n",
    "    if retval and points is not None:\n",
    "        # Get bounding box from detected points\n",
    "        points = points[0]\n",
    "        x_coords = points[:, 0]\n",
    "        y_coords = points[:, 1]\n",
    "        \n",
    "        x_min, x_max = int(x_coords.min()), int(x_coords.max())\n",
    "        y_min, y_max = int(y_coords.min()), int(y_coords.max())\n",
    "        \n",
    "        # Crop with some margin\n",
    "        margin = 20\n",
    "        h, w = image.shape[:2]\n",
    "        x_min = max(0, x_min - margin)\n",
    "        y_min = max(0, y_min - margin)\n",
    "        x_max = min(w, x_max + margin)\n",
    "        y_max = min(h, y_max + margin)\n",
    "        \n",
    "        cropped = image[y_min:y_max, x_min:x_max]\n",
    "        \n",
    "        cv2.imwrite(f\"{DEBUG_DIR}/02_extracted_region.png\", cropped)\n",
    "        return cropped\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ========================================\n",
    "# STAGE 3: PREPROCESS EXTRACTED QR\n",
    "# ========================================\n",
    "\n",
    "def add_white_padding(image, padding_px=40):\n",
    "    \"\"\"Add white padding around image\"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            image, padding_px, padding_px, padding_px, padding_px,\n",
    "            cv2.BORDER_CONSTANT, value=[255, 255, 255]\n",
    "        )\n",
    "    else:\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            image, padding_px, padding_px, padding_px, padding_px,\n",
    "            cv2.BORDER_CONSTANT, value=255\n",
    "        )\n",
    "    return padded\n",
    "\n",
    "def ensure_minimum_size(image, min_size=300):\n",
    "    \"\"\"Ensure image is at least min_size x min_size\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if h < min_size or w < min_size:\n",
    "        scale = min_size / min(h, w)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        return cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "    return image\n",
    "\n",
    "def preprocess_qr(image, save_prefix=\"03\"):\n",
    "    \"\"\"\n",
    "    Apply various preprocessing techniques\n",
    "    Returns list of (processed_image, description) tuples\n",
    "    \"\"\"\n",
    "    print(\"Stage 3: Preprocessing QR region...\")\n",
    "    \n",
    "    variants = []\n",
    "    \n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image.copy()\n",
    "    \n",
    "    # 1. PADDING + RESIZE (MOST IMPORTANT)\n",
    "    padded = add_white_padding(gray, padding_px=50)\n",
    "    padded_resized = ensure_minimum_size(padded, min_size=400)\n",
    "    variants.append((padded_resized, \"padded_resized\"))\n",
    "    cv2.imwrite(f\"{DEBUG_DIR}/{save_prefix}_a_padded_resized.png\", padded_resized)\n",
    "    \n",
    "    # 2. CLAHE (Contrast enhancement)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    enhanced_padded = add_white_padding(enhanced, padding_px=50)\n",
    "    enhanced_padded = ensure_minimum_size(enhanced_padded, min_size=400)\n",
    "    variants.append((enhanced_padded, \"clahe_padded\"))\n",
    "    cv2.imwrite(f\"{DEBUG_DIR}/{save_prefix}_b_clahe.png\", enhanced_padded)\n",
    "    \n",
    "    # 3. ADAPTIVE THRESHOLD (Binary)\n",
    "    binary = cv2.adaptiveThreshold(\n",
    "        gray, 255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 51, 10\n",
    "    )\n",
    "    binary_padded = add_white_padding(binary, padding_px=50)\n",
    "    binary_padded = ensure_minimum_size(binary_padded, min_size=400)\n",
    "    variants.append((binary_padded, \"binary_padded\"))\n",
    "    cv2.imwrite(f\"{DEBUG_DIR}/{save_prefix}_c_binary.png\", binary_padded)\n",
    "    \n",
    "    # 4. OTSU THRESHOLD (Alternative binary)\n",
    "    _, otsu = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    otsu_padded = add_white_padding(otsu, padding_px=50)\n",
    "    otsu_padded = ensure_minimum_size(otsu_padded, min_size=400)\n",
    "    variants.append((otsu_padded, \"otsu_padded\"))\n",
    "    cv2.imwrite(f\"{DEBUG_DIR}/{save_prefix}_d_otsu.png\", otsu_padded)\n",
    "    \n",
    "    # 5. SHARPEN\n",
    "    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    sharpened = cv2.filter2D(enhanced, -1, kernel)\n",
    "    sharpened_padded = add_white_padding(sharpened, padding_px=50)\n",
    "    sharpened_padded = ensure_minimum_size(sharpened_padded, min_size=400)\n",
    "    variants.append((sharpened_padded, \"sharpened\"))\n",
    "    cv2.imwrite(f\"{DEBUG_DIR}/{save_prefix}_e_sharpened.png\", sharpened_padded)\n",
    "    \n",
    "    # 6. BILATERAL FILTER (Denoise while keeping edges)\n",
    "    denoised = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    denoised_padded = add_white_padding(denoised, padding_px=50)\n",
    "    denoised_padded = ensure_minimum_size(denoised_padded, min_size=400)\n",
    "    variants.append((denoised_padded, \"denoised\"))\n",
    "    cv2.imwrite(f\"{DEBUG_DIR}/{save_prefix}_f_denoised.png\", denoised_padded)\n",
    "    \n",
    "    return variants\n",
    "\n",
    "# ========================================\n",
    "# STAGE 4: TRY DECODING VARIANTS\n",
    "# ========================================\n",
    "\n",
    "def try_decode_variants(variants, stage_name=\"\"):\n",
    "    \"\"\"Try decoding each variant with both ZXing and OpenCV\"\"\"\n",
    "    \n",
    "    for idx, (variant, desc) in enumerate(variants):\n",
    "        # Try ZXing first\n",
    "        try:\n",
    "            results = zxingcpp.read_barcodes(variant, formats=zxingcpp.BarcodeFormat.QRCode)\n",
    "            for r in results:\n",
    "                if r.text:\n",
    "                    print(f\" Success with {stage_name} - {desc} (ZXing)\")\n",
    "                    return {\"success\": True, \"method\": \"ZXing\", \"variant\": desc, \"data\": r.text}\n",
    "        except Exception as e:\n",
    "            print(f\"  ZXing error on {desc}: {e}\")\n",
    "        \n",
    "        # Try OpenCV as fallback\n",
    "        try:\n",
    "            detector = cv2.QRCodeDetector()\n",
    "            data, _, _ = detector.detectAndDecode(variant)\n",
    "            if data:\n",
    "                print(f\" Success with {stage_name} - {desc} (OpenCV)\")\n",
    "                return {\"success\": True, \"method\": \"OpenCV\", \"variant\": desc, \"data\": data}\n",
    "        except Exception as e:\n",
    "            print(f\"  OpenCV error on {desc}: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ========================================\n",
    "# MAIN PIPELINE\n",
    "# ========================================\n",
    "\n",
    "def decode_aadhaar_qr_smart(image_path):\n",
    "    \"\"\"\n",
    "    Smart multi-stage pipeline:\n",
    "    1. Try whole image (fastest, often works)\n",
    "    2. Extract QR region\n",
    "    3. Preprocess extracted region\n",
    "    4. Try various techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {os.path.basename(image_path)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return {\"success\": False, \"error\": \"Cannot read image\"}\n",
    "    \n",
    "    print(f\"Image size: {image.shape}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # STAGE 1: Try whole image first\n",
    "    # ----------------------------------------\n",
    "    result = try_whole_image(image)\n",
    "    if result:\n",
    "        return {\"success\": True, \"stage\": \"whole_image\", \"data\": result}\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # STAGE 2: Extract QR region\n",
    "    # ----------------------------------------\n",
    "    qr_region = extract_qr_region(image)\n",
    "    \n",
    "    if qr_region is not None:\n",
    "        # Try decoding extracted region directly\n",
    "        if len(qr_region.shape) == 3:\n",
    "            qr_gray = cv2.cvtColor(qr_region, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            qr_gray = qr_region\n",
    "        \n",
    "        results = zxingcpp.read_barcodes(qr_gray, formats=zxingcpp.BarcodeFormat.QRCode)\n",
    "        for r in results:\n",
    "            if r.text:\n",
    "                print(\" Success with extracted region!\")\n",
    "                return {\"success\": True, \"stage\": \"extracted_region\", \"data\": r.text}\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # STAGE 3: Preprocess and try variants\n",
    "        # ----------------------------------------\n",
    "        variants = preprocess_qr(qr_region)\n",
    "        result = try_decode_variants(variants, \"preprocessed extracted region\")\n",
    "        if result:\n",
    "            result[\"stage\"] = \"preprocessed_extracted\"\n",
    "            return result\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # STAGE 4: Fallback - preprocess whole image\n",
    "    # ----------------------------------------\n",
    "    print(\"Stage 4: Preprocessing whole image as fallback...\")\n",
    "    variants = preprocess_qr(image, save_prefix=\"04_whole\")\n",
    "    result = try_decode_variants(variants, \"preprocessed whole image\")\n",
    "    if result:\n",
    "        result[\"stage\"] = \"preprocessed_whole\"\n",
    "        return result\n",
    "    \n",
    "    return {\n",
    "        \"success\": False,\n",
    "        \"error\": \"Could not decode QR in any stage\",\n",
    "        \"stages_tried\": [\"whole_image\", \"extracted_region\", \"preprocessed_extracted\", \"preprocessed_whole\"]\n",
    "    }\n",
    "\n",
    "# ========================================\n",
    "# BATCH PROCESSING\n",
    "# ========================================\n",
    "\n",
    "def batch_process(folder_path):\n",
    "    \"\"\"Process all images in a folder\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            continue\n",
    "        \n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        result = decode_aadhaar_qr_smart(image_path)\n",
    "        \n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"success\": result.get(\"success\", False),\n",
    "            \"stage\": result.get(\"stage\", \"failed\"),\n",
    "            \"has_data\": bool(result.get(\"data\"))\n",
    "        })\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total = len(results)\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    \n",
    "    print(f\"Total images: {total}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {total - successful}\")\n",
    "    print(f\"Success rate: {successful/total*100:.1f}%\\n\")\n",
    "    \n",
    "    # Stage breakdown\n",
    "    from collections import Counter\n",
    "    stages = Counter(r[\"stage\"] for r in results)\n",
    "    print(\"Success by stage:\")\n",
    "    for stage, count in stages.most_common():\n",
    "        print(f\"  {stage}: {count}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ========================================\n",
    "# USAGE\n",
    "# ========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Single image\n",
    "    # image_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Real_Images\\\\94.jpg\"\n",
    "    image_path = r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Trust\\data\\input\\1.jpg\"\n",
    "    result = decode_aadhaar_qr_smart(image_path)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL RESULT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(result)\n",
    "    \n",
    "    # Batch processing (uncomment to use)\n",
    "    # folder_path = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Dataset\\\\Real_Images\"\n",
    "    # batch_results = batch_process(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7dda486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 800x800 small  75\n",
      " 348x640 small 101\n",
      " 302x551 small 101\n",
      " 640x640 small  75\n",
      " 640x640 small 101\n",
      " 640x640 small  95\n",
      " 640x640 small 101\n",
      " 619x931 small 101\n",
      " 890x1404 large 101\n",
      " 890x1404 large  95\n",
      " 390x625 small 101\n",
      " 390x625 small  95\n",
      " 745x1175 small 101\n",
      " 1600x1200 large 101\n",
      " 800x800 small  94\n",
      " 1600x1200 large  80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Noiseprint_Images'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from Noiseprint import *\n",
    "import os\n",
    "\n",
    "def noiseprint_creation(img_path):\n",
    "    examples=[]\n",
    "    noiseprints=[]\n",
    "\n",
    "    for img_path in glob.glob(f'{img_path}\\\\*'):\n",
    "        example,noise_print=getNoiseprint(img_path)\n",
    "        examples.append(example)\n",
    "        # print(img_path.split('\\\\')[-1].split('.')[0])\n",
    "        noiseprints.append([img_path.split('\\\\')[-1].split('.')[0],noise_print])\n",
    "\n",
    "    output_dir = \"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Noiseprint_Images\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    border = 34\n",
    "\n",
    "    for name, res in noiseprints:\n",
    "        if res.shape[0] > 2 * border and res.shape[1] > 2 * border:\n",
    "            crop = res[border:-border, border:-border]\n",
    "        else:\n",
    "            crop = res\n",
    "\n",
    "        # Normalize for saving\n",
    "        vmin = np.min(crop)\n",
    "        vmax = np.max(crop)\n",
    "        norm_crop = np.clip((crop - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "        # Save with name\n",
    "        filename = os.path.join(output_dir, f\"noiseprint_{name}.png\")\n",
    "        plt.imsave(filename, norm_crop, cmap='gray')\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "noiseprint_creation(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Aadhaar_Trust\\\\Image_Folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e17322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class ForgeryDetectionCNN(nn.Module):\n",
    "    \"\"\"Enhanced CNN for forgery detection\"\"\"\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(ForgeryDetectionCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.3),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)  # Binary: Real or Fake\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"Siamese network for comparing noise patterns\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((4, 4)),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        self.comparison = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)  # Similar or Different\n",
    "        )\n",
    "    \n",
    "    def forward_once(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        feat1 = self.forward_once(x1)\n",
    "        feat2 = self.forward_once(x2)\n",
    "        combined = torch.cat([feat1, feat2], dim=1)\n",
    "        out = self.comparison(combined)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83368c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noiseprint_78_forged1.png {'prediction': 'fake', 'fake_probability': 0.9881, 'num_patches': 49}\n",
      "noiseprint_78_forged10.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_78_forged2.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_78_forged3.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_78_forged4.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_78_forged5.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_78_forged6.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_78_forged7.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_78_forged8.png {'prediction': 'fake', 'fake_probability': 0.9994, 'num_patches': 49}\n",
      "noiseprint_78_forged9.png {'prediction': 'fake', 'fake_probability': 0.9994, 'num_patches': 49}\n",
      "noiseprint_79_forged1.png {'prediction': 'fake', 'fake_probability': 0.978, 'num_patches': 49}\n",
      "noiseprint_79_forged10.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_79_forged2.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_79_forged3.png {'prediction': 'fake', 'fake_probability': 0.993, 'num_patches': 49}\n",
      "noiseprint_79_forged4.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_79_forged5.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_79_forged6.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_79_forged7.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_79_forged8.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_79_forged9.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_80_forged1.png {'prediction': 'fake', 'fake_probability': 0.9867, 'num_patches': 49}\n",
      "noiseprint_80_forged10.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_80_forged2.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_80_forged3.png {'prediction': 'fake', 'fake_probability': 0.9719, 'num_patches': 49}\n",
      "noiseprint_80_forged4.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_80_forged5.png {'prediction': 'fake', 'fake_probability': 0.9999, 'num_patches': 49}\n",
      "noiseprint_80_forged6.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_80_forged7.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_80_forged8.png {'prediction': 'fake', 'fake_probability': 0.9999, 'num_patches': 49}\n",
      "noiseprint_80_forged9.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_81_forged1.png {'prediction': 'fake', 'fake_probability': 0.9924, 'num_patches': 49}\n",
      "noiseprint_81_forged10.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_81_forged2.png {'prediction': 'fake', 'fake_probability': 0.9858, 'num_patches': 49}\n",
      "noiseprint_81_forged3.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_81_forged4.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_81_forged5.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_81_forged6.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_81_forged7.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_81_forged8.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_81_forged9.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_82_forged1.png {'prediction': 'fake', 'fake_probability': 0.9989, 'num_patches': 49}\n",
      "noiseprint_82_forged10.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_82_forged2.png {'prediction': 'fake', 'fake_probability': 0.9951, 'num_patches': 49}\n",
      "noiseprint_82_forged3.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_82_forged4.png {'prediction': 'fake', 'fake_probability': 0.9953, 'num_patches': 49}\n",
      "noiseprint_82_forged5.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_82_forged6.png {'prediction': 'fake', 'fake_probability': 0.9684, 'num_patches': 49}\n",
      "noiseprint_82_forged7.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_82_forged8.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_82_forged9.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_83_forged1.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_83_forged10.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_83_forged2.png {'prediction': 'fake', 'fake_probability': 0.9802, 'num_patches': 49}\n",
      "noiseprint_83_forged3.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_83_forged4.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_83_forged5.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_83_forged6.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_83_forged7.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_83_forged8.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_83_forged9.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_84_forged1.png {'prediction': 'fake', 'fake_probability': 0.9998, 'num_patches': 49}\n",
      "noiseprint_84_forged10.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_84_forged2.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_84_forged3.png {'prediction': 'fake', 'fake_probability': 0.9826, 'num_patches': 49}\n",
      "noiseprint_84_forged4.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_84_forged5.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_84_forged6.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_84_forged7.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_84_forged8.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_84_forged9.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_85_forged1.png {'prediction': 'fake', 'fake_probability': 0.9706, 'num_patches': 286}\n",
      "noiseprint_85_forged10.png {'prediction': 'fake', 'fake_probability': 0.9979, 'num_patches': 286}\n",
      "noiseprint_85_forged2.png {'prediction': 'fake', 'fake_probability': 0.949, 'num_patches': 286}\n",
      "noiseprint_85_forged3.png {'prediction': 'fake', 'fake_probability': 0.9822, 'num_patches': 286}\n",
      "noiseprint_85_forged4.png {'prediction': 'fake', 'fake_probability': 0.978, 'num_patches': 286}\n",
      "noiseprint_85_forged5.png {'prediction': 'fake', 'fake_probability': 0.9804, 'num_patches': 286}\n",
      "noiseprint_85_forged6.png {'prediction': 'fake', 'fake_probability': 0.9527, 'num_patches': 286}\n",
      "noiseprint_85_forged7.png {'prediction': 'fake', 'fake_probability': 0.9834, 'num_patches': 286}\n",
      "noiseprint_85_forged8.png {'prediction': 'fake', 'fake_probability': 0.9825, 'num_patches': 286}\n",
      "noiseprint_85_forged9.png {'prediction': 'fake', 'fake_probability': 0.9829, 'num_patches': 286}\n",
      "noiseprint_86_forged1.png {'prediction': 'fake', 'fake_probability': 0.9951, 'num_patches': 49}\n",
      "noiseprint_86_forged10.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_86_forged2.png {'prediction': 'fake', 'fake_probability': 0.9929, 'num_patches': 49}\n",
      "noiseprint_86_forged3.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_86_forged4.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_86_forged5.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_86_forged6.png {'prediction': 'fake', 'fake_probability': 0.9799, 'num_patches': 49}\n",
      "noiseprint_86_forged7.png {'prediction': 'fake', 'fake_probability': 0.982, 'num_patches': 49}\n",
      "noiseprint_86_forged8.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_86_forged9.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_87_forged1.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_87_forged10.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_87_forged2.png {'prediction': 'fake', 'fake_probability': 0.986, 'num_patches': 49}\n",
      "noiseprint_87_forged3.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_87_forged4.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_87_forged5.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_87_forged6.png {'prediction': 'fake', 'fake_probability': 0.9999, 'num_patches': 49}\n",
      "noiseprint_87_forged7.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_87_forged8.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n",
      "noiseprint_87_forged9.png {'prediction': 'fake', 'fake_probability': 1.0, 'num_patches': 49}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD MODEL\n",
    "# -------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = ForgeryDetectionCNN(dropout=0.5).to(device)\n",
    "checkpoint = torch.load(r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Codes\\best_acc.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# PREDICT IMAGE (STABLE VERSION)\n",
    "# -------------------------------\n",
    "def predict_image(\n",
    "    image_path,\n",
    "    model,\n",
    "    patch_size=128,\n",
    "    stride=64\n",
    "):\n",
    "    # Load noiseprint (grayscale)\n",
    "    noiseprint = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    noiseprint = noiseprint.astype(np.float32) / 127.5 - 1.0\n",
    "\n",
    "    # Pad if image is smaller than patch\n",
    "    H, W = noiseprint.shape\n",
    "    if H < patch_size or W < patch_size:\n",
    "        noiseprint = np.pad(\n",
    "            noiseprint,\n",
    "            ((0, max(0, patch_size - H)),\n",
    "             (0, max(0, patch_size - W))),\n",
    "            mode=\"reflect\"\n",
    "        )\n",
    "\n",
    "    fake_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, noiseprint.shape[0] - patch_size + 1, stride):\n",
    "            for j in range(0, noiseprint.shape[1] - patch_size + 1, stride):\n",
    "\n",
    "                patch = noiseprint[i:i+patch_size, j:j+patch_size]\n",
    "                patch = torch.from_numpy(patch)\\\n",
    "                             .unsqueeze(0)\\\n",
    "                             .unsqueeze(0)\\\n",
    "                             .to(device)\n",
    "\n",
    "                logits = model(patch)\n",
    "                prob = F.softmax(logits, dim=1)\n",
    "                fake_probs.append(prob[:, 1].item())\n",
    "\n",
    "    # Aggregate\n",
    "    final_fake_prob = float(np.mean(fake_probs))\n",
    "    prediction = \"fake\" if final_fake_prob >= 0.5 else \"real\"\n",
    "\n",
    "    return {\n",
    "        \"prediction\": prediction,\n",
    "        \"fake_probability\": round(final_fake_prob, 4),\n",
    "        \"num_patches\": len(fake_probs)\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# RUN ON FOLDER\n",
    "# -------------------------------\n",
    "image_dir = r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Aadhaar_Dataset\\Balanced_Data\\test\\fake\"\n",
    "\n",
    "for img_name in os.listdir(image_dir):\n",
    "    img_path = os.path.join(image_dir, img_name)\n",
    "    result = predict_image(img_path, model)\n",
    "    print(img_name, result)\n",
    "\n",
    "\n",
    "# import cv2\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "# import os\n",
    "# # -------------------------------\n",
    "# # LOAD BEST MODEL\n",
    "# # -------------------------------\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model = ForgeryDetectionCNN(dropout=0.5).to(device)\n",
    "# checkpoint = torch.load('best_acc.pth', map_location=device)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.eval()\n",
    "\n",
    "# # -------------------------------\n",
    "# # PREDICT SINGLE IMAGE\n",
    "# # -------------------------------\n",
    "# def predict_image(image_path, model, patch_size=128, patches_per_image=10):\n",
    "#     noiseprint = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     noiseprint = noiseprint.astype(np.float32) / 127.5 - 1.0\n",
    "\n",
    "#     h, w = noiseprint.shape\n",
    "#     if h < patch_size or w < patch_size:\n",
    "#         noiseprint = np.pad(\n",
    "#             noiseprint,\n",
    "#             ((0, max(0, patch_size - h)), (0, max(0, patch_size - w))),\n",
    "#             mode='reflect'\n",
    "#         )\n",
    "\n",
    "#     preds = []\n",
    "#     probs = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(patches_per_image):\n",
    "#             i = np.random.randint(0, noiseprint.shape[0] - patch_size + 1)\n",
    "#             j = np.random.randint(0, noiseprint.shape[1] - patch_size + 1)\n",
    "\n",
    "#             patch = noiseprint[i:i+patch_size, j:j+patch_size]\n",
    "#             patch = torch.from_numpy(patch).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "#             output = model(patch)\n",
    "#             prob = F.softmax(output, dim=1)\n",
    "#             preds.append(torch.argmax(prob, dim=1).item())\n",
    "#             probs.append(prob[:, 1].item())  # fake prob\n",
    "\n",
    "#     final_pred = int(np.mean(preds) >= 0.5)\n",
    "#     final_prob = float(np.mean(probs))\n",
    "\n",
    "#     return {\n",
    "#         \"prediction\": \"fake\" if final_pred == 1 else \"real\",\n",
    "#         \"fake_probability\": round(final_prob, 4)\n",
    "#     }\n",
    "\n",
    "# # -------------------------------\n",
    "# # USAGE\n",
    "# # -------------------------------\n",
    "# for i in os.listdir(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\Noiseprint_Images\"):\n",
    "#     result = predict_image(f\" \", model)\n",
    "#     print(i,result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71173cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Noiseprints: 100%|| 2/2 [00:00<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_noiseprint_preserve_quality(input_folder, output_folder):\n",
    "    augmentation = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.05, contrast=0.05),  # very light to preserve noise\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.02, 0.02), scale=(0.98, 1.02)),\n",
    "    ])\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    for file_name in tqdm(image_files, desc=\"Augmenting Noiseprints\"):\n",
    "        try:\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            image_path = os.path.join(input_folder, file_name)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            original_size = image.size  # Preserve exact size\n",
    "\n",
    "            # Save original\n",
    "            image.save(os.path.join(output_folder, f\"noiseprint_{base_name}_orig.png\"))\n",
    "\n",
    "            # Save 9 augmented versions\n",
    "            for i in range(9):\n",
    "                aug_img = augmentation(image)\n",
    "                aug_img = aug_img.resize(original_size)  # Ensure size matches\n",
    "                aug_img.save(os.path.join(output_folder, f\"noiseprint_{base_name}_aug_{i}.png\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "augment_noiseprint_preserve_quality(\n",
    "    input_folder=\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\temp\",  # Replace with your folder\n",
    "    output_folder=\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\output_Temp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d78436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\dhruv\\onedrive\\desktop\\aadhaar_detection\\venv\\lib\\site-packages (1.26.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ad85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page_1.png\n",
      "Saved page_2.png\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "\n",
    "# # Convert PDF to images\n",
    "# pdf_path = 'C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\mahek_adhar.pdf'\n",
    "# pdf = fitz.open(pdf_path)\n",
    "\n",
    "# for page_num in range(len(pdf)):\n",
    "#     page = pdf[page_num]\n",
    "#     pix = page.get_pixmap(dpi=300)  # 300 DPI for good quality\n",
    "#     pix.save(f'page_{page_num + 1}.png')\n",
    "#     print(f'Saved page_{page_num + 1}.png')\n",
    "\n",
    "# pdf.close()\n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed00caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "# Load modelx\n",
    "model = YOLO(\"C:\\\\Users\\\\dhruv\\\\Downloads\\\\yolo_runsnew_2\\\\detect\\\\train\\\\weights\\\\best.pt\")\n",
    "\n",
    "# Run inference\n",
    "# results = model(\"C:\\\\Users\\\\dhruv\\\\OneDrive\\\\Desktop\\\\Aadhaar_Detection\\\\noiseprints_folder\\\\noiseprint_14_forged4_original.png\", conf=0.25, save=True)\n",
    "for i in os.listdir(r\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Noiseprint_Images\"):\n",
    "    results = model(\"C:\\Users\\dhruv\\OneDrive\\Desktop\\Aadhaar_Detection\\Noiseprint_Images\",f\"\\{i}\", conf=0.25, verbose = False)\n",
    "    # print(results[0])\n",
    "    print(i,\"\",results[0].boxes.cls)\n",
    "    # break\n",
    "    # r = results[0]          # first image result\n",
    "    # if len(r.boxes) == 0:\n",
    "    #     print(\" No forgery detected\")\n",
    "    # else:\n",
    "    #     print(f\" Forgery detected: {len(r.boxes)} regions\")\n",
    "\n",
    "    # for box in r.boxes:\n",
    "    #     print(\"Class:\", int(box.cls))\n",
    "    #     print(\"Confidence:\", float(box.conf))\n",
    "    #     print(\"BBox:\", box.xyxy)\n",
    "    # break\n",
    "    # print(\"Inference done. Saved to runs/detect/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc0dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
